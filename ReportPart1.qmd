---
title: "Time series regression - Store sales forecasting, Part 1"
author: "Ahmet Zamanis"
format: ipynb
editor: visual
jupyter: python3
toc: true
execute:
  warning: false
---

## Introduction

This is a report on time series analysis & regression modeling, performed in Python, mainly with the [Darts](https://unit8co.github.io/darts/) library. The dataset is from the [Kaggle Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting) competition. The data consists of daily sales data for an Ecuadorian supermarket chain between 2013 and 2017. This is Part 1 of the analysis, which will focus only on forecasting the daily national sales of the chain, across all stores and categories. In Part 2, I will forecast the sales in each category - store combination as required by the competition, and attempt various hierarchical reconciliation techniques.

The main information source used extensively for this analysis is the textbook [Forecasting: Principles and Practice](https://otexts.com/fpp3/), written by Rob Hyndman and George Athanasopoulos. The book is the most complete source on time series analysis & forecasting I could find. It uses R and the [tidyverts](https://tidyverts.org/) libraries in its example code.

```{python Settings}

# Import libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set printing options
np.set_printoptions(suppress=True, precision=4)
pd.options.display.float_format = '{:.4f}'.format
pd.set_option('display.max_columns', None)

# Set plotting options
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams["figure.autolayout"] = True
sns.set_theme(context="paper")
```

## Data preparation

The data is split into several .csv files.

```{python Load data}
#| include: false

# Load original datasets
df_train = pd.read_csv("./OriginalData/train.csv", encoding="utf-8")
df_stores = pd.read_csv("./OriginalData/stores.csv", encoding="utf-8")
df_oil = pd.read_csv("./OriginalData/oil.csv", encoding="utf-8")
df_holidays = pd.read_csv("./OriginalData/holidays_events.csv", encoding="utf-8")
df_trans = pd.read_csv("./OriginalData/transactions.csv", encoding="utf-8")
```

**.csv** and **test.csv** are the main datasets, consisting of daily sales data. The ing data ranges from 01-01-2013 to 15-08-2017, and the testing data consists of the next 15 days, in August 2017. We won't do a competition submission in Part 1, so we won't load the testing data.

```{python df_}

# View the daily sales data
df_train.head(5)
```

-   For each day, we have the sales in each store (out of a possible 54) and each product category (out of a possible 33). This amounts to 1782 time series that need to be forecasted for the competition, but in Part 1 of this analysis, we will keep it simple and only forecast the national sales in each day, in all categories.

-   **onpromotion** is the number of items on sale that day, in that category & store.

**stores.csv** contains more information about each store: The city, state, store type and store cluster.

```{python df_stores}

# View the stores data
df_stores.head(5)
```

**holidays.csv** contains information about local (city-wide), regional (state-wide) and national holidays, as well as some special nation-wide events in the time period. We will use these along with the stores' location data to create calendar features.

```{python df_holidays}

# View the holidays data
df_holidays.head(5)
```

**oil.csv** consists of the daily oil prices in the time period. Ecuador has an oil-dependent economy, so this may be a useful predictor of the cyclicality in supermarket sales.

```{python df_oil}

# View the oil data
df_oil.head(5)
```

**transactions.csv** consists of the daily number of transactions in a store. Another potentially useful feature.

```{python df_trans}

df_trans.head(5)
```

We will rename some columns from the datasets and merge the supplementary information into the ing data. We'll aggregate daily transactions across all stores beforehand, as we are only interested in predicting daily national sales.

```{python MergeData}
#| output: false

# Rename columns
df_holidays = df_holidays.rename(columns = {"type":"holiday_type"})
df_oil = df_oil.rename(columns = {"dcoilwtico":"oil"})
df_stores = df_stores.rename(columns = {
  "type":"store_type", "cluster":"store_cluster"})

# Aggregate daily transactions across all stores
df_trans = df_trans.groupby("date").transactions.sum()

# Add columns from oil, stores and transactions datasets into main data
df_train = df_train.merge(df_trans, on = ["date"], how = "left")
df_train = df_train.merge(df_oil, on = "date", how = "left")
df_train = df_train.merge(df_stores, on = "store_nbr", how = "left")

```

Incorporating the holidays information into the sales dataset will require more work.

```{python SplitHolidays}

# Split holidays data into local, regional, national and events
events = df_holidays[df_holidays["holiday_type"] == "Event"]
df_holidays = df_holidays.drop(labels=(events.index), axis=0)
local = df_holidays.loc[df_holidays["locale"] == "Local"]
regional = df_holidays.loc[df_holidays["locale"] == "Regional"]
national = df_holidays.loc[df_holidays["locale"] == "National"]

```

There are cases of multiple holidays or events sharing the same date and locale. We'll inspect the duplicates and drop them, so we don't have issues in feature engineering.

-   Rows with **transferred = True** are dates that are normally holidays, but the holiday was transferred to another day. In other words, these are not holidays in effect.

-   Rows with **holiday_type = Transfer** are dates that are not normally holidays, but had another holiday transferred. In other words, these are holidays in effect.

-   Rows with **holiday_type = Bridge** are dates that are not normally holidays, but were added to preceding / following holidays.

```{python Duplicates1}

# Inspect local holidays sharing same date & locale. Drop the transfer row
local[local.duplicated(["date", "locale_name"], keep = False)]
local = local.drop(265, axis = 0)
```

```{python Duplicates2}

# Inspect regional holidays sharing same date & locale. None exist
regional[regional.duplicated(["date", "locale_name"], keep = False)]
```

```{python Duplicates3}

# Inspect national holidays sharing same date & locale. Drop bridge days
national[national.duplicated(["date"], keep = False)]
national = national.drop([35, 39, 156], axis = 0)
```

```{python Duplicates4}

# Inspect events sharing same date. Drop the earthquake row as it is a one-time event
events[events.duplicated(["date"], keep = False)]
events = events.drop(244, axis = 0)
```

After getting rid of duplicates, we can create binary columns that signify whether a date was a local / regional / national holiday / event. We'll merge these back into the sales data.

```{python HolidayEventDummies}
#| include: false

# Add local_holiday binary column to local holidays data, to be merged into main 
# data.
local["local_holiday"] = (
  local.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((local.holiday_type == "Holiday") & (local.transferred == False))
).astype(int)

# Add regional_holiday binary column to regional holidays data
regional["regional_holiday"] = (
  regional.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((regional.holiday_type == "Holiday") & (regional.transferred == False))
).astype(int)

# Add national_holiday binary column to national holidays data
national["national_holiday"] = (
  national.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((national.holiday_type == "Holiday") & (national.transferred == False))
).astype(int)

# Add event column to events
events["event"] = 1

# Merge local holidays binary column to main data, on date and city
local_merge = local.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"city"})
df_train = df_train.merge(local_merge, how="left", on=["date", "city"])
df_train["local_holiday"] = df_train["local_holiday"].fillna(0).astype(int)

# Merge regional holidays binary column to main data
regional_merge = regional.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"state"})
df_train = df_train.merge(regional_merge, how="left", on=["date", "state"])
df_train["regional_holiday"] = df_train["regional_holiday"].fillna(0).astype(int)

# Merge national holidays binary column to main data, on date
national_merge = national.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df_train = df_train.merge(national_merge, how="left", on="date")
df_train["national_holiday"] = df_train["national_holiday"].fillna(0).astype(int)

# Merge events binary column to main data
events_merge = events.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df_train = df_train.merge(events_merge, how="left", on="date")
df_train["event"] = df_train["event"].fillna(0).astype(int)

```

We'll set the **date** column to a DateTimeIndex, and view the sales data with the added columns.

```{python DatetimeIndex}

# Set datetime index
df_train = df_train.set_index(pd.to_datetime(df_train.date))
df_train = df_train.drop("date", axis=1)
df_train.head(5)

```

With financial data, it's a good idea to normalize for inflation. We'll CPI adjust the sales and oil prices columns, with 2010 as our base year. The CPI values for Ecuador in the time period were retrieved [here](https://data.worldbank.org/indicator/FP.CPI.TOTL?end=2017&locations=EC&start=2010&view=chart).

-   We'll use the yearly CPI values for simplicity's sake, but it's possible to use monthly CPI for more accuracy.

-   Since 2017 is not complete in the data, and we'll use it as the validation-testing period, we'll use 2016's CPI for 2017 to avoid leaking information from the future into our predictions.

```{python CPIAdjust}
#| include: false

# CPI adjust sales and oil, with CPI 2010 = 100
cpis = {
  "2010":100, "2013":112.8, "2014":116.8, "2015":121.5, "2016":123.6, 
  "2017":123.6
  }
  
for year in [2013, 2014, 2015, 2016, 2017]:
  df_train["sales"].loc[df_train.index.year==year] = df_train["sales"].loc[
    df_train.index.year==year] / cpis[str(year)] * cpis["2010"]
  df_train["oil"].loc[df_train.index.year==year] = df_train["oil"].loc[
    df_train.index.year==year] / cpis[str(year)] * cpis["2010"]

```

We have some rows with missing values in our training data.

```{python NACheck}

# Check missing values in each column
pd.isnull(df_train).sum()
```

We will interpolate the missing values in the oil and transactions columns using time interpolation. This performs linear interpolation, but also takes the date-time index of observations into account.

```{python NAInterpolate}

df_train["oil"] = df_train["oil"].interpolate("time", limit_direction = "both")
df_train["transactions"] = df_train["transactions"].interpolate("time", limit_direction = "both")
```

We will now aggregate daily sales across all categories and stores, to retrieve our target variable.

```{python AggSales}

sales = df_train.groupby("date").sales.sum()
sales
```

We will create a Darts TimeSeries with our target variable.

```{python TSTarget}

from darts import TimeSeries
ts_sales = TimeSeries.from_series(sales, freq="D")
ts_sales

```

-   Each Pandas series / dataframe column passed is stored as a component in the Darts TS. The date-time index is stored in **time_index.**

-   To create a multivariate time series from a Pandas dataframe, we create a dataframe with each time series as a column, and a common date-time index. If the time series have a **hierarchy**, i.e. if they sum up together in a certain way, we can map that hierarchy as a dictionary. We will explore this further in part 2 of the analysis.

-   Static covariates are time-invariant covariates that may be used in predictions. In our case, the city or cluster of a store may be static covariates, but for part 1 of our analysis, we won't use these.

## Overview of hybrid modeling approach

A time series can be written as the sum of several components:

-   **Trend:** The long-term change.

-   **Seasonality:** A fluctuation that repeats based on a fixed, known time period. For example, the effect of month / season on ice cream sales.

-   **Cyclicality:** A fluctuation that does not repeat on a fixed, known time period. For example, the effect of oil prices on car sales.

-   **Remainder / Error:** The unpredictable component of the time series, at least with the available data.

When analyzing a time series with plots, it can be difficult to determine the sources of changes and fluctuations. It can be especially tricky to tell apart the cyclical effects from seasonality. Because of this, we will split our analysis and modeling into two steps:

-   In step 1, we will analyze the effects of trend, seasonality and calendar effects (such as holidays & events), build a model that predicts these effects and removes them from the series. This is called **time decomposition.**

-   In step 2, we will re-analyze the decomposed time series, this time considering the effects of covariates and lagged values of sales itself as predictors. We'll build a model that uses these predictors, train it on the decomposed sales, and add up the predictions of both models to arrive at our final predictions. This approach is called a **hybrid model.**

## Exploratory analysis 1 - Time & calendar effects

### Trend

Let's start by analyzing the overall trend in sales. Darts offers the ability to plot time series quickly.

```{python TrendPlot}

_ =  ts_sales.plot()
_ =  plt.ylabel("Daily sales, millions")
plt.show()
plt.close()
```

The series plot shows us several things:

-   Supermarket sales show an increasing trend over the years. The trend is close to linear overall, but the rate of increase declines roughly from the start of 2015.

-   Sales mostly fluctuate around a certain range, which suggests strong seasonality. However, there are also sharp deviations in certain periods, mainly across 2014 and at the start of 2015. This is likely cyclical in nature.

-   Sales decline very sharply in the first day of every year.

### Seasonality

#### Annual

Let's look at annual seasonality: How sales fluctuate over a year based on quarters, months, weeks of a year and days of a year. In the plots below, we have the daily sales averaged by each respective calendar period, colored by each year in the data. The confidence bands indicate the minimum and maximum daily sales in each respective period (in the last plot, we just have the daily sales without any averaging).

```{python AnnualSeasonality}
#| echo: false

# FIG1: Annual seasonality, period averages
fig1, axes1 = plt.subplots(2,2, sharey=True)
_ =  fig1.suptitle('Annual seasonality: Average daily sales in given time periods, millions')

# Average sales per quarter of year
_ =  sns.lineplot(
  ax = axes1[0,0],
  x = sales.index.quarter.astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ =  axes1[0,0].set_xlabel("quarter", fontsize=8)
_ =  axes1[0,0].set_ylabel("sales", fontsize=8)
_ =  axes1[0,0].tick_params(axis='both', which='major', labelsize=6)

# Average sales per month of year
_ =  sns.lineplot(
  ax = axes1[0,1],
  x = sales.index.month.astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales)
_ =  axes1[0,1].set_xlabel("month", fontsize=8)
_ =  axes1[0,1].set_ylabel("sales",fontsize=8)
_ =  axes1[0,1].legend(title = "year", bbox_to_anchor=(1.05, 1.0), fontsize="small", loc='best')
_ =  axes1[0,1].tick_params(axis='both', which='major', labelsize=6)

# Average sales per week of year
_ =  sns.lineplot(
  ax = axes1[1,0],
  x = sales.index.week, 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ =  axes1[1,0].set_xlabel("week of year", fontsize=8)
_ =  axes1[1,0].set_ylabel("sales",fontsize=8)
_ =  axes1[1,0].tick_params(axis='both', which='major', labelsize=6)
_ =  axes1[1,0].xaxis.set_ticks(np.arange(0, 52, 10))

# Average sales per day of year
_ =  sns.lineplot(
  ax = axes1[1,1],
  x = sales.index.dayofyear, 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ =  axes1[1,1].set_xlabel("day of year", fontsize=8)
_ =  axes1[1,1].set_ylabel("sales",fontsize=8)
_ =  axes1[1,1].tick_params(axis='both', which='major', labelsize=6)
_ =  axes1[1,1].xaxis.set_ticks(np.arange(0, 365, 100))

# Show fig1
plt.show()
plt.close("all")
```

-   **Quarterly:** Sales do not seem to have a considerable quarterly seasonality pattern. However, the plot still shows us a few things:

    -   Sales generally slightly increase over a year.

    -   In Q2 2014, there was a considerable drop. Sales declined almost to the level of Q2 2013. This was likely a cyclical effect.

-    **Monthly:** Sales do seem to fluctuate slightly over months, but there's no clear seasonal pattern that holds across all years. However, sales seem to sharply increase in December every year, likely due to Christmas.

    -   The cyclicality in 2014 is seen in more detail: Sales dropped almost to their 2013 levels in certain months, and recovered sharply in others.

    -   We also see a considerable drop in the first half of 2015, where sales dropped roughly to 2014 levels, followed by a recovery.

    -   There is a very sharp increase in April 2016, where sales were even higher than 2017 levels. This is due to a large earthquake that happened in April 16, 2016, and the related relief efforts.

-   **Weekly:** The seasonal patterns are more visible in the weekly plot, as we see the "waves" line up across years. It's very likely the data has strong weekly seasonality, which is what we'd expect from supermarket sales.

    -   The data for 2017 ends after August 15, so the sharp decline afterwards is misleading.

    -   The decline at the end of 2016 is also misleading, as 2016 was a 366-day year.

-   **Daily:** This plot is a bit noisy, but the very similar fluctuations across all years indicate the data is strongly seasonal. It also highlights some cyclical effects such as the April 2016 earthquake and the 2014 drops.

Another way to look at annual seasonality is to average sales in a certain calendar period across all years, without grouping by year. This shows us the "overall" seasonality pattern across one year: We likely have strong weekly seasonality that persists over years, and some monthly seasonality especially towards December.

```{python AnnualSeasonalityAgg}
#| echo: false

# FIG1.1: Annual seasonality, averaged over years
fig11, axes11 = plt.subplots(2,2, sharey=True)
_ = fig11.suptitle('Annual seasonality: Average daily sales in given time periods,\n across all years, millions');

# Average sales per quarter of year
_ = sns.lineplot(
  ax = axes11[0,0],
  x = sales.index.quarter.astype(str), 
  y = (sales / 1000000), 
  data=sales, legend=False)
_ = axes11[0,0].set_xlabel("quarter", fontsize=8)
_ = axes11[0,0].set_ylabel("sales", fontsize=8)
_ = axes11[0,0].tick_params(axis='both', which='major', labelsize=6)

# Average sales per month of year
_ = sns.lineplot(
  ax = axes11[0,1],
  x = sales.index.month.astype(str), 
  y = (sales / 1000000), 
  data=sales)
_ = axes11[0,1].set_xlabel("month", fontsize=8)
_ = axes11[0,1].set_ylabel("sales",fontsize=8)
_ = axes11[0,1].tick_params(axis='both', which='major', labelsize=6)

# Average sales per week of year
_ = sns.lineplot(
  ax = axes11[1,0],
  x = sales.index.week, 
  y = (sales / 1000000), 
  data=sales, legend=False)
_ = axes11[1,0].set_xlabel("week of year", fontsize=8)
_ = axes11[1,0].set_ylabel("sales",fontsize=8)
_ = axes11[1,0].tick_params(axis='both', which='major', labelsize=6)
_ = axes11[1,0].xaxis.set_ticks(np.arange(0, 52, 10))

# Average sales per day of year
_ = sns.lineplot(
  ax = axes11[1,1],
  x = sales.index.dayofyear, 
  y = (sales / 1000000), 
  data=sales, legend=False)
_ = axes11[1,1].set_xlabel("day of year", fontsize=8)
_ = axes11[1,1].set_ylabel("sales",fontsize=8)
_ = axes11[1,1].tick_params(axis='both', which='major', labelsize=6)
_ = axes11[1,1].xaxis.set_ticks(np.arange(0, 365, 100))

# Show fig1.1
plt.show()
plt.close("all")
```

#### Monthly & weekly

Now let's look at seasonality across days of a month and days of a week. These will likely be the most important seasonality patterns in the sales. There are three ways to look at these plots: First we will group them by year.

```{python MonthlyWeeklyByYear}
#| echo: false

# FIG2: Monthly and weekly seasonality
fig2, axes2 = plt.subplots(2)
_ = fig2.suptitle('Monthly and weekly seasonality, average daily sales in millions')

# Average sales per day of month
_ = sns.lineplot(
  ax = axes2[0],
  x = sales.index.day, 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales)
_ = axes2[0].legend(title = "year", bbox_to_anchor=(1.05, 1.0), fontsize="small", loc='best')
_ = axes2[0].set_xlabel("day of month", fontsize=8)
_ = axes2[0].set_ylabel("sales", fontsize=8)
_ = axes2[0].xaxis.set_ticks(np.arange(1, 32, 6))
_ = axes2[0].xaxis.set_ticks(np.arange(1, 32, 1), minor=True)
_ = axes2[0].yaxis.set_ticks(np.arange(0, 1.25, 0.25))
_ = axes2[0].grid(which='minor', alpha=0.5)
_ = axes2[0].grid(which='major', alpha=1)

# Average sales per day of week
_ = sns.lineplot(
  ax = axes2[1],
  x = (sales.index.dayofweek+1).astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ = axes2[1].set_xlabel("day of week", fontsize=8)
_ = axes2[1].set_ylabel("sales", fontsize=8)
_ = axes2[1].yaxis.set_ticks(np.arange(0, 1.25, 0.25))

# Show fig2
plt.show()
plt.close("all")
```

We can look at the same plots grouped by month.

```{python MonthlyYearlyByMonth}
#| echo: false

# FIG2.1: Monthly and weekly seasonality, colored by month
fig21, axes21 = plt.subplots(2)
_ = fig21.suptitle('Monthly and weekly seasonality, average daily sales in millions')

# Average sales per day of month, colored by month
_ = sns.lineplot(
  ax = axes21[0],
  x = sales.index.day, 
  y = (sales / 1000000), 
  hue = sales.index.month.astype(str), data=sales, errorbar=None)
_ = axes21[0].legend(title = "month", bbox_to_anchor=(1.05, 1.0), fontsize="x-small", loc='best')
_ = axes21[0].set_xlabel("day of month", fontsize=8)
_ = axes21[0].set_ylabel("sales", fontsize=8)
_ = axes21[0].xaxis.set_ticks(np.arange(1, 32, 6))
_ = axes21[0].xaxis.set_ticks(np.arange(1, 32, 1), minor=True)
_ = axes21[0].yaxis.set_ticks(np.arange(0, 1.25, 0.25))
_ = axes21[0].grid(which='minor', alpha=0.5)
_ = axes21[0].grid(which='major', alpha=1)

# Average sales per day of week, colored by month
_ = sns.lineplot(
  ax = axes21[1],
  x = (sales.index.dayofweek+1).astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.month.astype(str), data=sales, errorbar=None, legend=None)
_ = axes21[1].set_xlabel("day of week", fontsize=8)
_ = axes21[1].set_ylabel("sales", fontsize=8)
_ = axes21[1].yaxis.set_ticks(np.arange(0, 1.25, 0.25))

# Show fig2.1
plt.show()
plt.close("all")
```

And finally, without any grouping: The averages across all years.

```{python MonthlyYearly}
#| echo: false

# FIG2.2: Monthly and weekly seasonality, average across years
fig22, axes22 = plt.subplots(2)
_ =  fig22.suptitle('Monthly and weekly seasonality, averaged across years, in millions')

# Average sales per day of month
_ =  sns.lineplot(
  ax = axes22[0],
  x = sales.index.day, 
  y = (sales / 1000000), 
  data=sales)
_ =  axes22[0].set_xlabel("day of month", fontsize=8)
_ =  axes22[0].set_ylabel("sales", fontsize=8)
_ =  axes22[0].xaxis.set_ticks(np.arange(1, 32, 6))
_ =  axes22[0].xaxis.set_ticks(np.arange(1, 32, 1), minor=True)
_ =  axes22[0].yaxis.set_ticks(np.arange(0, 1.25, 0.25))
_ =  axes22[0].grid(which='minor', alpha=0.5)
_ =  axes22[0].grid(which='major', alpha=1)

# Average sales per day of week
_ =  sns.lineplot(
  ax = axes22[1],
  x = (sales.index.dayofweek+1).astype(str), 
  y = (sales / 1000000), 
  data=sales)
_ =  axes22[1].set_xlabel("day of week", fontsize=8)
_ =  axes22[1].set_ylabel("sales", fontsize=8)
_ =  axes22[1].yaxis.set_ticks(np.arange(0, 1.25, 0.25))

# Show fig22
plt.show()
plt.close("all")
```

### Autocorrelation & partial autocorrelation

```{python AcfPacfTime}
#| echo: false

# FIG3: ACF and PACF plots
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
fig3, axes3 = plt.subplots(2)
_ = fig3.suptitle('Autocorrelation and partial autocorrelation, daily sales, up to 54 days')
_ = plot_acf(sales, lags=range(0,55), ax=axes3[0])
_ = plot_pacf(sales, lags=range(0,55), ax=axes3[1], method="ywm")

# Show fig3
plt.show()
plt.close("all")
```

### 

## Feature engineering 1 - Time & calendar features

## Model 1 - Time effects decomposition

## Exploratory analysis 2 - Lags & covariates

## Feature engineering 2 - Lags & covariates

## Model 2 - Lags & covariates

## Conclusion
