---
title: "Time series regression - Store sales forecasting, Part 1"
author: "Ahmet Zamanis"
format: ipynb
  toc: true
editor: visual
jupyter: python3
execute:
  warning: false
  message: false
  error: false
---

## Introduction

This is a report on time series analysis & regression modeling, performed in Python, mainly with the [Darts](https://unit8co.github.io/darts/) library. The dataset is from the [Kaggle Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting) competition. The data consists of daily sales data for an Ecuadorian supermarket chain between 2013 and 2017. This is Part 1 of the analysis, which will focus only on forecasting the daily national sales of the chain, across all stores and categories. In Part 2, we will forecast the sales in each category - store combination as required by the competition, and attempt various hierarchical reconciliation techniques.

The main information source used extensively for this analysis is the textbook [Forecasting: Principles and Practice](https://otexts.com/fpp3/), written by Rob J. Hyndman and George Athanasopoulos. The book is the most complete source on time series analysis & forecasting I could find. It uses R and the [tidyverts](https://tidyverts.org/) libraries in its example code.

```{python Settings}
#| echo: false

# Import libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set printing options
np.set_printoptions(suppress=True, precision=4)
pd.options.display.float_format = '{:.4f}'.format
pd.set_option('display.max_columns', None)

# Set plotting options
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams["figure.autolayout"] = True
sns.set_theme(context="paper")
```

## Data preparation

```{python Load data}
#| include: false

# Load original datasets
df_train = pd.read_csv("./OriginalData/train.csv", encoding="utf-8")
df_stores = pd.read_csv("./OriginalData/stores.csv", encoding="utf-8")
df_oil = pd.read_csv("./OriginalData/oil.csv", encoding="utf-8")
df_holidays = pd.read_csv("./OriginalData/holidays_events.csv", encoding="utf-8")
df_trans = pd.read_csv("./OriginalData/transactions.csv", encoding="utf-8")
```

The data is split into several .csv files. **train.csv** and **test.csv** are the main datasets, consisting of daily sales data. The training data ranges from 01-01-2013 to 15-08-2017, and the testing data consists of the following 15 days, in August 2017. We won't do a competition submission in Part 1, so we won't load the testing data.

```{python df}
# View the daily sales data
df_train.head(5)
```

-   For each day, we have the sales in each store (out of a possible 54) and each product category (out of a possible 33). This amounts to 1782 time series that need to be forecasted for the competition, but in Part 1 of this analysis, we will keep it simple and only forecast the national sales in each day, in all categories.

-   **onpromotion** is the number of items on sale that day, in that category & store.

**stores.csv** contains more information about each store: The city, state, store type and store cluster.

```{python df_stores}
df_stores.head(5)
```

**holidays.csv** contains information about local (city-wide), regional (state-wide) and national holidays, and some special nation-wide events in the time period. We will use these along with the stores' location data to create calendar features.

```{python df_holidays}
df_holidays.head(5)
```

**oil.csv** consists of the daily oil prices in the time period. Ecuador has an oil-dependent economy, so this may be a useful predictor of the cyclicality in supermarket sales. We don't have the oil price for the first day of the time series.

```{python df_oil}
df_oil.head(5)
```

**transactions.csv** consists of the daily number of transactions at a store. Another potentially useful feature. Each row is the number of transactions in all categories, one day, at one store.

```{python df_trans}
df_trans.head(5)
```

We will rename some columns from the datasets and merge the supplementary information into the sales dataset. We'll aggregate daily transactions across all stores beforehand, as we are only interested in predicting daily national sales.

```{python MergeData}
#| output: false

# Rename columns
df_holidays = df_holidays.rename(columns = {"type":"holiday_type"})
df_oil = df_oil.rename(columns = {"dcoilwtico":"oil"})
df_stores = df_stores.rename(columns = {
  "type":"store_type", "cluster":"store_cluster"})

# Aggregate daily transactions across all stores
df_trans = df_trans.groupby("date").transactions.sum()

# Add columns from oil, stores and transactions datasets into main data
df_train = df_train.merge(df_trans, on = ["date"], how = "left")
df_train = df_train.merge(df_oil, on = "date", how = "left")
df_train = df_train.merge(df_stores, on = "store_nbr", how = "left")
```

Incorporating the holidays information into the sales dataset will require more work.

```{python SplitHolidays}
#| output: false

# Split holidays data into local, regional, national and events
events = df_holidays[df_holidays["holiday_type"] == "Event"]
df_holidays = df_holidays.drop(labels=(events.index), axis=0)
local = df_holidays.loc[df_holidays["locale"] == "Local"]
regional = df_holidays.loc[df_holidays["locale"] == "Regional"]
national = df_holidays.loc[df_holidays["locale"] == "National"]

```

There are cases of multiple holidays or events sharing the same date and locale. We'll inspect the duplicates and drop them so we don't have issues in feature engineering.

-   Rows with **transferred = True** are dates that are normally holidays, but the holiday was transferred to another date. In other words, these are not holidays in effect.

-   Rows with **holiday_type = Transfer** are dates that are not normally holidays, but had another holiday transferred to this date. In other words, these are holidays in effect.

-   Rows with **holiday_type = Bridge** are dates that are not normally holidays, but were added to extend preceding / following holidays.

```{python Duplicates1}

# Inspect local holidays sharing same date & locale. Drop the transfer row
local[local.duplicated(["date", "locale_name"], keep = False)]
local = local.drop(265, axis = 0)
```

```{python Duplicates2}

# Inspect regional holidays sharing same date & locale. None exist
regional[regional.duplicated(["date", "locale_name"], keep = False)]
```

```{python Duplicates3}

# Inspect national holidays sharing same date & locale. Drop bridge days
national[national.duplicated(["date"], keep = False)]
national = national.drop([35, 39, 156], axis = 0)
```

```{python Duplicates4}

# Inspect events sharing same date. Drop the earthquake row as it is a one-time event
events[events.duplicated(["date"], keep = False)]
events = events.drop(244, axis = 0)
```

After getting rid of duplicates, we can create binary columns that signify whether a date was a local / regional / national holiday / event. We'll merge these back into the sales data.

```{python HolidayEventDummies}
#| output: false

# Add local_holiday binary column to local holidays data, to be merged into main 
# data.
local["local_holiday"] = (
  local.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((local.holiday_type == "Holiday") & (local.transferred == False))
  ).astype(int)

# Add regional_holiday binary column to regional holidays data
regional["regional_holiday"] = (
  regional.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((regional.holiday_type == "Holiday") & (regional.transferred == False))
  ).astype(int)

# Add national_holiday binary column to national holidays data
national["national_holiday"] = (
  national.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((national.holiday_type == "Holiday") & (national.transferred == False))
  ).astype(int)

# Add event column to events
events["event"] = 1

# Merge local holidays binary column to main data, on date and city
local_merge = local.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"city"})
df_train = df_train.merge(local_merge, how="left", on=["date", "city"])
df_train["local_holiday"] = df_train["local_holiday"].fillna(0).astype(int)

# Merge regional holidays binary column to main data
regional_merge = regional.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"state"})
df_train = df_train.merge(regional_merge, how="left", on=["date", "state"])
df_train["regional_holiday"] = df_train["regional_holiday"].fillna(0).astype(int)

# Merge national holidays binary column to main data, on date
national_merge = national.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df_train = df_train.merge(national_merge, how="left", on="date")
df_train["national_holiday"] = df_train["national_holiday"].fillna(0).astype(int)

# Merge events binary column to main data
events_merge = events.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df_train = df_train.merge(events_merge, how="left", on="date")
df_train["event"] = df_train["event"].fillna(0).astype(int)

```

We'll set the **date** column to a DateTimeIndex, and view the sales data with the added columns.

```{python DatetimeIndex}

# Set datetime index
df_train = df_train.set_index(pd.to_datetime(df_train.date))
df_train = df_train.drop("date", axis=1)
df_train.head(5)

```

With financial data, it's a good idea to normalize for inflation. We'll CPI adjust the sales and oil prices columns, with 2010 as our base year. The CPI values for Ecuador in the time period were retrieved [here](https://data.worldbank.org/indicator/FP.CPI.TOTL?end=2017&locations=EC&start=2010&view=chart).

-   We'll use the yearly CPI values for simplicity's sake, but it's possible to use monthly CPI for more accuracy.

-   Since 2017 is not complete in the data, and we'll use it as the validation period, we'll use 2016's CPI for 2017 to avoid leaking information from the future into our predictions.

```{python CPIAdjust}
#| output: false

# CPI adjust sales and oil, with CPI 2010 = 100
cpis = {
  "2010":100, "2013":112.8, "2014":116.8, "2015":121.5, "2016":123.6, 
  "2017":123.6
  }
  
for year in [2013, 2014, 2015, 2016, 2017]:
  df_train["sales"].loc[df_train.index.year==year] = df_train["sales"].loc[
    df_train.index.year==year] / cpis[str(year)] * cpis["2010"]
  df_train["oil"].loc[df_train.index.year==year] = df_train["oil"].loc[
    df_train.index.year==year] / cpis[str(year)] * cpis["2010"]

```

We have some rows with missing values in our training data.

```{python NACheck}

# Check missing values in each column
pd.isnull(df_train).sum()
```

We will interpolate the missing values in the oil and transactions columns using time interpolation. This performs linear interpolation, but also takes the date-time index of observations into account.

```{python NAInterpolate}
df_train["oil"] = df_train["oil"].interpolate("time", limit_direction = "both")
df_train["transactions"] = df_train["transactions"].interpolate(
  "time", limit_direction = "both")
```

We will now aggregate daily sales across all categories and stores, to retrieve our target variable. We have 1684 days of national sales data.

```{python AggSales}
sales = df_train.groupby("date").sales.sum()
sales
```

We will create a Darts [TimeSeries](https://unit8co.github.io/darts/generated_api/darts.timeseries.html) with our target variable.

```{python TSTarget}
from darts import TimeSeries
ts_sales = TimeSeries.from_series(
  sales, 
  freq="D" # Time series frequency is daily
  )
ts_sales
```

-   Each Pandas series / dataframe column passed is stored as a component in the Darts TS. The date-time index is stored in **time_index.** We had 1684 rows in our Pandas series, but the Darts TS has 1688 dates. This means our series had some missing dates, which Darts completed automatically. We'll fill in the values for these dates later.

-   To create a multivariate time series, we create a Pandas dataframe with each time series as a column, and a common date-time index. When we pass this dataframe to TimeSeries, we'll have each time series as a component. If the time series have a **hierarchy**, i.e. if they sum up together in a certain way, we can map that hierarchy as a dictionary to later perform hierarchical reconciliation. We will explore this further in Part 2 of the analysis.

-   Static covariates are time-invariant covariates that may be used in predictions. In our case, the city or cluster of a store may be static covariates, but for part 1 of our analysis we are looking at national sales, so we won't use these.

## Overview of hybrid modeling approach

A time series can be written as the sum of several components:

-   **Trend:** The long-term change.

-   **Seasonality:** A fluctuation (or several) that repeats based on a fixed, known time period. For example, the fluctuation of retail store sales across days of a week, or hours of a day.

-   **Cyclicality:** A fluctuation that does not repeat on a fixed, known time period. For example, the effect of a sharp increase / decrese in oil prices on car sales.

-   **Remainder / Error:** The unpredictable component of the time series, at least with the available data and methods.

When analyzing a time series with plots, it can be difficult to determine the nature and causes of fluctuations. It can be especially tricky to tell apart the cyclical effects from repeating seasonality. Because of this, we will split our analysis and modeling into two steps:

-   In step 1, we will analyze the time effects: The trend, seasonality and calendar effects (such as holidays & events). We'll build a model that predicts these effects and remove the predictions from the time series, leaving the effects of cyclicality and the unpredictable component. This is called **time decomposition.**

-   In step 2, we will re-analyze the decomposed time series, this time considering the effects of covariates and lagged values of sales itself as predictors, to try and account for the cyclicality. We'll build a model that uses these predictors, train it on the decomposed sales, and add up the predictions of both models to arrive at our final predictions. This approach is called a **hybrid model.**

## Exploratory analysis 1 - Time & calendar effects

### Trend

Let's start by analyzing the overall trend in sales. Darts offers the ability to plot time series quickly.

```{python TrendPlot}
_ =  ts_sales.plot()
_ =  plt.ylabel("Daily sales, millions")
plt.show()
plt.close()
```

The time series plot shows us several things:

-   Supermarket sales show an increasing trend over the years. The trend is close to linear overall, but the rate of increase declines roughly from the start of 2015.

-   Sales mostly fluctuate around a certain range, which suggests strong seasonality. However, there are also sharp deviations in certain periods, mainly across 2014 and at the start of 2015. This is likely cyclical in nature.

-   The "waves" of seasonal fluctuations seem to be getting bigger over time. This suggests we should use a multiplicative time decomposition instead of additive.

-   Sales decline very sharply in the first day of every year, likely because it's a holiday.

### Seasonality

#### Annual seasonality

Let's look at annual seasonality: How sales fluctuate over a year based on quarters, months, weeks of a year and days of a year. In the plots below, we have the daily sales averaged by each respective calendar period, colored by each year in the data. The confidence bands indicate the minimum and maximum daily sales in each respective period (in the last plot, we just have the daily sales without any averaging).

```{python AnnualSeasonality}
#| echo: false

# FIG1: Annual seasonality, period averages
fig1, axes1 = plt.subplots(2,2, sharey=True)
_ =  fig1.suptitle('Annual seasonality: Average daily sales in given time periods, millions')

# Average sales per quarter of year
_ =  sns.lineplot(
  ax = axes1[0,0],
  x = sales.index.quarter.astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ =  axes1[0,0].set_xlabel("quarter", fontsize=8)
_ =  axes1[0,0].set_ylabel("sales", fontsize=8)
_ =  axes1[0,0].tick_params(axis='both', which='major', labelsize=6)

# Average sales per month of year
_ =  sns.lineplot(
  ax = axes1[0,1],
  x = sales.index.month.astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales)
_ =  axes1[0,1].set_xlabel("month", fontsize=8)
_ =  axes1[0,1].set_ylabel("sales",fontsize=8)
_ =  axes1[0,1].legend(title = "year", bbox_to_anchor=(1.05, 1.0), fontsize="small", loc='best')
_ =  axes1[0,1].tick_params(axis='both', which='major', labelsize=6)

# Average sales per week of year
_ =  sns.lineplot(
  ax = axes1[1,0],
  x = sales.index.week, 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ =  axes1[1,0].set_xlabel("week of year", fontsize=8)
_ =  axes1[1,0].set_ylabel("sales",fontsize=8)
_ =  axes1[1,0].tick_params(axis='both', which='major', labelsize=6)
_ =  axes1[1,0].xaxis.set_ticks(np.arange(0, 52, 10))

# Average sales per day of year
_ =  sns.lineplot(
  ax = axes1[1,1],
  x = sales.index.dayofyear, 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ =  axes1[1,1].set_xlabel("day of year", fontsize=8)
_ =  axes1[1,1].set_ylabel("sales",fontsize=8)
_ =  axes1[1,1].tick_params(axis='both', which='major', labelsize=6)
_ =  axes1[1,1].xaxis.set_ticks(np.arange(0, 365, 100))

# Show fig1
plt.show()
plt.close("all")
```

-   **Quarterly:** Sales do not seem to have a considerable quarterly seasonality pattern. However, the plot still shows us a few things:

    -   Sales generally slightly increase over a year.

    -   In Q2 2014, there was a considerable drop. Sales declined almost to the level of Q2 2013. This was likely a cyclical effect.

-   **Monthly:** Sales do seem to fluctuate slightly over months, but there's no clear seasonal pattern that's apparent across all years. However, sales seem to sharply increase in November and December every year, likely due to Christmas.

    -   The cyclicality in 2014 is seen in more detail: Sales dropped almost to their 2013 levels in certain months, and recovered sharply in others.

    -   We also see a considerable drop in the first half of 2015, where sales dropped roughly to 2014 levels, followed by a recovery.

    -   There is a very sharp increase in April 2016, where sales were even higher than 2017 levels. This is due to a large earthquake that happened in April 16, 2016, and the related relief efforts.

-   **Weekly:** The seasonal patterns are more visible in the weekly plot, as we see the "waves" of fluctuation line up across years. It's very likely the data has strong weekly seasonality, which is what we'd expect from supermarket sales.

    -   The data for 2017 ends after August 15, so the sharp decline afterwards is misleading.

    -   The sharp decline at the end of 2016 is also misleading, as 2016 was a 366-day year.

-   **Daily:** This plot is a bit noisy, but the very similar fluctuations across all years indicate the data is strongly seasonal. It also highlights some cyclical effects such as the April 2016 earthquake and the 2014 drops.

Another way to look at annual seasonality is to average sales in a certain calendar period across all years, without grouping by year. This shows us the "overall" seasonality pattern across one year: We likely have strong weekly seasonality that persists over years, and some monthly seasonality especially towards December.

```{python AnnualSeasonalityAgg}
#| echo: false

# FIG1.1: Annual seasonality, averaged over years
fig11, axes11 = plt.subplots(2,2, sharey=True)
_ = fig11.suptitle('Annual seasonality: Average daily sales in given time periods,\n across all years, millions');

# Average sales per quarter of year
_ = sns.lineplot(
  ax = axes11[0,0],
  x = sales.index.quarter.astype(str), 
  y = (sales / 1000000), 
  data=sales, legend=False)
_ = axes11[0,0].set_xlabel("quarter", fontsize=8)
_ = axes11[0,0].set_ylabel("sales", fontsize=8)
_ = axes11[0,0].tick_params(axis='both', which='major', labelsize=6)

# Average sales per month of year
_ = sns.lineplot(
  ax = axes11[0,1],
  x = sales.index.month.astype(str), 
  y = (sales / 1000000), 
  data=sales)
_ = axes11[0,1].set_xlabel("month", fontsize=8)
_ = axes11[0,1].set_ylabel("sales",fontsize=8)
_ = axes11[0,1].tick_params(axis='both', which='major', labelsize=6)

# Average sales per week of year
_ = sns.lineplot(
  ax = axes11[1,0],
  x = sales.index.week, 
  y = (sales / 1000000), 
  data=sales, legend=False)
_ = axes11[1,0].set_xlabel("week of year", fontsize=8)
_ = axes11[1,0].set_ylabel("sales",fontsize=8)
_ = axes11[1,0].tick_params(axis='both', which='major', labelsize=6)
_ = axes11[1,0].xaxis.set_ticks(np.arange(0, 52, 10))

# Average sales per day of year
_ = sns.lineplot(
  ax = axes11[1,1],
  x = sales.index.dayofyear, 
  y = (sales / 1000000), 
  data=sales, legend=False)
_ = axes11[1,1].set_xlabel("day of year", fontsize=8)
_ = axes11[1,1].set_ylabel("sales",fontsize=8)
_ = axes11[1,1].tick_params(axis='both', which='major', labelsize=6)
_ = axes11[1,1].xaxis.set_ticks(np.arange(0, 365, 100))

# Show fig1.1
plt.show()
plt.close("all")
```

#### Monthly & weekly seasonality

Now let's look at seasonality across days of a month and days of a week. These will likely be the most important seasonality patterns in supermarket sales. There are three ways to look at these plots: First we will group them by year.

```{python MonthlyWeeklyByYear}
#| echo: false

# FIG2: Monthly and weekly seasonality
fig2, axes2 = plt.subplots(2)
_ = fig2.suptitle('Monthly and weekly seasonality, average daily sales in millions')

# Average sales per day of month
_ = sns.lineplot(
  ax = axes2[0],
  x = sales.index.day, 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales)
_ = axes2[0].legend(title = "year", bbox_to_anchor=(1.05, 1.0), fontsize="small", loc='best')
_ = axes2[0].set_xlabel("day of month", fontsize=8)
_ = axes2[0].set_ylabel("sales", fontsize=8)
_ = axes2[0].xaxis.set_ticks(np.arange(1, 32, 6))
_ = axes2[0].xaxis.set_ticks(np.arange(1, 32, 1), minor=True)
_ = axes2[0].yaxis.set_ticks(np.arange(0, 1.25, 0.25))
_ = axes2[0].grid(which='minor', alpha=0.5)
_ = axes2[0].grid(which='major', alpha=1)

# Average sales per day of week
_ = sns.lineplot(
  ax = axes2[1],
  x = (sales.index.dayofweek+1).astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ = axes2[1].set_xlabel("day of week", fontsize=8)
_ = axes2[1].set_ylabel("sales", fontsize=8)
_ = axes2[1].yaxis.set_ticks(np.arange(0, 1.25, 0.25))

# Show fig2
plt.show()
plt.close("all")
```

-   The weekly seasonality across days of a week is strong and clear. Sales are lowest in Tuesdays, then increase and peak at Sundays, afterwards dropping on Mondays. The pattern holds in all years.

-   The monthly seasonality across days of a month aren't as strong, but look considerable. Sales are generally highest at the start of a month, likely because most salaries are paid at the end of a month, though the competition information also says salaries are paid biweekly in the middle and end of each month.

We can look at the same plots grouped by month.

```{python MonthlyYearlyByMonth}
#| echo: false

# FIG2.1: Monthly and weekly seasonality, colored by month
fig21, axes21 = plt.subplots(2)
_ = fig21.suptitle('Monthly and weekly seasonality, average daily sales in millions')

# Average sales per day of month, colored by month
_ = sns.lineplot(
  ax = axes21[0],
  x = sales.index.day, 
  y = (sales / 1000000), 
  hue = sales.index.month.astype(str), data=sales, errorbar=None)
_ = axes21[0].legend(title = "month", bbox_to_anchor=(1.05, 1.0), fontsize="x-small", loc='best')
_ = axes21[0].set_xlabel("day of month", fontsize=8)
_ = axes21[0].set_ylabel("sales", fontsize=8)
_ = axes21[0].xaxis.set_ticks(np.arange(1, 32, 6))
_ = axes21[0].xaxis.set_ticks(np.arange(1, 32, 1), minor=True)
_ = axes21[0].yaxis.set_ticks(np.arange(0, 1.25, 0.25))
_ = axes21[0].grid(which='minor', alpha=0.5)
_ = axes21[0].grid(which='major', alpha=1)

# Average sales per day of week, colored by month
_ = sns.lineplot(
  ax = axes21[1],
  x = (sales.index.dayofweek+1).astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.month.astype(str), data=sales, errorbar=None, legend=None)
_ = axes21[1].set_xlabel("day of week", fontsize=8)
_ = axes21[1].set_ylabel("sales", fontsize=8)
_ = axes21[1].yaxis.set_ticks(np.arange(0, 1.25, 0.25))

# Show fig2.1
plt.show()
plt.close("all")
```

This plot shows us the monthly and weekly seasonality pattern generally holds across all months, but December is a notable exception:

-   Sales in December are considerably higher roughly after the 13th, due to Christmas. The Christmas peak seems to happen in the 23th, followed by a decline, and another sharp increase in the 30th. The sales by day of week are also higher in December for every day, but the pattern of the weekly seasonality is the same.

-   We can also see that sales decline very sharply, almost to zero in January 1st, and make a considerably sharp recovery in the 2nd.

And finally, without any grouping: The averages across all years.

```{python MonthlyYearly}
#| echo: false

# FIG2.2: Monthly and weekly seasonality, average across years
fig22, axes22 = plt.subplots(2)
_ =  fig22.suptitle('Monthly and weekly seasonality, averaged across years, in millions')

# Average sales per day of month
_ =  sns.lineplot(
  ax = axes22[0],
  x = sales.index.day, 
  y = (sales / 1000000), 
  data=sales)
_ =  axes22[0].set_xlabel("day of month", fontsize=8)
_ =  axes22[0].set_ylabel("sales", fontsize=8)
_ =  axes22[0].xaxis.set_ticks(np.arange(1, 32, 6))
_ =  axes22[0].xaxis.set_ticks(np.arange(1, 32, 1), minor=True)
_ =  axes22[0].yaxis.set_ticks(np.arange(0, 1.25, 0.25))
_ =  axes22[0].grid(which='minor', alpha=0.5)
_ =  axes22[0].grid(which='major', alpha=1)

# Average sales per day of week
_ =  sns.lineplot(
  ax = axes22[1],
  x = (sales.index.dayofweek+1).astype(str), 
  y = (sales / 1000000), 
  data=sales)
_ =  axes22[1].set_xlabel("day of week", fontsize=8)
_ =  axes22[1].set_ylabel("sales", fontsize=8)
_ =  axes22[1].yaxis.set_ticks(np.arange(0, 1.25, 0.25))

# Show fig22
plt.show()
plt.close("all")
```

This plot allows us to see the monthly seasonality pattern more clearly: Sales are higher at the start of a month, slightly decline until the mid-month payday, slightly increase afterwards, and start peaking again after the end-of-month payday.

### Autocorrelation & partial autocorrelation

Autocorrelation is the correlation of a variable with its own lagged values.

```{python AcfPacfTime}
#| echo: false

# FIG3: ACF and PACF plots
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
fig3, axes3 = plt.subplots(2)
_ = fig3.suptitle('Autocorrelation and partial autocorrelation, daily sales, up to 54 days')
_ = plot_acf(sales, lags=range(0,55), ax=axes3[0])
_ = plot_pacf(sales, lags=range(0,55), ax=axes3[1], method="ywm")

# Show fig3
plt.show()
plt.close("all")
```

### April 2016 Earthquake

```{python EarthquakePlot}
#| echo: false

# FIG6: Zoom in on earthquake: 16 April 2016
april_sales = sales.loc[sales.index.month == 4]
may_sales = sales.loc[sales.index.month == 5]

fig6, axes6 = plt.subplots(2, sharex=True)
_ = fig6.suptitle("Effect of 16 April 2016 earthquake on sales")

# April
_ = sns.lineplot(
  ax = axes6[0],
  x = april_sales.index.day,
  y = april_sales,
  hue = april_sales.index.year.astype(str),
  data = april_sales
)
_ = axes6[0].set_title("April")
_ = axes6[0].legend(title = "year", bbox_to_anchor=(1.05, 1.0), fontsize="small", loc='best')
_ = axes6[0].set_xlabel("days of month")
_ = axes6[0].set_xticks(np.arange(1, 32, 6))
_ = axes6[0].set_xticks(np.arange(1, 32, 1), minor=True)
_ = axes6[0].grid(which='minor', alpha=0.5)
_ = axes6[0].grid(which='major', alpha=1)
_ = axes6[0].axvline(x = 16, color = "black", linestyle = "dashed")

# May
_ = sns.lineplot(
  ax = axes6[1],
  x = may_sales.index.day,
  y = may_sales,
  hue = may_sales.index.year.astype(str),
  data = may_sales, legend=False
)
_ = axes6[1].set_title("May")
_ = axes6[1].set_xlabel("days of month")
_ = axes6[1].set_xticks(np.arange(1, 32, 6))
_ = axes6[1].set_xticks(np.arange(1, 32, 1), minor=True)
_ = axes6[1].grid(which='minor', alpha=0.5)
_ = axes6[1].grid(which='major', alpha=1)

# Show FIG6
plt.show()
plt.close("all")
```

## Feature engineering 1 - Time & calendar features

### Calendar effects features

```{python FeatEngNY}

# New year's day features
df_train["ny1"] = ((df_train.index.day == 1) & (df_train.index.month == 1)).astype(int)


# Set holiday dummies to 0 if NY dummies are 1
df_train.loc[df_train["ny1"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0

df_train["ny2"] = ((df_train.index.day == 2) & (df_train.index.month == 1)).astype(int)

df_train.loc[df_train["ny2"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0
```

```{python FeatEngDecember}

# NY's eve features
df_train["ny_eve31"] = ((df_train.index.day == 31) & (df_train.index.month == 12)).astype(int)

df_train["ny_eve30"] = ((df_train.index.day == 30) & (df_train.index.month == 12)).astype(int)

df_train.loc[(df_train["ny_eve31"] == 1) | (df_train["ny_eve30"] == 1), ["local_holiday", "regional_holiday", "national_holiday"]] = 0


# Proximity to Christmas sales peak
df_train["xmas_before"] = 0

df_train.loc[
  (df_train.index.day.isin(range(13,24))) & (df_train.index.month == 12), "xmas_before"] = df_train.loc[
  (df_train.index.day.isin(range(13,24))) & (df_train.index.month == 12)].index.day - 12

df_train["xmas_after"] = 0
df_train.loc[
  (df_train.index.day.isin(range(24,28))) & (df_train.index.month == 12), "xmas_after"] = abs(df_train.loc[
  (df_train.index.day.isin(range(24,28))) & (df_train.index.month == 12)].index.day - 27)

df_train.loc[(df_train["xmas_before"] != 0) | (df_train["xmas_after"] != 0), ["local_holiday", "regional_holiday", "national_holiday"]] = 0
```

```{python FeatEngQuake}

# Strength of earthquake effect on sales
# April 18 > 17 > 19 > 20 > 21 > 22
df_train["quake_after"] = 0
df_train.loc[df_train.index == "2016-04-18", "quake_after"] = 6
df_train.loc[df_train.index == "2016-04-17", "quake_after"] = 5
df_train.loc[df_train.index == "2016-04-19", "quake_after"] = 4
df_train.loc[df_train.index == "2016-04-20", "quake_after"] = 3
df_train.loc[df_train.index == "2016-04-21", "quake_after"] = 2
df_train.loc[df_train.index == "2016-04-22", "quake_after"] = 1
```

```{python FeatEngEvents}

# Split events, delete events column
df_train["dia_madre"] = ((df_train["event"] == 1) & (df_train.index.month == 5) & (df_train.index.day.isin([8,10,11,12,14]))).astype(int)

df_train["futbol"] = ((df_train["event"] == 1) & (df_train.index.isin(pd.date_range(start = "2014-06-12", end = "2014-07-13")))).astype(int)

df_train["black_friday"] = ((df_train["event"] == 1) & (df_train.index.isin(["2014-11-28", "2015-11-27", "2016-11-25"]))).astype(int)

df_train["cyber_monday"] = ((df_train["event"] == 1) & (df_train.index.isin(["2014-12-01", "2015-11-30", "2016-11-28"]))).astype(int)

df_train = df_train.drop("event", axis=1)
```

```{python FeatEngDaysWeek}

# Days of week dummies (monday intercept)
df_train["tuesday"] = (df_train.index.dayofweek == 1).astype(int)
df_train["wednesday"] = (df_train.index.dayofweek == 2).astype(int)
df_train["thursday"] = (df_train.index.dayofweek == 3).astype(int)
df_train["friday"] = (df_train.index.dayofweek == 4).astype(int)
df_train["saturday"] = (df_train.index.dayofweek == 5).astype(int)
df_train["sunday"] = (df_train.index.dayofweek == 6).astype(int)
```

```{python FeatEngLeads}

# Holiday-event leads
df_train["local_lead1"] = df_train["local_holiday"].shift(-1).fillna(0)
df_train["regional_lead1"] = df_train["regional_holiday"].shift(-1).fillna(0)
df_train["national_lead1"] = df_train["national_holiday"].shift(-1).fillna(0)
df_train["diamadre_lead1"] = df_train["dia_madre"].shift(-1).fillna(0)

# Check missing values
pd.isnull(df_train).sum()

```

```{python FeatEngTimeAgg}

# Aggregate time features by mean
time_covars = df_train.drop(columns=['id', 'store_nbr', 'family', 'sales', 'onpromotion', 'transactions', 'oil', 'city', 'state', 'store_type', 'store_cluster'], axis=1).groupby("date").mean(numeric_only=True)
```

### Trend & seasonality features

```{python FeatEngTrend}

# Add piecewise linear trend dummies
time_covars["trend"] = range(1, 1685) # Linear dummy 1

# Knot to be put at period 729
time_covars.loc[time_covars.index=="2015-01-01"]["trend"] 

# Add second linear trend dummy
time_covars["trend_knot"] = 0
time_covars.iloc[728:,-1] = range(0, 956)

# Check start and end of knot
time_covars.loc[time_covars["trend"]>=729][["trend", "trend_knot"]] 
```

```{python FeatEngFourier}

from statsmodels.tsa.deterministic import DeterministicProcess

# Add Fourier features for monthly seasonality
dp = DeterministicProcess(
  index = time_covars.index,
  constant = False,
  order = 0, # No trend feature
  seasonal = False, # No seasonal dummy features
  period = 28, # 28-period seasonality (28 days, 1 month)
  fourier = 5, # 5 Fourier pairs
  drop = True # Drop perfectly collinear terms
)
time_covars = time_covars.merge(dp.in_sample(), how="left", on="date")

# View Fourier features
time_covars.iloc[0:5, -10:]
```

## Model 1 - Time effects decomposition

### Preprocessing

```{python TSTimeCovars}

# Make Darts time series with time feats
ts_timecovars = TimeSeries.from_dataframe(
  time_covars, freq="D", fill_missing_dates=False)

ts_timecovars
```

```{python TSGaps}

# Scan for gaps
ts_timecovars.gaps()
```

```{python FillGaps}

# Fill gaps by interpolating missing values
from darts.dataprocessing.transformers import MissingValuesFiller
na_filler = MissingValuesFiller()
ts_sales = na_filler.transform(ts_sales)
ts_timecovars = na_filler.transform(ts_timecovars)

# Scan for gaps again
ts_timecovars.gaps()
```

```{python LogExpTrafos}

# Define functions to perform log transformation and reverse it
def trafo_log(x):
  return x.map(lambda x: np.log(x+1))

def trafo_exp(x):
  return x.map(lambda x: np.exp(x)-1)
```

```{python TrainValSplit1}

# Train-validation split: Pre 2017 vs 2017
y_train1, y_val1 = trafo_log(ts_sales[:-227]), trafo_log(ts_sales[-227:])
x_train1, x_val1 = ts_timecovars[:-227], ts_timecovars[-227:]
```

### Model specification

```{python ModelSpec1}

# Import models
from darts.models.forecasting.baselines import NaiveDrift, NaiveSeasonal
from darts.models.forecasting.fft import FFT
from darts.models.forecasting.sf_ets import StatsForecastETS as ETS
from darts.models.forecasting.linear_regression_model import LinearRegressionModel

# Specify baseline models
model_drift = NaiveDrift()
model_seasonal = NaiveSeasonal()

# Specify FFT model
model_fft = FFT(
  nr_freqs_to_keep = 10,
  trend = "poly",
  trend_poly_degree = 1
)

# Specify ETS model
model_ets = ETS(
  season_length = 7,
  model = "AAA"
)

# Specify linear regression model
model_linear1 = LinearRegressionModel(
  lags_future_covariates = [0])
```

### Model validation: Predicting 2017 sales

```{python 2017Pred1}
#| include: false

# Fit models on train data (pre-2017), predict validation data (2017)
model_drift.fit(y_train1)
pred_drift = model_drift.predict(n = 227)

model_seasonal.fit(y_train1)
pred_seasonal = model_seasonal.predict(n = 227)

model_fft.fit(y_train1)
pred_fft = model_fft.predict(n = 227)

model_ets.fit(y_train1)
pred_ets = model_ets.predict(n = 227)

model_linear1.fit(y_train1, future_covariates = x_train1)
pred_linear1 = model_linear1.predict(n = 227, future_covariates = x_val1)
```

```{python 2017Score1}

# Define model scoring function
from darts.metrics import mape, rmse, rmsle
def perf_scores(val, pred, model="drift"):
  
  scores_dict = {
    "RMSE": rmse(trafo_exp(val), trafo_exp(pred)), 
    "RMSLE": rmse(val, pred), 
    "MAPE": mape(trafo_exp(val), trafo_exp(pred))
      }
      
  print("Model: " + model)
  
  for key in scores_dict:
    print(
      key + ": " + 
      str(round(scores_dict[key], 4))
       )
  print("--------")  

# Score models' performance
perf_scores(y_val1, pred_drift, model="Naive drift")
perf_scores(y_val1, pred_seasonal, model="Naive seasonal")
perf_scores(y_val1, pred_fft, model="FFT")
perf_scores(y_val1, pred_ets, model="Exponential smoothing")
perf_scores(y_val1, pred_linear1, model="Linear regression")
```

```{python 2017Plot1}
#| echo: false
#| message: false

# FIG7: Plot models' predictions against actual values
fig7, axes7 = plt.subplots(3, sharex=True, sharey=True)
_ = fig7.suptitle("Actual vs. predicted sales, time decomposition models, orange=Predicted")

# FFT
_ = trafo_exp(y_val1).plot(ax = axes7[0], label="Actual")
_ = trafo_exp(pred_fft).plot(ax = axes7[0], label="Predicted")
_ = axes7[0].set_title("FFT")
_ = axes7[0].set_label(fontsize="small")

# ETS
_ = trafo_exp(y_val1).plot(ax = axes7[1], label="Actual")
_ = trafo_exp(pred_ets).plot(ax = axes7[1], label="Predicted")
_ = axes7[1].set_title("ETS")
_ = axes7[1].set_label(fontsize="small")

# Linear regression
_ = trafo_exp(y_val1).plot(ax = axes7[2], label="Actual")
_ = trafo_exp(pred_linear1).plot(ax = axes7[2], label="Predicted")
_ = axes7[2].legend("", frameon = False)
_ = axes7[2].set_title("Linear regression")
_ = axes7[2].set_label(fontsize="small")

# Show FIG7
plt.show()
plt.close("all")
```

### Backtesting / Historical forecasts

```{python HistFore1}

# Retrieve historical forecasts and 14-17 decomposed residuals
pred_hist1 = model_linear1.historical_forecasts(
  trafo_log(ts_sales), 
  future_covariates = ts_timecovars, start = 365, stride = 1,
  verbose = True)
res_linear1 = trafo_log(ts_sales[365:]) - pred_hist1

# Score historical forecasts
perf_scores(trafo_log(ts_sales[365:]), pred_hist1, model="Linear regression, historical")
```

```{python HistForePlot1}
#| echo: false

# Plot historical forecasts for linear regression
_ = ts_sales.plot(label="Actual")
_ = trafo_exp(pred_hist1).plot(label="Predicted")
_ = plt.title("Time decomposition linear model, historical forecasts")
_ = plt.ylabel("sales")
plt.show()
plt.close("all")
```

### Residuals diagnosis

```{python ResDiag1}

# Diagnose linear model 1's innovation residuals
from darts.utils.statistics import plot_residuals_analysis, plot_pacf
_ = plot_residuals_analysis(res_linear1)
plt.show()
plt.close("all")

# PACF plot of decomped sales residuals
_ =  plot_pacf(res_linear1, max_lag=56)
_ = plt.title("Partial autocorrelation plot, residuals of linear model 1")
_ = plt.xlabel("Lags")
_ = plt.ylabel("PACF")
_ = plt.xticks(np.arange(0, 56, 10))
_ = plt.xticks(np.arange(0, 56, 1), minor=True)
_ = plt.grid(which='minor', alpha=0.5)
_ = plt.grid(which='major', alpha=1)
plt.show()
plt.close("all")
```

```{python StatTest1}

# KPSS and ADF stationarity test on model 1 residuals
from darts.utils.statistics import stationarity_test_kpss, stationarity_test_adf
stationarity_test_kpss(res_linear1) # Null rejected = data is non-stationary
stationarity_test_adf(res_linear1) # Null rejected = data is stationary around a constant
```

### Time decomposition

```{python TimeDecompFinal}

# Perform full time decomposition in Sklearn

# Retrieve Darts target and features with filled gaps
sales = ts_sales.pd_series()
time_covars = ts_timecovars.pd_dataframe()

# Train-test split
y_train, y_val = trafo_log(sales[:-227]), trafo_log(sales[-227:])
x_train, x_val = time_covars.iloc[:-227,], time_covars.iloc[-227:,]

# Fit & predict on 13-16
from sklearn.linear_model import LinearRegression
model_decomp = LinearRegression()
model_decomp.fit(x_train, y_train)
pred1 = model_decomp.predict(x_train)
res1 = y_train - pred1

# Predict on 17
pred2 = model_decomp.predict(x_val)
res2 = y_val - pred2

# Concatenate residuals to get decomposed sales
sales_decomp = pd.concat([res1, res2])

# Concatenate predictions to get model 1 predictions
preds_model1 = pd.Series(np.concatenate((pred1, pred2)), index = sales_decomp.index)
```

## Exploratory analysis 2 - Lags & covariates

```{python AggCovars}

# Aggregate daily covariates from df_train: oil, onpromotion, transactions
covars = df_train.groupby("date").agg(
 { "oil": "mean",
  "onpromotion": "sum",
  "transactions": "mean"})

# Merge decomposed sales and covariates
sales_covariates = covars.merge(sales_decomp.rename("sales"), on = "date", how = "left")
```

### Differencing

```{python TestStatCovars}

# Test stationarity of covariates
from statsmodels.tsa.stattools import kpss, adfuller
kpss(sales_covariates["oil"])
adfuller(sales_covariates["oil"])
```

```{python DiffCovars}

# Difference the covariates
from sktime.transformations.series.difference import Differencer
diff = Differencer(lags = 1)
sales_covariates[
  ['oil', 'onpromotion', 'transactions']] = diff.fit_transform(
    sales_covariates[['oil', 'onpromotion', 'transactions']])
sales_covariates
```

### Sales features

```{python LoadLagPlots}

from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf as pacf_tsa
from sktime.utils.plotting import plot_lags
from scipy.stats import pearsonr, spearmanr
```

```{python SalesAcfPacf}
#| echo: false

# FIG8: Sales ACF - PACF plot
fig8, axes8 = plt.subplots(2, sharex=True)
_ = fig8.suptitle('Autocorrelation and partial autocorrelation, decomposed sales, up to 14 days')

_ = plot_acf(
  sales_covariates["sales"], lags=np.arange(0, 15, 1, dtype=int), ax=axes8[0], marker=".")
_ = pacf_tsa(
  sales_covariates["sales"], lags=np.arange(0, 15, 1, dtype=int), ax=axes8[1], method="ywm", marker=".")

_ = axes8[0].xaxis.set_ticks(np.arange(0, 15, 1, dtype=int), minor=True)
_ = axes8[0].xaxis.set_ticks(np.arange(0, 15, 7, dtype=int))
_ = axes8[0].grid(which='minor', alpha=0.5)
_ = axes8[1].xaxis.set_ticks(np.arange(0, 15, 1, dtype=int), minor=True)
_ = axes8[1].xaxis.set_ticks(np.arange(0, 15, 7, dtype=int))
_ = axes8[1].grid(which='minor', alpha=0.5)

# Show fig8
plt.show()
plt.close("all")
```

```{python SalesLagsScatter}
#| echo: false

# FIG9: Sales lag scatterplots
fig9, axes9 = plot_lags(
  sales_covariates["sales"], lags = [1,2,3,4,5,6,7,8,9],
  suptitle = "Sales lags")
  
# Show fig9
plt.show()
plt.close("all")
```

```{python SalesEma7}

# Calculate 7-day exponential moving average of sales lags
sales_covariates["sales_ema7"] = sales_covariates["sales"].rolling(
  window = 7, min_periods = 1, center = False, win_type = "exponential").mean()
```

```{python SalesEma7Plot}
#| echo: false

# Plot sales_ema7 vs sales  
_ = sns.regplot(
  data = sales_covariates,
  x = "sales_ema7",
  y = "sales"
)
_ = plt.title("Relationship of sales and 7-day exponential moving average of sales")
plt.show()
plt.close("all")
```

```{python SalesCorrs}

# Correlation of sales and lag 1
pearsonr(sales_covariates["sales"], sales_covariates["sales"].shift(1).fillna(method="bfill")) 

# Correlation of sales and ema7
pearsonr(sales_covariates["sales"], sales_covariates["sales_ema7"]) 
```

### Oil features

```{python OilMAs}

# Calculate oil MAs
oil_ma = sales_covariates.assign(
  oil_ma7 = lambda x: x["oil"].rolling(window = 7, center = False).mean(),
  oil_ma14 = lambda x: x["oil"].rolling(window = 14, center = False).mean(),
  oil_ma28 = lambda x: x["oil"].rolling(window = 28, center = False).mean(),
  oil_ma84 = lambda x: x["oil"].rolling(window = 84, center = False).mean(),
  oil_ma168 = lambda x: x["oil"].rolling(window = 168, center = False).mean(),
  oil_ma336 = lambda x: x["oil"].rolling(window = 336, center = False).mean(),
)
```

```{python OilMAPlots}
#| echo: false

# FIG10: Regplots of oil moving averages & sales
fig10, axes10 = plt.subplots(3,2, sharey=True)
_ = fig10.suptitle("Oil price change moving averages & decomposed sales")

# MA7
_ = sns.regplot(
  ax = axes10[0,0],
  data = oil_ma,
  x = "oil_ma7",
  y = "sales"
)
_ = axes10[0,0].set_xlabel("weekly MA")
_ = axes10[0,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(oil_ma["oil_ma7"], oil_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA14
_ = sns.regplot(
  ax = axes10[0,1],
  data = oil_ma,
  x = "oil_ma14",
  y = "sales"
)
_ = axes10[0,1].set_xlabel("biweekly MA")
_ = axes10[0,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(oil_ma["oil_ma14"], oil_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA28
_ = sns.regplot(
  ax = axes10[1,0],
  data = oil_ma,
  x = "oil_ma28",
  y = "sales"
)
_ = axes10[1,0].set_xlabel("monthly MA")
_ = axes10[1,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(oil_ma["oil_ma28"], oil_ma["sales"], nan_policy='omit')[0]
      ),
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA84
_ = sns.regplot(
  ax = axes10[1,1],
  data = oil_ma,
  x = "oil_ma84",
  y = "sales"
)
_ = axes10[1,1].set_xlabel("quarterly MA")
_ = axes10[1,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(oil_ma["oil_ma84"], oil_ma["sales"], nan_policy='omit')[0]
      ),
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA168
_ = sns.regplot(
  ax = axes10[2,0],
  data = oil_ma,
  x = "oil_ma168",
  y = "sales"
)
_ = axes10[2,0].set_xlabel("semi-annual MA")
_ = axes10[2,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(oil_ma["oil_ma168"], oil_ma["sales"], nan_policy='omit')[0]
      ),
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA336
_ = sns.regplot(
  ax = axes10[2,1],
  data = oil_ma,
  x = "oil_ma336",
  y = "sales"
)
_ = axes10[2,1].set_xlabel("annual MA")
_ = axes10[2,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(oil_ma["oil_ma336"], oil_ma["sales"], nan_policy='omit')[0]
      ),
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# Show fig10
plt.show()
plt.close("all")
```

```{python OilMaInterpolate}

# Keep monthly oil MAs as a feature
sales_covariates["oil_ma28"] = sales_covariates["oil"].rolling(window = 28, center = False).mean()

# Spline interpolation for missing values
sales_covariates["oil_ma28"] = sales_covariates["oil_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Check quality of interpolation
_ = sales_covariates["oil"].plot()
_ = sales_covariates["oil_ma28"].plot()
_ = plt.title("Oil price change and its 28-day moving average, first 28 MAs interpolated")
plt.show()
plt.close("all")
```

### Onpromotion features

```{python OnpCCF}
#| echo: false

# Cross-correlation of sales & onpromotion
_ = plt.xcorr(
  sales_covariates["sales"], sales_covariates["onpromotion"], usevlines=True, maxlags=56, normed=True)
_ = plt.grid(True)
_ = plt.ylim([-0.1, 0.1])
_ = plt.xlabel("onpromotion lags / leads")
_ = plt.title("Cross-correlation, decomposed sales & onpromotion change")
plt.show()
plt.close("all")
```

```{python OnpLags}
#| echo: false

# FIG12: Onpromotion lags 0 1 6 7
fig12, axes12 = plt.subplots(2,2, sharey=True)
_ = fig12.suptitle("Onpromotion change lags & decomposed sales")

# Lag 0
_ = sns.regplot(
  ax = axes12[0,0],
  x = sales_covariates["onpromotion"],
  y = sales_covariates["sales"]
)
_ = axes12[0,0].set_xlabel("lag 0")
_ = axes12[0,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(sales_covariates["onpromotion"], sales_covariates["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )
      
# Lag 1
_ = sns.regplot(
  ax = axes12[0,1],
  x = sales_covariates["onpromotion"].shift(1),
  y = sales_covariates["sales"]
)
_ = axes12[0,1].set_xlabel("lag 1")
_ = axes12[0,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(sales_covariates["onpromotion"].shift(1), sales_covariates["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# Lag 6
_ = sns.regplot(
  ax = axes12[1,0],
  x = sales_covariates["onpromotion"].shift(6),
  y = sales_covariates["sales"]
)
_ = axes12[1,0].set_xlabel("lag 6")
_ = axes12[1,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(sales_covariates["onpromotion"].shift(6), sales_covariates["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# Lag 7
_ = sns.regplot(
  ax = axes12[1,1],
  x = sales_covariates["onpromotion"].shift(7),
  y = sales_covariates["sales"]
)
_ = axes12[1,1].set_xlabel("lag 7")
_ = axes12[1,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(sales_covariates["onpromotion"].shift(7), sales_covariates["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# Show fig12
plt.show()
plt.close("all")
```

```{python OnpMAs}
#| echo: false

# FIG11: Onpromotion MAs
fig11, axes11 = plt.subplots(2,2)
_ = fig11.suptitle("Onpromotion change moving averages & decomposed sales")

# Calculate MAs without min_periods = 1
onp_ma = sales_covariates.assign(
  onp_ma7 = lambda x: x["onpromotion"].rolling(window = 7, center = False).mean(),
  onp_ma14 = lambda x: x["onpromotion"].rolling(window = 14, center = False).mean(),
  onp_ma28 = lambda x: x["onpromotion"].rolling(window = 28, center = False).mean(),
  onp_ma84 = lambda x: x["onpromotion"].rolling(window = 84, center = False).mean()
)

# MA7
_ = sns.regplot(
  ax = axes11[0,0],
  data = onp_ma,
  x = "onp_ma7",
  y = "sales"
)
_ = axes11[0,0].set_xlabel("weekly MA")
_ = axes11[0,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(onp_ma["onp_ma7"], onp_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA14
_ = sns.regplot(
  ax = axes11[0,1],
  data = onp_ma,
  x = "onp_ma14",
  y = "sales"
)
_ = axes11[0,1].set_xlabel("biweekly MA")
_ = axes11[0,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(onp_ma["onp_ma14"], onp_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA28
_ = sns.regplot(
  ax = axes11[1,0],
  data = onp_ma,
  x = "onp_ma28",
  y = "sales"
)
_ = axes11[1,0].set_xlabel("monthly MA")
_ = axes11[1,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(onp_ma["onp_ma28"], onp_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA84
_ = sns.regplot(
  ax = axes11[1,1],
  data = onp_ma,
  x = "onp_ma84",
  y = "sales"
)
_ = axes11[1,1].set_xlabel("quarterly MA")
_ = axes11[1,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(onp_ma["onp_ma84"], onp_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# Show fig11
plt.show()
plt.close("all")
```

```{python OnpMaInterpolate}

# Keep monthly onpromotion MAs as a feature
sales_covariates["onp_ma28"] = sales_covariates["onpromotion"].rolling(window = 28, center = False).mean()

# Spline interpolation for missing values
sales_covariates["onp_ma28"] = sales_covariates["onp_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Check quality of interpolation
_ = sales_covariates["onpromotion"].plot()
_ = sales_covariates["onp_ma28"].plot()
_ = plt.title("Onpromotion change and its 28-day moving average, first 28 MAs interpolated")
plt.show()
plt.close("all")
```

### Transactions features

```{python TransCCF}
#| echo: false

# Cross-correlation of sales & transactions change
_ = plt.xcorr(
  sales_covariates["sales"], sales_covariates["transactions"], usevlines=True, maxlags=56, normed=True)
_ = plt.grid(True)
_ = plt.ylim([-0.15, 0.15])
_ = plt.axvline(x = -14, color = "red", linestyle = "dashed")
_ = plt.xlabel("transactions lags / leads")
_ = plt.title("Cross-correlation, decomposed sales & transactions change")
plt.show()
plt.close("all")
```

```{python TransLags}
#| echo: false

# FIG14: Transactions lags 0-3
fig14, axes14 = plt.subplots(2,2, sharey=True)
_ = fig14.suptitle("Transactions change lags & decomposed sales")

# Lag 0
_ = sns.regplot(
  ax = axes14[0,0],
  x = sales_covariates["transactions"],
  y = sales_covariates["sales"]
)
_ = axes14[0,0].set_xlabel("lag 0")
_ = axes14[0,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(sales_covariates["transactions"], sales_covariates["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )
      
# Lag 1
_ = sns.regplot(
  ax = axes14[0,1],
  x = sales_covariates["transactions"].shift(1),
  y = sales_covariates["sales"]
)
_ = axes14[0,1].set_xlabel("lag 1")
_ = axes14[0,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(sales_covariates["transactions"].shift(1), sales_covariates["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# Lag 2
_ = sns.regplot(
  ax = axes14[1,0],
  x = sales_covariates["transactions"].shift(2),
  y = sales_covariates["sales"]
)
_ = axes14[1,0].set_xlabel("lag 2")
_ = axes14[1,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(sales_covariates["transactions"].shift(2), sales_covariates["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# Lag 3
_ = sns.regplot(
  ax = axes14[1,1],
  x = sales_covariates["transactions"].shift(3),
  y = sales_covariates["sales"]
)
_ = axes14[1,1].set_xlabel("lag 3")
_ = axes14[1,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(sales_covariates["onpromotion"].shift(3), sales_covariates["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# Show fig14
plt.show()
plt.close("all")

```

```{python TransMAs}
#| echo: true

# FIG13: Transactions MAs
fig13, axes13 = plt.subplots(2,2)
_ = fig13.suptitle("Transactions change moving averages & decomposed sales")

# Calculate MAs without min_periods = 1
trns_ma = sales_covariates.assign(
  trns_ma7 = lambda x: x["transactions"].rolling(window = 7, center = False).mean(),
  trns_ma14 = lambda x: x["transactions"].rolling(window = 14, center = False).mean(),
  trns_ma28 = lambda x: x["transactions"].rolling(window = 28, center = False).mean(),
  trns_ma84 = lambda x: x["transactions"].rolling(window = 84, center = False).mean()
)

# MA7
_ = sns.regplot(
  ax = axes13[0,0],
  data = trns_ma,
  x = "trns_ma7",
  y = "sales"
)
_ = axes13[0,0].set_xlabel("weekly MA")
_ = axes13[0,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(trns_ma["trns_ma7"], trns_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA14
_ = sns.regplot(
  ax = axes13[0,1],
  data = trns_ma,
  x = "trns_ma14",
  y = "sales"
)
_ = axes13[0,1].set_xlabel("biweekly MA")
_ = axes13[0,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(trns_ma["trns_ma14"], trns_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA28
_ = sns.regplot(
  ax = axes13[1,0],
  data = trns_ma,
  x = "trns_ma28",
  y = "sales"
)
_ = axes13[1,0].set_xlabel("monthly MA")
_ = axes13[1,0].annotate(
    'Corr={:.2f}'.format(
      spearmanr(trns_ma["trns_ma28"], trns_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# MA84
_ = sns.regplot(
  ax = axes13[1,1],
  data = trns_ma,
  x = "trns_ma84",
  y = "sales"
)
_ = axes13[1,1].set_xlabel("quarterly MA")
_ = axes13[1,1].annotate(
    'Corr={:.2f}'.format(
      spearmanr(trns_ma["trns_ma84"], trns_ma["sales"], nan_policy='omit')[0]
      ), 
      xy=(.6, .9), xycoords="axes fraction", bbox=dict(alpha=0.5)
      )

# Show fig13
plt.show()
plt.close("all")
```

```{python TransMAInterpolate}

# Keep weekly transactions MA
sales_covariates["trns_ma7"] = sales_covariates["transactions"].rolling(window = 7, center = False).mean()

# Backwards linear interpolation
sales_covariates["trns_ma7"] = sales_covariates["trns_ma7"].interpolate("linear", limit_direction = "backward")

# Check quality of interpolation
_ = sales_covariates["transactions"].plot()
_ = sales_covariates["trns_ma7"].plot()
_ = plt.title("Transactions change and its 7-day moving average, first 7 MAs interpolated")
plt.show()
plt.close("all")
```

## Model 2 - Lags & covariates

### Preprocessing

```{python Model2Covars}

# Drop original covariates & target
lags_covars = sales_covariates.drop(
  ['oil', 'onpromotion', 'transactions', 'sales'], axis = 1)

# Make Darts TS with covariates
ts_lagscovars = TimeSeries.from_dataframe(
  lags_covars, freq="D")

# Make Darts TS with decomposed sales
ts_decomp = TimeSeries.from_series(sales_decomp.rename("sales"), freq="D")

# Make Darts TS with model 1 predictions
ts_preds1 = TimeSeries.from_series(preds_model1.rename("sales"), freq="D")

# Fill gaps in TS
ts_decomp = na_filler.transform(ts_decomp)
ts_lagscovars = na_filler.transform(ts_lagscovars)

# Train-test split (pre-post 2017)
y_train2, y_val2 = ts_decomp[:-227], trafo_log(ts_sales[-227:])
x_train2, x_val2 = ts_lagscovars[:-227], ts_lagscovars[-227:]
```

```{python ScaleCovars}

# Scale covariates
from sklearn.preprocessing import StandardScaler
from darts.dataprocessing.transformers import Scaler
scaler = Scaler(StandardScaler())
x_train2 = scaler.fit_transform(x_train2)
x_val2 = scaler.transform(x_val2)
```

### Model specification

```{python ModelSpec2}

# Import models
from darts.models.forecasting.baselines import NaiveDrift, NaiveSeasonal
from darts.models.forecasting.arima import ARIMA
from darts.models.forecasting.linear_regression_model import LinearRegressionModel
from darts.models.forecasting.random_forest import RandomForest

# Specify baseline models
model_drift = NaiveDrift()
model_seasonal = NaiveSeasonal()

# Specify ARIMA model
model_arima = ARIMA(p = 1, d = 0, q = 0, trend = "n", random_state = 1923)

# Specify second linear regression model
model_linear2 = LinearRegressionModel(
  lags = 1,
  lags_future_covariates = [0])

# Specify random forest model  
model_forest = RandomForest(
  lags = 1,
  lags_future_covariates = [0], 
  random_state = 1923,
  n_jobs = 3)
```

### Model validation: Predicting 2017 sales

```{python 2017Pred2}
#| include: false

# Fit models on train data (pre-2017), predict validation data (2017)
model_drift.fit(y_train2)
pred_drift2 = model_drift.predict(n = 227) + ts_preds1[-227:]

model_seasonal.fit(y_train2)
pred_seasonal2 = model_seasonal.predict(n = 227) + ts_preds1[-227:]

model_arima.fit(y_train2)
pred_arima = model_arima.predict(n = 227) + ts_preds1[-227:]

model_linear2.fit(y_train2, future_covariates = x_train2)
pred_linear2 = model_linear2.predict(
  n = 227, future_covariates = x_val2) + ts_preds1[-227:]

model_forest.fit(y_train2, future_covariates = x_train2)
pred_forest = model_forest.predict(
  n = 227, future_covariates = x_val2) + ts_preds1[-227:]
```

```{python 2017Score2}

# Score models' performance
perf_scores(y_val2, pred_drift2, model="Naive drift")
perf_scores(y_val2, pred_seasonal2, model="Naive seasonal")
perf_scores(y_val2, pred_arima, model="ARIMA")
perf_scores(y_val2, pred_linear2, model="Linear")
perf_scores(y_val2, pred_forest, model="Random forest")
```

```{python 2017Plot2}
#| echo: false

# FIG15: Plot models' predictions against actual values
fig15, axes15 = plt.subplots(3, sharex=True, sharey=True)
_ = fig15.suptitle("Actual vs. predicted sales, hybrid models, orange = Actual")

# ARIMA
_ = trafo_exp(y_val2).plot(ax = axes15[0], label="Actual")
_ = trafo_exp(pred_arima).plot(ax = axes15[0], label="Predicted")
_ = axes15[0].set_title("Linear regression + ARIMA")

# Linear regression
_ = trafo_exp(y_val2).plot(ax = axes15[1], label="Actual")
_ = trafo_exp(pred_linear2).plot(ax = axes15[1], label="Predicted")
_ = axes15[1].set_title("Linear regression + Linear regression")

# Random forest
_ = trafo_exp(y_val2).plot(ax = axes15[2], label="Actual")
_ = trafo_exp(pred_forest).plot(ax = axes15[2], label="Predicted")
_ = axes15[2].legend("", frameon = False)
_ = axes15[2].set_title("Linear regression + Random forest")

# Show fig15
plt.show()
plt.close("all")
```

### Backtesting / Historical forecasts

```{python HistFore2}

# Retrieve historical forecasts & residuals for linear + random forest

# Fit scaler on 2013
scaler.fit(ts_lagscovars[:365])

# Predict historical forecasts for 2014-2017
pred_hist2 = model_forest.historical_forecasts(
  trafo_log(ts_decomp), 
  future_covariates = scaler.transform(ts_lagscovars), start = 365, stride = 1,
  verbose = True) + ts_preds1[365:]

# Retrieve residuals for 2014-2017
res_hist2 = trafo_log(ts_sales[365:]) - pred_hist2

# Score historical forecasts for linear + random_forest
perf_scores(trafo_log(ts_sales[365:]), pred_hist2, model="Linear + random forest")
```

```{python HistForePlot2}
#| echo: false

# Plot historical forecasts for random forest
_ = ts_sales.plot(label="Actual")
_ = trafo_exp(pred_hist2).plot(label="Predicted")
_ = plt.title("Linear regression + random forest hybrid model, historical forecasts")
_ = plt.ylabel("sales")
plt.show()
plt.close("all")
```

### Residuals diagnosis

```{python DiagModel2}

# Residuals diagnosis
_ = plot_residuals_analysis(res_hist2)
plt.show()
plt.close("all")

# PACF plot
_ = plot_pacf(res_hist2, max_lag=56)
_ = plt.title("Partial autocorrelation plot, hybrid model innovation residuals")
_ = plt.xlabel("Lags")
_ = plt.ylabel("PACF")
_ = plt.xticks(np.arange(0, 56, 10))
_ = plt.xticks(np.arange(0, 56, 1), minor=True)
_ = plt.grid(which='minor', alpha=0.5)
_ = plt.grid(which='major', alpha=1)
plt.show()
plt.close("all")
```

```{python StatTestModel2}

# Stationarity tests on hybrid model residuals
stationarity_test_kpss(res_hist2)
stationarity_test_adf(res_hist2)
```

## Conclusion
