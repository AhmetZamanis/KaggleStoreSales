---
title: "Time series regression - Store sales forecasting, Part 1"
author: "Ahmet Zamanis"
format: ipynb
editor: visual
jupyter: python3
toc: true
execute:
  warning: false
---

## Introduction

This is a report on time series analysis & regression modeling, performed in Python, mainly with the [Darts](https://unit8co.github.io/darts/) library. The dataset is from the [Kaggle Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting) competition. The data consists of daily sales data for an Ecuadorian supermarket chain between 2013 and 2017. This is Part 1 of the analysis, which will focus only on forecasting the daily national sales of the chain, across all stores and categories. In Part 2, I will forecast the sales in each category - store combination as required by the competition, and attempt various hierarchical reconciliation techniques.

The main information source used extensively for this analysis is the textbook [Forecasting: Principles and Practice](https://otexts.com/fpp3/), written by Rob Hyndman and George Athanasopoulos. The book is the most complete source on time series analysis & forecasting I could find. It uses R and the [tidyverts](https://tidyverts.org/) libraries in its example code.

```{python Settings}

# Import libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set printing options
np.set_printoptions(suppress=True, precision=4)
pd.options.display.float_format = '{:.4f}'.format
pd.set_option('display.max_columns', None)

# Set plotting options
'exec(%matplotlib inline)'
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams["figure.autolayout"] = True
sns.set_theme(context="paper")
```

## Data preparation

The data is split into several .csv files.

```{python Load data}
#| include: false

# Load original datasets
df_train = pd.read_csv("./OriginalData/train.csv", encoding="utf-8")
df_stores = pd.read_csv("./OriginalData/stores.csv", encoding="utf-8")
df_oil = pd.read_csv("./OriginalData/oil.csv", encoding="utf-8")
df_holidays = pd.read_csv("./OriginalData/holidays_events.csv", encoding="utf-8")
df_trans = pd.read_csv("./OriginalData/transactions.csv", encoding="utf-8")
```

**.csv** and **test.csv** are the main datasets, consisting of daily sales data. The ing data ranges from 01-01-2013 to 15-08-2017, and the testing data consists of the next 15 days, in August 2017. We won't do a competition submission in Part 1, so we won't load the testing data.

```{python df_}

# View the daily sales data
df_train.head(5)
```

-   For each day, we have the sales in each store (out of a possible 54) and each product category (out of a possible 33). This amounts to 1782 time series that need to be forecasted for the competition, but in Part 1 of this analysis, we will keep it simple and only forecast the national sales in each day, in all categories.

-   **onpromotion** is the number of items on sale that day, in that category & store.

**stores.csv** contains more information about each store: The city, state, store type and store cluster.

```{python df_stores}

# View the stores data
df_stores.head(5)
```

**holidays.csv** contains information about local (city-wide), regional (state-wide) and national holidays, as well as some special nation-wide events in the time period. We will use these along with the stores' location data to create calendar features.

```{python df_holidays}

# View the holidays data
df_holidays.head(5)
```

**oil.csv** consists of the daily oil prices in the time period. Ecuador has an oil-dependent economy, so this may be a useful predictor of the cyclicality in supermarket sales.

```{python df_oil}

# View the oil data
df_oil.head(5)
```

**transactions.csv** consists of the daily number of transactions in a store. Another potentially useful feature.

```{python df_trans}

df_trans.head(5)
```

We will rename some columns from the datasets and merge the supplementary information into the ing data. We'll aggregate daily transactions across all stores beforehand, as we are only interested in predicting daily national sales.

```{python MergeData}
#| output: false

# Rename columns
df_holidays = df_holidays.rename(columns = {"type":"holiday_type"})
df_oil = df_oil.rename(columns = {"dcoilwtico":"oil"})
df_stores = df_stores.rename(columns = {
  "type":"store_type", "cluster":"store_cluster"})

# Aggregate daily transactions across all stores
df_trans = df_trans.groupby("date").transactions.sum()

# Add columns from oil, stores and transactions datasets into main data
df_train = df_train.merge(df_trans, on = ["date"], how = "left")
df_train = df_train.merge(df_oil, on = "date", how = "left")
df_train = df_train.merge(df_stores, on = "store_nbr", how = "left")

```

Incorporating the holidays information into the sales dataset will require more work.

```{python SplitHolidays}

# Split holidays data into local, regional, national and events
events = df_holidays[df_holidays["holiday_type"] == "Event"]
df_holidays = df_holidays.drop(labels=(events.index), axis=0)
local = df_holidays.loc[df_holidays["locale"] == "Local"]
regional = df_holidays.loc[df_holidays["locale"] == "Regional"]
national = df_holidays.loc[df_holidays["locale"] == "National"]

```

There are cases of multiple holidays or events sharing the same date and locale. We'll inspect the duplicates and drop them, so we don't have issues in feature engineering.

-   Rows with **transferred = True** are dates that are normally holidays, but the holiday was transferred to another day. In other words, these are not holidays in effect.

-   Rows with **holiday_type = Transfer** are dates that are not normally holidays, but had another holiday transferred. In other words, these are holidays in effect.

-   Rows with **holiday_type = Bridge** are dates that are not normally holidays, but were added to preceding / following holidays.

```{python Duplicates1}

# Inspect local holidays sharing same date & locale. Drop the transfer row
local[local.duplicated(["date", "locale_name"], keep = False)]
local = local.drop(265, axis = 0)
```

```{python Duplicates2}

# Inspect regional holidays sharing same date & locale. None exist
regional[regional.duplicated(["date", "locale_name"], keep = False)]
```

```{python Duplicates3}

# Inspect national holidays sharing same date & locale. Drop bridge days
national[national.duplicated(["date"], keep = False)]
national = national.drop([35, 39, 156], axis = 0)
```

```{python Duplicates4}

# Inspect events sharing same date. Drop the earthquake row as it is a one-time event
events[events.duplicated(["date"], keep = False)]
events = events.drop(244, axis = 0)
```

After getting rid of duplicates, we can create binary columns that signify whether a date was a local / regional / national holiday / event. We'll merge these back into the sales data.

```{python HolidayEventDummies}
#| include: false

# Add local_holiday binary column to local holidays data, to be merged into main 
# data.
local["local_holiday"] = (
  local.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((local.holiday_type == "Holiday") & (local.transferred == False))
).astype(int)

# Add regional_holiday binary column to regional holidays data
regional["regional_holiday"] = (
  regional.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((regional.holiday_type == "Holiday") & (regional.transferred == False))
).astype(int)

# Add national_holiday binary column to national holidays data
national["national_holiday"] = (
  national.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((national.holiday_type == "Holiday") & (national.transferred == False))
).astype(int)

# Add event column to events
events["event"] = 1

# Merge local holidays binary column to main data, on date and city
local_merge = local.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"city"})
df_train = df_train.merge(local_merge, how="left", on=["date", "city"])
df_train["local_holiday"] = df_train["local_holiday"].fillna(0).astype(int)

# Merge regional holidays binary column to main data
regional_merge = regional.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"state"})
df_train = df_train.merge(regional_merge, how="left", on=["date", "state"])
df_train["regional_holiday"] = df_train["regional_holiday"].fillna(0).astype(int)

# Merge national holidays binary column to main data, on date
national_merge = national.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df_train = df_train.merge(national_merge, how="left", on="date")
df_train["national_holiday"] = df_train["national_holiday"].fillna(0).astype(int)

# Merge events binary column to main data
events_merge = events.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df_train = df_train.merge(events_merge, how="left", on="date")
df_train["event"] = df_train["event"].fillna(0).astype(int)

```

We'll set the **date** column to a DateTimeIndex, and view the sales data with the added columns.

```{python DatetimeIndex}

# Set datetime index
df_train = df_train.set_index(pd.to_datetime(df_train.date))
df_train = df_train.drop("date", axis=1)
df_train.head(5)

```

With financial data, it's a good idea to normalize for inflation. We'll CPI adjust the sales and oil prices columns, with 2010 as our base year. The CPI values for Ecuador in the time period were retrieved [here](https://data.worldbank.org/indicator/FP.CPI.TOTL?end=2017&locations=EC&start=2010&view=chart).

-   We'll use the yearly CPI values for simplicity's sake, but it's possible to use monthly CPI for more accuracy.

-   Since 2017 is not complete in the data, and we'll use it as the validation-testing period, we'll use 2016's CPI for 2017 to avoid leaking information from the future into our predictions.

```{python CPIAdjust}
#| include: false

# CPI adjust sales and oil, with CPI 2010 = 100
cpis = {
  "2010":100, "2013":112.8, "2014":116.8, "2015":121.5, "2016":123.6, 
  "2017":123.6
  }
  
for year in [2013, 2014, 2015, 2016, 2017]:
  df_train["sales"].loc[df_train.index.year==year] = df_train["sales"].loc[
    df_train.index.year==year] / cpis[str(year)] * cpis["2010"]
  df_train["oil"].loc[df_train.index.year==year] = df_train["oil"].loc[
    df_train.index.year==year] / cpis[str(year)] * cpis["2010"]

```

We have some rows with missing values in our training data.

```{python NACheck}

# Check missing values in each column
pd.isnull(df_train).sum()
```

We will interpolate the missing values in the oil and transactions columns using time interpolation. This performs linear interpolation, but also takes the date-time index of observations into account.

```{python NAInterpolate}

df_train["oil"] = df_train["oil"].interpolate("time", limit_direction = "both")
df_train["transactions"] = df_train["transactions"].interpolate("time", limit_direction = "both")
```

We will now aggregate daily sales across all categories and stores, to retrieve our target variable.

```{python AggSales}

sales = df_train.groupby("date").sales.sum()
sales
```

We will create a Darts TimeSeries with our target variable.

```{python TSTarget}

from darts import TimeSeries
ts_sales = TimeSeries.from_series(sales, freq="D")
ts_sales

```

-   Each Pandas series / dataframe column passed is stored as a component in the Darts TS. The date-time index is stored in **time_index.**

-   To create a multivariate time series from a Pandas dataframe, we create a dataframe with each time series as a column, and a common date-time index. If the time series have a **hierarchy**, i.e. if they sum up together in a certain way, we can map that hierarchy as a dictionary. We will explore this further in part 2 of the analysis.

-   Static covariates are time-invariant covariates that may be used in predictions. In our case, the city or cluster of a store may be static covariates, but for part 1 of our analysis, we won't use these.

## Overview of hybrid modeling approach 

A time series can be written as the sum of several components:

-   **Trend:** The long-term change.

-   **Seasonality:** A fluctuation that repeats based on a fixed, known time period. For example, the effect of month / season on ice cream sales.

-   **Cyclicality:** A fluctuation that does not repeat on a fixed, known time period. For example, the effect of oil prices on car sales.

-   **Remainder / Error:** The unpredictable component of the time series, at least with the available data.

When analyzing a time series with plots, it can be difficult to determine the sources of changes and fluctuations. It can be especially tricky to tell apart the effects of covariates (cyclicality) from calendar effects (seasonality). Because of this, we will split our analysis and modeling into two steps:

-   In step 1, we will analyze the effects of trend, seasonality and calendar effects (such as holidays & events), build a model that predicts these effects and removes them from the series. This is called **time decomposition.**

-   In step 2, we will re-analyze the decomposed time series, this time considering the effects of covariates and lagged values of sales itself as predictors. We'll build a model that uses these predictors, train it on the decomposed sales, and add up the predictions of both models to arrive at our final predictions. This approach is called a **hybrid model.**

## Exploratory analysis 1 - Time & calendar effects

Let's start by analyzing the overall trend in sales. Darts TimeSeries can be plotted quickly.

```{python TrendPlot}

ts_sales.plot()
plt.ylabel("Daily sales, millions")
```

## Feature engineering 1 - Time & calendar features

## Model 1 - Time effects decomposition

## Exploratory analysis 2 - Lags & covariates

## Feature engineering 2 - Lags & covariates

## Model 2 - Lags & covariates

## Conclusion
