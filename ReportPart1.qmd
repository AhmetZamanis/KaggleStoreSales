---
title: "Time series regression - Store sales forecasting, Part 1"
author: "Ahmet Zamanis"
format: ipynb
  toc: true
editor: visual
jupyter: python3
execute:
  warning: false
  message: false
---

## Introduction

This is a report on time series analysis & regression modeling, performed in Python, mainly with the [Darts](https://unit8co.github.io/darts/) library. The dataset is from the [Kaggle Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting) competition. The data consists of daily sales data for an Ecuadorian supermarket chain between 2013 and 2017. This is Part 1 of the analysis, which will focus only on forecasting the daily national sales of the chain, across all stores and categories. In Part 2, I will forecast the sales in each category - store combination as required by the competition, and attempt various hierarchical reconciliation techniques.

The main information source used extensively for this analysis is the textbook [Forecasting: Principles and Practice](https://otexts.com/fpp3/), written by Rob Hyndman and George Athanasopoulos. The book is the most complete source on time series analysis & forecasting I could find. It uses R and the [tidyverts](https://tidyverts.org/) libraries in its example code.

```{python Settings}
#| echo: false

# Import libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set printing options
np.set_printoptions(suppress=True, precision=4)
pd.options.display.float_format = '{:.4f}'.format
pd.set_option('display.max_columns', None)

# Set plotting options
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams["figure.autolayout"] = True
sns.set_theme(context="paper")

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')
```

## Data preparation

The data is split into several .csv files.

```{python Load data}
#| include: false

# Load original datasets
df_train = pd.read_csv("./OriginalData/train.csv", encoding="utf-8")
df_stores = pd.read_csv("./OriginalData/stores.csv", encoding="utf-8")
df_oil = pd.read_csv("./OriginalData/oil.csv", encoding="utf-8")
df_holidays = pd.read_csv("./OriginalData/holidays_events.csv", encoding="utf-8")
df_trans = pd.read_csv("./OriginalData/transactions.csv", encoding="utf-8")
```

**.csv** and **test.csv** are the main datasets, consisting of daily sales data. The ing data ranges from 01-01-2013 to 15-08-2017, and the testing data consists of the next 15 days, in August 2017. We won't do a competition submission in Part 1, so we won't load the testing data.

```{python df}

# View the daily sales data
df_train.head(5)
```

-   For each day, we have the sales in each store (out of a possible 54) and each product category (out of a possible 33). This amounts to 1782 time series that need to be forecasted for the competition, but in Part 1 of this analysis, we will keep it simple and only forecast the national sales in each day, in all categories.

-   **onpromotion** is the number of items on sale that day, in that category & store.

**stores.csv** contains more information about each store: The city, state, store type and store cluster.

```{python df_stores}

# View the stores data
df_stores.head(5)
```

**holidays.csv** contains information about local (city-wide), regional (state-wide) and national holidays, as well as some special nation-wide events in the time period. We will use these along with the stores' location data to create calendar features.

```{python df_holidays}

# View the holidays data
df_holidays.head(5)
```

**oil.csv** consists of the daily oil prices in the time period. Ecuador has an oil-dependent economy, so this may be a useful predictor of the cyclicality in supermarket sales.

```{python df_oil}

# View the oil data
df_oil.head(5)
```

**transactions.csv** consists of the daily number of transactions in a store. Another potentially useful feature.

```{python df_trans}

df_trans.head(5)
```

We will rename some columns from the datasets and merge the supplementary information into the ing data. We'll aggregate daily transactions across all stores beforehand, as we are only interested in predicting daily national sales.

```{python MergeData}
#| output: false

# Rename columns
df_holidays = df_holidays.rename(columns = {"type":"holiday_type"})
df_oil = df_oil.rename(columns = {"dcoilwtico":"oil"})
df_stores = df_stores.rename(columns = {
  "type":"store_type", "cluster":"store_cluster"})

# Aggregate daily transactions across all stores
df_trans = df_trans.groupby("date").transactions.sum()

# Add columns from oil, stores and transactions datasets into main data
df_train = df_train.merge(df_trans, on = ["date"], how = "left")
df_train = df_train.merge(df_oil, on = "date", how = "left")
df_train = df_train.merge(df_stores, on = "store_nbr", how = "left")

```

Incorporating the holidays information into the sales dataset will require more work.

```{python SplitHolidays}

# Split holidays data into local, regional, national and events
events = df_holidays[df_holidays["holiday_type"] == "Event"]
df_holidays = df_holidays.drop(labels=(events.index), axis=0)
local = df_holidays.loc[df_holidays["locale"] == "Local"]
regional = df_holidays.loc[df_holidays["locale"] == "Regional"]
national = df_holidays.loc[df_holidays["locale"] == "National"]

```

There are cases of multiple holidays or events sharing the same date and locale. We'll inspect the duplicates and drop them, so we don't have issues in feature engineering.

-   Rows with **transferred = True** are dates that are normally holidays, but the holiday was transferred to another day. In other words, these are not holidays in effect.

-   Rows with **holiday_type = Transfer** are dates that are not normally holidays, but had another holiday transferred. In other words, these are holidays in effect.

-   Rows with **holiday_type = Bridge** are dates that are not normally holidays, but were added to preceding / following holidays.

```{python Duplicates1}

# Inspect local holidays sharing same date & locale. Drop the transfer row
local[local.duplicated(["date", "locale_name"], keep = False)]
local = local.drop(265, axis = 0)
```

```{python Duplicates2}

# Inspect regional holidays sharing same date & locale. None exist
regional[regional.duplicated(["date", "locale_name"], keep = False)]
```

```{python Duplicates3}

# Inspect national holidays sharing same date & locale. Drop bridge days
national[national.duplicated(["date"], keep = False)]
national = national.drop([35, 39, 156], axis = 0)
```

```{python Duplicates4}

# Inspect events sharing same date. Drop the earthquake row as it is a one-time event
events[events.duplicated(["date"], keep = False)]
events = events.drop(244, axis = 0)
```

After getting rid of duplicates, we can create binary columns that signify whether a date was a local / regional / national holiday / event. We'll merge these back into the sales data.

```{python HolidayEventDummies}
#| include: false

# Add local_holiday binary column to local holidays data, to be merged into main 
# data.
local["local_holiday"] = (
  local.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((local.holiday_type == "Holiday") & (local.transferred == False))
).astype(int)

# Add regional_holiday binary column to regional holidays data
regional["regional_holiday"] = (
  regional.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((regional.holiday_type == "Holiday") & (regional.transferred == False))
).astype(int)

# Add national_holiday binary column to national holidays data
national["national_holiday"] = (
  national.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((national.holiday_type == "Holiday") & (national.transferred == False))
).astype(int)

# Add event column to events
events["event"] = 1

# Merge local holidays binary column to main data, on date and city
local_merge = local.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"city"})
df_train = df_train.merge(local_merge, how="left", on=["date", "city"])
df_train["local_holiday"] = df_train["local_holiday"].fillna(0).astype(int)

# Merge regional holidays binary column to main data
regional_merge = regional.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"state"})
df_train = df_train.merge(regional_merge, how="left", on=["date", "state"])
df_train["regional_holiday"] = df_train["regional_holiday"].fillna(0).astype(int)

# Merge national holidays binary column to main data, on date
national_merge = national.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df_train = df_train.merge(national_merge, how="left", on="date")
df_train["national_holiday"] = df_train["national_holiday"].fillna(0).astype(int)

# Merge events binary column to main data
events_merge = events.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df_train = df_train.merge(events_merge, how="left", on="date")
df_train["event"] = df_train["event"].fillna(0).astype(int)

```

We'll set the **date** column to a DateTimeIndex, and view the sales data with the added columns.

```{python DatetimeIndex}

# Set datetime index
df_train = df_train.set_index(pd.to_datetime(df_train.date))
df_train = df_train.drop("date", axis=1)
df_train.head(5)

```

With financial data, it's a good idea to normalize for inflation. We'll CPI adjust the sales and oil prices columns, with 2010 as our base year. The CPI values for Ecuador in the time period were retrieved [here](https://data.worldbank.org/indicator/FP.CPI.TOTL?end=2017&locations=EC&start=2010&view=chart).

-   We'll use the yearly CPI values for simplicity's sake, but it's possible to use monthly CPI for more accuracy.

-   Since 2017 is not complete in the data, and we'll use it as the validation-testing period, we'll use 2016's CPI for 2017 to avoid leaking information from the future into our predictions.

```{python CPIAdjust}
#| include: false

# CPI adjust sales and oil, with CPI 2010 = 100
cpis = {
  "2010":100, "2013":112.8, "2014":116.8, "2015":121.5, "2016":123.6, 
  "2017":123.6
  }
  
for year in [2013, 2014, 2015, 2016, 2017]:
  df_train["sales"].loc[df_train.index.year==year] = df_train["sales"].loc[
    df_train.index.year==year] / cpis[str(year)] * cpis["2010"]
  df_train["oil"].loc[df_train.index.year==year] = df_train["oil"].loc[
    df_train.index.year==year] / cpis[str(year)] * cpis["2010"]

```

We have some rows with missing values in our training data.

```{python NACheck}

# Check missing values in each column
pd.isnull(df_train).sum()
```

We will interpolate the missing values in the oil and transactions columns using time interpolation. This performs linear interpolation, but also takes the date-time index of observations into account.

```{python NAInterpolate}

df_train["oil"] = df_train["oil"].interpolate("time", limit_direction = "both")
df_train["transactions"] = df_train["transactions"].interpolate("time", limit_direction = "both")
```

We will now aggregate daily sales across all categories and stores, to retrieve our target variable.

```{python AggSales}

sales = df_train.groupby("date").sales.sum()
sales
```

We will create a Darts TimeSeries with our target variable.

```{python TSTarget}

from darts import TimeSeries
ts_sales = TimeSeries.from_series(sales, freq="D")
ts_sales

```

-   Each Pandas series / dataframe column passed is stored as a component in the Darts TS. The date-time index is stored in **time_index.**

-   To create a multivariate time series from a Pandas dataframe, we create a dataframe with each time series as a column, and a common date-time index. If the time series have a **hierarchy**, i.e. if they sum up together in a certain way, we can map that hierarchy as a dictionary. We will explore this further in part 2 of the analysis.

-   Static covariates are time-invariant covariates that may be used in predictions. In our case, the city or cluster of a store may be static covariates, but for part 1 of our analysis, we won't use these.

## Overview of hybrid modeling approach

A time series can be written as the sum of several components:

-   **Trend:** The long-term change.

-   **Seasonality:** A fluctuation that repeats based on a fixed, known time period. For example, the effect of month / season on ice cream sales.

-   **Cyclicality:** A fluctuation that does not repeat on a fixed, known time period. For example, the effect of oil prices on car sales.

-   **Remainder / Error:** The unpredictable component of the time series, at least with the available data.

When analyzing a time series with plots, it can be difficult to determine the sources of changes and fluctuations. It can be especially tricky to tell apart the cyclical effects from seasonality. Because of this, we will split our analysis and modeling into two steps:

-   In step 1, we will analyze the effects of trend, seasonality and calendar effects (such as holidays & events), build a model that predicts these effects and remove them from the series. This is called **time decomposition.**

-   In step 2, we will re-analyze the decomposed time series, this time considering the effects of covariates and lagged values of sales itself as predictors, to try and account for the cyclicality. We'll build a model that uses these predictors, train it on the decomposed sales, and add up the predictions of both models to arrive at our final predictions. This approach is called a **hybrid model.**

## Exploratory analysis 1 - Time & calendar effects

### Trend

Let's start by analyzing the overall trend in sales. Darts offers the ability to plot time series quickly.

```{python TrendPlot}

_ =  ts_sales.plot()
_ =  plt.ylabel("Daily sales, millions")
plt.show()
plt.close()
```

The series plot shows us several things:

-   Supermarket sales show an increasing trend over the years. The trend is close to linear overall, but the rate of increase declines roughly from the start of 2015.

-   Sales mostly fluctuate around a certain range, which suggests strong seasonality. However, there are also sharp deviations in certain periods, mainly across 2014 and at the start of 2015. This is likely cyclical in nature.

-   The seasonal fluctuations seem to be getting stronger over time. This suggests we should use a multiplicative time decomposition instead of additive.

-   Sales decline very sharply in the first day of every year, likely because it's a holiday.

### Seasonality

#### Annual seasonality

Let's look at annual seasonality: How sales fluctuate over a year based on quarters, months, weeks of a year and days of a year. In the plots below, we have the daily sales averaged by each respective calendar period, colored by each year in the data. The confidence bands indicate the minimum and maximum daily sales in each respective period (in the last plot, we just have the daily sales without any averaging).

```{python AnnualSeasonality}
#| echo: false

# FIG1: Annual seasonality, period averages
fig1, axes1 = plt.subplots(2,2, sharey=True)
_ =  fig1.suptitle('Annual seasonality: Average daily sales in given time periods, millions')

# Average sales per quarter of year
_ =  sns.lineplot(
  ax = axes1[0,0],
  x = sales.index.quarter.astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ =  axes1[0,0].set_xlabel("quarter", fontsize=8)
_ =  axes1[0,0].set_ylabel("sales", fontsize=8)
_ =  axes1[0,0].tick_params(axis='both', which='major', labelsize=6)

# Average sales per month of year
_ =  sns.lineplot(
  ax = axes1[0,1],
  x = sales.index.month.astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales)
_ =  axes1[0,1].set_xlabel("month", fontsize=8)
_ =  axes1[0,1].set_ylabel("sales",fontsize=8)
_ =  axes1[0,1].legend(title = "year", bbox_to_anchor=(1.05, 1.0), fontsize="small", loc='best')
_ =  axes1[0,1].tick_params(axis='both', which='major', labelsize=6)

# Average sales per week of year
_ =  sns.lineplot(
  ax = axes1[1,0],
  x = sales.index.week, 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ =  axes1[1,0].set_xlabel("week of year", fontsize=8)
_ =  axes1[1,0].set_ylabel("sales",fontsize=8)
_ =  axes1[1,0].tick_params(axis='both', which='major', labelsize=6)
_ =  axes1[1,0].xaxis.set_ticks(np.arange(0, 52, 10))

# Average sales per day of year
_ =  sns.lineplot(
  ax = axes1[1,1],
  x = sales.index.dayofyear, 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ =  axes1[1,1].set_xlabel("day of year", fontsize=8)
_ =  axes1[1,1].set_ylabel("sales",fontsize=8)
_ =  axes1[1,1].tick_params(axis='both', which='major', labelsize=6)
_ =  axes1[1,1].xaxis.set_ticks(np.arange(0, 365, 100))

# Show fig1
plt.show()
plt.close("all")
```

-   **Quarterly:** Sales do not seem to have a considerable quarterly seasonality pattern. However, the plot still shows us a few things:

    -   Sales generally slightly increase over a year.

    -   In Q2 2014, there was a considerable drop. Sales declined almost to the level of Q2 2013. This was likely a cyclical effect.

-   **Monthly:** Sales do seem to fluctuate slightly over months, but there's no clear seasonal pattern that holds across all years. However, sales seem to sharply increase in December every year, likely due to Christmas.

    -   The cyclicality in 2014 is seen in more detail: Sales dropped almost to their 2013 levels in certain months, and recovered sharply in others.

    -   We also see a considerable drop in the first half of 2015, where sales dropped roughly to 2014 levels, followed by a recovery.

    -   There is a very sharp increase in April 2016, where sales were even higher than 2017 levels. This is due to a large earthquake that happened in April 16, 2016, and the related relief efforts.

-   **Weekly:** The seasonal patterns are more visible in the weekly plot, as we see the "waves" line up across years. It's very likely the data has strong weekly seasonality, which is what we'd expect from supermarket sales.

    -   The data for 2017 ends after August 15, so the sharp decline afterwards is misleading.

    -   The decline at the end of 2016 is also misleading, as 2016 was a 366-day year.

-   **Daily:** This plot is a bit noisy, but the very similar fluctuations across all years indicate the data is strongly seasonal. It also highlights some cyclical effects such as the April 2016 earthquake and the 2014 drops.

Another way to look at annual seasonality is to average sales in a certain calendar period across all years, without grouping by year. This shows us the "overall" seasonality pattern across one year: We likely have strong weekly seasonality that persists over years, and some monthly seasonality especially towards December.

```{python AnnualSeasonalityAgg}
#| echo: false

# FIG1.1: Annual seasonality, averaged over years
fig11, axes11 = plt.subplots(2,2, sharey=True)
_ = fig11.suptitle('Annual seasonality: Average daily sales in given time periods,\n across all years, millions');

# Average sales per quarter of year
_ = sns.lineplot(
  ax = axes11[0,0],
  x = sales.index.quarter.astype(str), 
  y = (sales / 1000000), 
  data=sales, legend=False)
_ = axes11[0,0].set_xlabel("quarter", fontsize=8)
_ = axes11[0,0].set_ylabel("sales", fontsize=8)
_ = axes11[0,0].tick_params(axis='both', which='major', labelsize=6)

# Average sales per month of year
_ = sns.lineplot(
  ax = axes11[0,1],
  x = sales.index.month.astype(str), 
  y = (sales / 1000000), 
  data=sales)
_ = axes11[0,1].set_xlabel("month", fontsize=8)
_ = axes11[0,1].set_ylabel("sales",fontsize=8)
_ = axes11[0,1].tick_params(axis='both', which='major', labelsize=6)

# Average sales per week of year
_ = sns.lineplot(
  ax = axes11[1,0],
  x = sales.index.week, 
  y = (sales / 1000000), 
  data=sales, legend=False)
_ = axes11[1,0].set_xlabel("week of year", fontsize=8)
_ = axes11[1,0].set_ylabel("sales",fontsize=8)
_ = axes11[1,0].tick_params(axis='both', which='major', labelsize=6)
_ = axes11[1,0].xaxis.set_ticks(np.arange(0, 52, 10))

# Average sales per day of year
_ = sns.lineplot(
  ax = axes11[1,1],
  x = sales.index.dayofyear, 
  y = (sales / 1000000), 
  data=sales, legend=False)
_ = axes11[1,1].set_xlabel("day of year", fontsize=8)
_ = axes11[1,1].set_ylabel("sales",fontsize=8)
_ = axes11[1,1].tick_params(axis='both', which='major', labelsize=6)
_ = axes11[1,1].xaxis.set_ticks(np.arange(0, 365, 100))

# Show fig1.1
plt.show()
plt.close("all")
```

#### Monthly & weekly seasonality

Now let's look at seasonality across days of a month and days of a week. These will likely be the most important seasonality patterns in the sales. There are three ways to look at these plots: First we will group them by year.

```{python MonthlyWeeklyByYear}
#| echo: false

# FIG2: Monthly and weekly seasonality
fig2, axes2 = plt.subplots(2)
_ = fig2.suptitle('Monthly and weekly seasonality, average daily sales in millions')

# Average sales per day of month
_ = sns.lineplot(
  ax = axes2[0],
  x = sales.index.day, 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales)
_ = axes2[0].legend(title = "year", bbox_to_anchor=(1.05, 1.0), fontsize="small", loc='best')
_ = axes2[0].set_xlabel("day of month", fontsize=8)
_ = axes2[0].set_ylabel("sales", fontsize=8)
_ = axes2[0].xaxis.set_ticks(np.arange(1, 32, 6))
_ = axes2[0].xaxis.set_ticks(np.arange(1, 32, 1), minor=True)
_ = axes2[0].yaxis.set_ticks(np.arange(0, 1.25, 0.25))
_ = axes2[0].grid(which='minor', alpha=0.5)
_ = axes2[0].grid(which='major', alpha=1)

# Average sales per day of week
_ = sns.lineplot(
  ax = axes2[1],
  x = (sales.index.dayofweek+1).astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.year.astype(str), data=sales, legend=False)
_ = axes2[1].set_xlabel("day of week", fontsize=8)
_ = axes2[1].set_ylabel("sales", fontsize=8)
_ = axes2[1].yaxis.set_ticks(np.arange(0, 1.25, 0.25))

# Show fig2
plt.show()
plt.close("all")
```

We can look at the same plots grouped by month.

```{python MonthlyYearlyByMonth}
#| echo: false

# FIG2.1: Monthly and weekly seasonality, colored by month
fig21, axes21 = plt.subplots(2)
_ = fig21.suptitle('Monthly and weekly seasonality, average daily sales in millions')

# Average sales per day of month, colored by month
_ = sns.lineplot(
  ax = axes21[0],
  x = sales.index.day, 
  y = (sales / 1000000), 
  hue = sales.index.month.astype(str), data=sales, errorbar=None)
_ = axes21[0].legend(title = "month", bbox_to_anchor=(1.05, 1.0), fontsize="x-small", loc='best')
_ = axes21[0].set_xlabel("day of month", fontsize=8)
_ = axes21[0].set_ylabel("sales", fontsize=8)
_ = axes21[0].xaxis.set_ticks(np.arange(1, 32, 6))
_ = axes21[0].xaxis.set_ticks(np.arange(1, 32, 1), minor=True)
_ = axes21[0].yaxis.set_ticks(np.arange(0, 1.25, 0.25))
_ = axes21[0].grid(which='minor', alpha=0.5)
_ = axes21[0].grid(which='major', alpha=1)

# Average sales per day of week, colored by month
_ = sns.lineplot(
  ax = axes21[1],
  x = (sales.index.dayofweek+1).astype(str), 
  y = (sales / 1000000), 
  hue = sales.index.month.astype(str), data=sales, errorbar=None, legend=None)
_ = axes21[1].set_xlabel("day of week", fontsize=8)
_ = axes21[1].set_ylabel("sales", fontsize=8)
_ = axes21[1].yaxis.set_ticks(np.arange(0, 1.25, 0.25))

# Show fig2.1
plt.show()
plt.close("all")
```

And finally, without any grouping: The averages across all years.

```{python MonthlyYearly}
#| echo: false

# FIG2.2: Monthly and weekly seasonality, average across years
fig22, axes22 = plt.subplots(2)
_ =  fig22.suptitle('Monthly and weekly seasonality, averaged across years, in millions')

# Average sales per day of month
_ =  sns.lineplot(
  ax = axes22[0],
  x = sales.index.day, 
  y = (sales / 1000000), 
  data=sales)
_ =  axes22[0].set_xlabel("day of month", fontsize=8)
_ =  axes22[0].set_ylabel("sales", fontsize=8)
_ =  axes22[0].xaxis.set_ticks(np.arange(1, 32, 6))
_ =  axes22[0].xaxis.set_ticks(np.arange(1, 32, 1), minor=True)
_ =  axes22[0].yaxis.set_ticks(np.arange(0, 1.25, 0.25))
_ =  axes22[0].grid(which='minor', alpha=0.5)
_ =  axes22[0].grid(which='major', alpha=1)

# Average sales per day of week
_ =  sns.lineplot(
  ax = axes22[1],
  x = (sales.index.dayofweek+1).astype(str), 
  y = (sales / 1000000), 
  data=sales)
_ =  axes22[1].set_xlabel("day of week", fontsize=8)
_ =  axes22[1].set_ylabel("sales", fontsize=8)
_ =  axes22[1].yaxis.set_ticks(np.arange(0, 1.25, 0.25))

# Show fig22
plt.show()
plt.close("all")
```

### Autocorrelation & partial autocorrelation

```{python AcfPacfTime}
#| echo: false

# FIG3: ACF and PACF plots
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
fig3, axes3 = plt.subplots(2)
_ = fig3.suptitle('Autocorrelation and partial autocorrelation, daily sales, up to 54 days')
_ = plot_acf(sales, lags=range(0,55), ax=axes3[0])
_ = plot_pacf(sales, lags=range(0,55), ax=axes3[1], method="ywm")

# Show fig3
plt.show()
plt.close("all")
```

### April 2016 Earthquake

```{python EarthquakePlot}
#| echo: false

# FIG6: Zoom in on earthquake: 16 April 2016
april_sales = sales.loc[sales.index.month == 4]
may_sales = sales.loc[sales.index.month == 5]

fig6, axes6 = plt.subplots(2, sharex=True)
_ = fig6.suptitle("Effect of 16 April 2016 earthquake on sales")

# April
_ = sns.lineplot(
  ax = axes6[0],
  x = april_sales.index.day,
  y = april_sales,
  hue = april_sales.index.year.astype(str),
  data = april_sales
)
_ = axes6[0].set_title("April")
_ = axes6[0].legend(title = "year", bbox_to_anchor=(1.05, 1.0), fontsize="small", loc='best')
_ = axes6[0].set_xlabel("days of month")
_ = axes6[0].set_xticks(np.arange(1, 32, 6))
_ = axes6[0].set_xticks(np.arange(1, 32, 1), minor=True)
_ = axes6[0].grid(which='minor', alpha=0.5)
_ = axes6[0].grid(which='major', alpha=1)
_ = axes6[0].axvline(x = 16, color = "black", linestyle = "dashed")

# May
_ = sns.lineplot(
  ax = axes6[1],
  x = may_sales.index.day,
  y = may_sales,
  hue = may_sales.index.year.astype(str),
  data = may_sales, legend=False
)
_ = axes6[1].set_title("May")
_ = axes6[1].set_xlabel("days of month")
_ = axes6[1].set_xticks(np.arange(1, 32, 6))
_ = axes6[1].set_xticks(np.arange(1, 32, 1), minor=True)
_ = axes6[1].grid(which='minor', alpha=0.5)
_ = axes6[1].grid(which='major', alpha=1)

# Show FIG6
plt.show()
plt.close("all")
```

## Feature engineering 1 - Time & calendar features

### Calendar effects features

```{python FeatEngNY}

# New year's day features
df_train["ny1"] = ((df_train.index.day == 1) & (df_train.index.month == 1)).astype(int)


# Set holiday dummies to 0 if NY dummies are 1
df_train.loc[df_train["ny1"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0

df_train["ny2"] = ((df_train.index.day == 2) & (df_train.index.month == 1)).astype(int)

df_train.loc[df_train["ny2"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0
```

```{python FeatEngDecember}

# NY's eve features
df_train["ny_eve31"] = ((df_train.index.day == 31) & (df_train.index.month == 12)).astype(int)

df_train["ny_eve30"] = ((df_train.index.day == 30) & (df_train.index.month == 12)).astype(int)

df_train.loc[(df_train["ny_eve31"] == 1) | (df_train["ny_eve30"] == 1), ["local_holiday", "regional_holiday", "national_holiday"]] = 0


# Proximity to Christmas sales peak
df_train["xmas_before"] = 0

df_train.loc[
  (df_train.index.day.isin(range(13,24))) & (df_train.index.month == 12), "xmas_before"] = df_train.loc[
  (df_train.index.day.isin(range(13,24))) & (df_train.index.month == 12)].index.day - 12

df_train["xmas_after"] = 0
df_train.loc[
  (df_train.index.day.isin(range(24,28))) & (df_train.index.month == 12), "xmas_after"] = abs(df_train.loc[
  (df_train.index.day.isin(range(24,28))) & (df_train.index.month == 12)].index.day - 27)

df_train.loc[(df_train["xmas_before"] != 0) | (df_train["xmas_after"] != 0), ["local_holiday", "regional_holiday", "national_holiday"]] = 0
```

```{python FeatEngQuake}

# Strength of earthquake effect on sales
# April 18 > 17 > 19 > 20 > 21 > 22
df_train["quake_after"] = 0
df_train.loc[df_train.index == "2016-04-18", "quake_after"] = 6
df_train.loc[df_train.index == "2016-04-17", "quake_after"] = 5
df_train.loc[df_train.index == "2016-04-19", "quake_after"] = 4
df_train.loc[df_train.index == "2016-04-20", "quake_after"] = 3
df_train.loc[df_train.index == "2016-04-21", "quake_after"] = 2
df_train.loc[df_train.index == "2016-04-22", "quake_after"] = 1
```

```{python FeatEngEvents}

# Split events, delete events column
df_train["dia_madre"] = ((df_train["event"] == 1) & (df_train.index.month == 5) & (df_train.index.day.isin([8,10,11,12,14]))).astype(int)

df_train["futbol"] = ((df_train["event"] == 1) & (df_train.index.isin(pd.date_range(start = "2014-06-12", end = "2014-07-13")))).astype(int)

df_train["black_friday"] = ((df_train["event"] == 1) & (df_train.index.isin(["2014-11-28", "2015-11-27", "2016-11-25"]))).astype(int)

df_train["cyber_monday"] = ((df_train["event"] == 1) & (df_train.index.isin(["2014-12-01", "2015-11-30", "2016-11-28"]))).astype(int)

df_train = df_train.drop("event", axis=1)
```

```{python FeatEngDaysWeek}

# Days of week dummies (monday intercept)
df_train["tuesday"] = (df_train.index.dayofweek == 1).astype(int)
df_train["wednesday"] = (df_train.index.dayofweek == 2).astype(int)
df_train["thursday"] = (df_train.index.dayofweek == 3).astype(int)
df_train["friday"] = (df_train.index.dayofweek == 4).astype(int)
df_train["saturday"] = (df_train.index.dayofweek == 5).astype(int)
df_train["sunday"] = (df_train.index.dayofweek == 6).astype(int)
```

```{python FeatEngLeads}

# Holiday-event leads
df_train["local_lead1"] = df_train["local_holiday"].shift(-1).fillna(0)
df_train["regional_lead1"] = df_train["regional_holiday"].shift(-1).fillna(0)
df_train["national_lead1"] = df_train["national_holiday"].shift(-1).fillna(0)
df_train["diamadre_lead1"] = df_train["dia_madre"].shift(-1).fillna(0)

# Check missing values
pd.isnull(df_train).sum()

```

```{python FeatEngTimeAgg}

# Aggregate time features by mean
time_covars = df_train.drop(columns=['id', 'store_nbr', 'family', 'sales', 'onpromotion', 'transactions', 'oil', 'city', 'state', 'store_type', 'store_cluster'], axis=1).groupby("date").mean(numeric_only=True)
```

### Trend & seasonality features

```{python FeatEngTrend}

# Add piecewise linear trend dummies
time_covars["trend"] = range(1, 1685) # Linear dummy 1

# Knot to be put at period 729
time_covars.loc[time_covars.index=="2015-01-01"]["trend"] 

# Add second linear trend dummy
time_covars["trend_knot"] = 0
time_covars.iloc[728:,-1] = range(0, 956)

# Check start and end of knot
time_covars.loc[time_covars["trend"]>=729][["trend", "trend_knot"]] 
```

```{python FeatEngFourier}

from statsmodels.tsa.deterministic import DeterministicProcess

# Add Fourier features for monthly seasonality
dp = DeterministicProcess(
  index = time_covars.index,
  constant = False,
  order = 0, # No trend feature
  seasonal = False, # No seasonal dummy features
  period = 28, # 28-period seasonality (28 days, 1 month)
  fourier = 5, # 5 Fourier pairs
  drop = True # Drop perfectly collinear terms
)
time_covars = time_covars.merge(dp.in_sample(), how="left", on="date")

# View Fourier features
time_covars.iloc[0:5, -10:]
```

## Model 1 - Time effects decomposition

### Preprocessing

```{python TSTimeCovars}

# Make Darts time series with time feats
ts_timecovars = TimeSeries.from_dataframe(
  time_covars, freq="D", fill_missing_dates=False)

ts_timecovars
```

```{python TSGaps}

# Scan for gaps
ts_timecovars.gaps()
```

```{python FillGaps}

# Fill gaps by interpolating missing values
from darts.dataprocessing.transformers import MissingValuesFiller
na_filler = MissingValuesFiller()
ts_sales = na_filler.transform(ts_sales)
ts_timecovars = na_filler.transform(ts_timecovars)

# Scan for gaps again
ts_timecovars.gaps()
```

```{python LogExpTrafos}

# Define functions to perform log transformation and reverse it
def trafo_log(x):
  return x.map(lambda x: np.log(x+1))

def trafo_exp(x):
  return x.map(lambda x: np.exp(x)-1)
```

```{python TrainValSplit1}

# Train-validation split: Pre 2017 vs 2017
y_train1, y_val1 = trafo_log(ts_sales[:-227]), trafo_log(ts_sales[-227:])
x_train1, x_val1 = ts_timecovars[:-227], ts_timecovars[-227:]
```

### Model specification

```{python ModelSpec1}

# Import models
from darts.models.forecasting.baselines import NaiveDrift, NaiveSeasonal
from darts.models.forecasting.fft import FFT
from darts.models.forecasting.sf_ets import StatsForecastETS as ETS
from darts.models.forecasting.linear_regression_model import LinearRegressionModel

# Specify baseline models
model_drift = NaiveDrift()
model_seasonal = NaiveSeasonal()

# Specify FFT model
model_fft = FFT(
  nr_freqs_to_keep = 10,
  trend = "poly",
  trend_poly_degree = 1
)

# Specify ETS model
model_ets = ETS(
  season_length = 7,
  model = "AAA"
)

# Specify linear regression model
model_linear1 = LinearRegressionModel(
  lags_future_covariates = [0])
```

### Model validation: Predicting 2017 sales

```{python 2017Pred1}
#| include: false

# Fit models on train data (pre-2017), predict validation data (2017)
model_drift.fit(y_train1)
pred_drift = model_drift.predict(n = 227)

model_seasonal.fit(y_train1)
pred_seasonal = model_seasonal.predict(n = 227)

model_fft.fit(y_train1)
pred_fft = model_fft.predict(n = 227)

model_ets.fit(y_train1)
pred_ets = model_ets.predict(n = 227)

model_linear1.fit(y_train1, future_covariates = x_train1)
pred_linear1 = model_linear1.predict(n = 227, future_covariates = x_val1)
```

```{python 2017Score1}

# Define model scoring function
from darts.metrics import mape, rmse, rmsle
def perf_scores(val, pred, model="drift"):
  
  scores_dict = {
    "RMSE": rmse(trafo_exp(val), trafo_exp(pred)), 
    "RMSLE": rmse(val, pred), 
    "MAPE": mape(trafo_exp(val), trafo_exp(pred))
      }
      
  print("Model: " + model)
  
  for key in scores_dict:
    print(
      key + ": " + 
      str(round(scores_dict[key], 4))
       )
  print("--------")  

# Score models' performance
perf_scores(y_val1, pred_drift, model="Naive drift")
perf_scores(y_val1, pred_seasonal, model="Naive seasonal")
perf_scores(y_val1, pred_fft, model="FFT")
perf_scores(y_val1, pred_ets, model="Exponential smoothing")
perf_scores(y_val1, pred_linear1, model="Linear regression")
```

```{python 2017Plot1}
#| echo: false
#| message: false

# FIG7: Plot models' predictions against actual values
fig7, axes7 = plt.subplots(3, sharex=True, sharey=True)
_ = fig7.suptitle("Actual vs. predicted sales, time decomposition models")

# FFT
_ = trafo_exp(y_val1).plot(ax = axes7[0], label="Actual")
_ = trafo_exp(pred_fft).plot(ax = axes7[0], label="Predicted")
_ = axes7[0].set_title("FFT")
_ = axes7[0].set_label(fontsize="small")

# ETS
_ = trafo_exp(y_val1).plot(ax = axes7[1], label="Actual")
_ = trafo_exp(pred_ets).plot(ax = axes7[1], label="Predicted")
_ = axes7[1].set_title("ETS")
_ = axes7[1].set_label(fontsize="small")

# Linear regression
_ = trafo_exp(y_val1).plot(ax = axes7[2], label="Actual")
_ = trafo_exp(pred_linear1).plot(ax = axes7[2], label="Predicted")
_ = axes7[2].set_title("Linear regression")
_ = axes7[2].set_label(fontsize="small")

# Show FIG7
plt.show()
plt.close("all")
```

### Backtesting / Historical forecasts

```{python HistFore1}

# Retrieve historical forecasts and 14-17 decomposed residuals
pred_hist1 = model_linear1.historical_forecasts(
  trafo_log(ts_sales), 
  future_covariates = ts_timecovars, start = 365, stride = 1,
  verbose = True)
res_linear1 = trafo_log(ts_sales[365:]) - pred_hist1

# Score historical forecasts
perf_scores(trafo_log(ts_sales[365:]), pred_hist1, model="Linear regression, historical")
```

```{python HistForePlot1}
#| echo: false

# Plot historical forecasts for linear regression
_ = ts_sales.plot(label="Actual")
_ = trafo_exp(pred_hist1).plot(label="Predicted")
_ = plt.title("Time decomposition linear model, historical forecasts")
_ = plt.ylabel("sales")
plt.show()
plt.close("all")
```

### Residuals diagnosis

```{python ResDiag1}

# Diagnose linear model 1's innovation residuals
from darts.utils.statistics import plot_residuals_analysis, plot_pacf
_ = plot_residuals_analysis(res_linear1)
plt.show()
plt.close("all")

# PACF plot of decomped sales residuals
_ =  plot_pacf(res_linear1, max_lag=56)
_ = plt.title("Partial autocorrelation plot, residuals of linear model 1")
_ = plt.xlabel("Lags")
_ = plt.ylabel("PACF")
_ = plt.xticks(np.arange(0, 56, 10))
_ = plt.xticks(np.arange(0, 56, 1), minor=True)
_ = plt.grid(which='minor', alpha=0.5)
_ = plt.grid(which='major', alpha=1)
plt.show()
plt.close("all")
```

```{python StatTest1}

# KPSS and ADF stationarity test on model 1 residuals
from darts.utils.statistics import stationarity_test_kpss, stationarity_test_adf
stationarity_test_kpss(res_linear1) # Null rejected = data is non-stationary
stationarity_test_adf(res_linear1) # Null rejected = data is stationary around a constant
```

### Time decomposition

```{python TimeDecompFinal}

# Perform full time decomposition in Sklearn

# Retrieve Darts target and features with filled gaps
sales = ts_sales.pd_series()
time_covars = ts_timecovars.pd_dataframe()

# Train-test split
y_train, y_val = trafo_log(sales[:-227]), trafo_log(sales[-227:])
x_train, x_val = time_covars.iloc[:-227,], time_covars.iloc[-227:,]

# Fit & predict on 13-16
from sklearn.linear_model import LinearRegression
model_decomp = LinearRegression()
model_decomp.fit(x_train, y_train)
pred1 = model_decomp.predict(x_train)
res1 = y_train - pred1

# Predict on 17
pred2 = model_decomp.predict(x_val)
res2 = y_val - pred2

# Concatenate residuals to get decomposed sales
sales_decomp = pd.concat([res1, res2])

# Concatenate predictions to get model 1 predictions
preds_model1 = pd.Series(np.concatenate((pred1, pred2)), index = sales_decomp.index)
```

## Exploratory analysis 2 - Lags & covariates

## Feature engineering 2 - Lags & covariates

## Model 2 - Lags & covariates

## Conclusion
