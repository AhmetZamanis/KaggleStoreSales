---
title: "Time series regression - Store sales forecasting, part 2"
author: "Ahmet Zamanis"
format: 
  gfm:
    toc: true
editor: visual
jupyter: python3
execute:
  warning: false
---

## Introduction

```{python Settings}
#| echo: false

# Import libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import torch

# Set printing options
np.set_printoptions(suppress=True, precision=4)
pd.options.display.float_format = '{:.4f}'.format
pd.set_option('display.max_columns', None)

# Set plotting options
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams["figure.autolayout"] = True
#sns.set_theme(context="paper")
```

## Data preparation

```{python DataPrepPart1}
#| echo: false
#| output: false

# Load original datasets
df_train = pd.read_csv("./OriginalData/train.csv", encoding="utf-8")
df_test = pd.read_csv("./OriginalData/test.csv", encoding="utf-8")
df_stores = pd.read_csv("./OriginalData/stores.csv", encoding="utf-8")
df_oil = pd.read_csv("./OriginalData/oil.csv", encoding="utf-8")
df_holidays = pd.read_csv("./OriginalData/holidays_events.csv", encoding="utf-8")
df_trans = pd.read_csv("./OriginalData/transactions.csv", encoding="utf-8")

# Combine df_train and df_test
df = pd.concat([df_train, df_test])

# Rename columns
df = df.rename(columns = {"family":"category"})
df_holidays = df_holidays.rename(columns = {"type":"holiday_type"})
df_oil = df_oil.rename(columns = {"dcoilwtico":"oil"})
df_stores = df_stores.rename(columns = {
  "type":"store_type", "cluster":"store_cluster"})

# Add columns from oil, stores and transactions datasets into main data
df = df.merge(df_stores, on = "store_nbr", how = "left")
df = df.merge(df_trans, on = ["date", "store_nbr"], how = "left")
df = df.merge(df_oil, on = "date", how = "left")


# Split holidays data into local, regional, national and events
events = df_holidays[df_holidays["holiday_type"] == "Event"]
df_holidays = df_holidays.drop(labels=(events.index), axis=0)
local = df_holidays.loc[df_holidays["locale"] == "Local"]
regional = df_holidays.loc[df_holidays["locale"] == "Regional"]
national = df_holidays.loc[df_holidays["locale"] == "National"]

# Drop duplicate rows in holidays-events
local = local.drop(265, axis = 0)
national = national.drop([35, 39, 156], axis = 0)
events = events.drop(244, axis = 0)

# Add local_holiday binary column to local holidays data, to be merged into main 
# data.
local["local_holiday"] = (
  local.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((local.holiday_type == "Holiday") & (local.transferred == False))
  ).astype(int)

# Add regional_holiday binary column to regional holidays data
regional["regional_holiday"] = (
  regional.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((regional.holiday_type == "Holiday") & (regional.transferred == False))
  ).astype(int)

# Add national_holiday binary column to national holidays data
national["national_holiday"] = (
  national.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((national.holiday_type == "Holiday") & (national.transferred == False))
  ).astype(int)

# Add event column to events
events["event"] = 1

# Merge local holidays binary column to main data, on date and city
local_merge = local.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"city"})
df = df.merge(local_merge, how="left", on=["date", "city"])
df["local_holiday"] = df["local_holiday"].fillna(0).astype(int)

# Merge regional holidays binary column to main data
regional_merge = regional.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"state"})
df = df.merge(regional_merge, how="left", on=["date", "state"])
df["regional_holiday"] = df["regional_holiday"].fillna(0).astype(int)

# Merge national holidays binary column to main data, on date
national_merge = national.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df = df.merge(national_merge, how="left", on="date")
df["national_holiday"] = df["national_holiday"].fillna(0).astype(int)

# Merge events binary column to main data
events_merge = events.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df = df.merge(events_merge, how="left", on="date")
df["event"] = df["event"].fillna(0).astype(int)

# Set datetime index
df = df.set_index(pd.to_datetime(df.date))
df = df.drop("date", axis=1)


# CPI adjust sales and oil, with CPI 2010 = 100
cpis = {
  "2010":100, "2013":112.8, "2014":116.8, "2015":121.5, "2016":123.6, 
  "2017":123.6
  }
  
for year in [2013, 2014, 2015, 2016, 2017]:
  df["sales"].loc[df.index.year==year] = df["sales"].loc[
    df.index.year==year] / cpis[str(year)] * cpis["2010"]
  df["oil"].loc[df.index.year==year] = df["oil"].loc[
    df.index.year==year] / cpis[str(year)] * cpis["2010"]

# Interpolate missing values in oil
df["oil"] = df["oil"].interpolate("time", limit_direction = "both")


# New year's day features
df["ny1"] = ((df.index.day == 1) & (df.index.month == 1)).astype(int)

# Set holiday dummies to 0 if NY dummies are 1
df.loc[df["ny1"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0
df["ny2"] = ((df.index.day == 2) & (df.index.month == 1)).astype(int)
df.loc[df["ny2"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# NY's eve features
df["ny_eve31"] = ((df.index.day == 31) & (df.index.month == 12)).astype(int)

df["ny_eve30"] = ((df.index.day == 30) & (df.index.month == 12)).astype(int)

df.loc[(df["ny_eve31"] == 1) | (df["ny_eve30"] == 1), ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# Proximity to Christmas sales peak
df["xmas_before"] = 0

df.loc[
  (df.index.day.isin(range(13,24))) & (df.index.month == 12), "xmas_before"] = df.loc[
  (df.index.day.isin(range(13,24))) & (df.index.month == 12)].index.day - 12

df["xmas_after"] = 0
df.loc[
  (df.index.day.isin(range(24,28))) & (df.index.month == 12), "xmas_after"] = abs(df.loc[
  (df.index.day.isin(range(24,28))) & (df.index.month == 12)].index.day - 27)

df.loc[(df["xmas_before"] != 0) | (df["xmas_after"] != 0), ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# Strength of earthquake effect on sales
# April 18 > 17 > 19 > 20 > 21 > 22
df["quake_after"] = 0
df.loc[df.index == "2016-04-18", "quake_after"] = 6
df.loc[df.index == "2016-04-17", "quake_after"] = 5
df.loc[df.index == "2016-04-19", "quake_after"] = 4
df.loc[df.index == "2016-04-20", "quake_after"] = 3
df.loc[df.index == "2016-04-21", "quake_after"] = 2
df.loc[df.index == "2016-04-22", "quake_after"] = 1

# Split events, delete events column
df["dia_madre"] = ((df["event"] == 1) & (df.index.month == 5) & (df.index.day.isin([8,10,11,12,14]))).astype(int)

df["futbol"] = ((df["event"] == 1) & (df.index.isin(pd.date_range(start = "2014-06-12", end = "2014-07-13")))).astype(int)

df["black_friday"] = ((df["event"] == 1) & (df.index.isin(["2014-11-28", "2015-11-27", "2016-11-25"]))).astype(int)

df["cyber_monday"] = ((df["event"] == 1) & (df.index.isin(["2014-12-01", "2015-11-30", "2016-11-28"]))).astype(int)

df = df.drop("event", axis=1)

# # Holiday-event leads
# df["local_lead1"] = df["local_holiday"].shift(-1).fillna(0).astype(int)
# df["regional_lead1"] = df["regional_holiday"].shift(-1).fillna(0).astype(int)
# df["national_lead1"] = df["national_holiday"].shift(-1).fillna(0).astype(int)
# df["diamadre_lead1"] = df["dia_madre"].shift(-1).fillna(0).astype(int)

# Days of week dummies
df["tuesday"] = (df.index.dayofweek == 1).astype(int)
df["wednesday"] = (df.index.dayofweek == 2).astype(int)
df["thursday"] = (df.index.dayofweek == 3).astype(int)
df["friday"] = (df.index.dayofweek == 4).astype(int)
df["saturday"] = (df.index.dayofweek == 5).astype(int)
df["sunday"] = (df.index.dayofweek == 6).astype(int)

# # Difference covariates
# from sktime.transformations.series.difference import Differencer
# trafo_diff = Differencer(lags=1)
# df["oil"] = trafo_diff.fit_transform(df["oil"].values)
# df["onpromotion"] = trafo_diff.fit_transform(df["onpromotion"].values)
# df["transactions"] = trafo_diff.fit_transform(df["transactions"].values)

# Add category X store_nbr column for Darts hierarchy
df["category_store_nbr"] = df["category"].astype(str) + "-" + df["store_nbr"].astype(str)

# Train-test split
df_train = df.loc[:"2017-08-15"]
df_test = df.loc["2017-08-16":]

# Replace transactions in train with 0
df_train["transactions"] = df_train["transactions"].fillna(0)
  
# Recombine train and test
df = pd.concat([df_train, df_test])
```

```{python PrintRawData}
#| echo: false
print(df)
```

## Hierarchical time series: Sales

```{python TargetDataFrames}

# Create wide dataframes with dates as rows, sales numbers for each hierarchy node as columns

# Total
total = pd.DataFrame(
  data=df_train.groupby("date").sales.sum(),
  index=df_train.groupby("date").sales.sum().index)

# Category
category = pd.DataFrame(
  data=df_train.groupby(["date", "category"]).sales.sum(),
  index=df_train.groupby(["date", "category"]).sales.sum().index)
category = category.reset_index(level=1)
category = category.pivot(columns="category", values="sales")

# Store
store_nbr = pd.DataFrame(
  data=df_train.groupby(["date", "store_nbr"]).sales.sum(),
  index=df_train.groupby(["date", "store_nbr"]).sales.sum().index)
store_nbr = store_nbr.reset_index(level=1)
store_nbr = store_nbr.pivot(columns="store_nbr", values="sales")

# Category x store
category_store_nbr = pd.DataFrame(
  data=df_train.groupby(["date", "category_store_nbr"]).sales.sum(),
  index=df_train.groupby(["date", "category_store_nbr"]).sales.sum().index)
category_store_nbr = category_store_nbr.reset_index(level=1)
category_store_nbr = category_store_nbr.pivot(columns="category_store_nbr", values="sales")


# Merge all wide dataframes
from functools import reduce
wide_frames = [total, category, store_nbr, category_store_nbr]
df_sales = reduce(lambda left, right: pd.merge(
  left, right, how="left", on="date"), wide_frames)
df_sales = df_sales.rename(columns = {"sales":"TOTAL"})
del total, category, store_nbr, wide_frames, category_store_nbr

# Print wide sales dataframe
print(df_sales.iloc[0:5, [0, 1, 2, 34, 35, 88, 153]])
print("Rows x columns: " + str(df_sales.shape))
```

```{python TargetHierarchy}

from darts import TimeSeries

# Create multivariate time series with sales components
ts_sales = TimeSeries.from_dataframe(df_sales, freq="D")

# Create lists of hierarchy nodes
categories = df_train.category.unique().tolist()
stores = df_train.store_nbr.unique().astype(str).tolist()

# Initialize empty dict
hierarchy_target = dict()

# Map category sales to total sales
for category in categories:
  hierarchy_target[category] = ["TOTAL"]

# Map store sales to total sales
for store in stores:
  hierarchy_target[store] = ["TOTAL"]

# Map category X store combinations to respective category sales and store sales
from itertools import product
for category, store in product(categories, stores):
  hierarchy_target["{}-{}".format(category, store)] = [category, store]

#map hierarchy to ts_train
ts_sales = ts_sales.with_hierarchy(hierarchy_target)
print(ts_sales)
del category, store

```

```{python FillTargetGaps}

# Scan gaps
print(ts_sales.gaps())

# Fill gaps
from darts.dataprocessing.transformers import MissingValuesFiller
na_filler = MissingValuesFiller()
ts_sales = na_filler.transform(ts_sales)
```

```{python CategoryPlots}
_ = ts_sales["BREAD/BAKERY"].plot()
_ = ts_sales["CLEANING"].plot()
_ = ts_sales["CELEBRATION"].plot()
_ = ts_sales["LIQUOR,WINE,BEER"].plot()
_ = ts_sales["SCHOOL AND OFFICE SUPPLIES"].plot()
_ = plt.title("Sales of 5 select categories")
plt.show()
plt.close("all")
```

```{python StorePlots}
#| echo = false
_ = ts_sales["1"].plot()
_ = ts_sales["8"].plot()
_ = ts_sales["23"].plot()
_ = ts_sales["42"].plot()
_ = ts_sales["51"].plot()
_ = plt.title("Sales of 5 select stores")
plt.show()
plt.close("all")

```

```{python CatStorePlots1}
#| echo = false
_ = ts_sales["BREAD/BAKERY-1"].plot()
_ = ts_sales["BREAD/BAKERY-8"].plot()
_ = ts_sales["BREAD/BAKERY-23"].plot()
_ = plt.title("Sales of one category across 3 stores")
plt.show()
plt.close("all")
```

```{python CatStorePlots2}
#| echo = false
_ = ts_sales["CELEBRATION-1"].plot()
_ = ts_sales["CELEBRATION-8"].plot()
_ = ts_sales["CELEBRATION-23"].plot()
_ = plt.title("Sales of one category across 3 stores")
plt.show()
plt.close("all")
```

## Covariate series

```{python LogTrafoFuncs}
#| echo = false

# Define functions to perform log transformation and reverse it. +1 to avoid zeroes
def trafo_log(x):
  return x.map(lambda x: np.log(x+1))

def trafo_exp(x):
  return x.map(lambda x: np.exp(x)-1)
```

### Total sales covariates

```{python TotalCovars1}
#| echo = false

# Aggregate time features by mean
total_covars1 = df.drop(
  columns=['id', 'store_nbr', 'category', 'sales', 'onpromotion', 'transactions', 'oil', 'city', 'state', 'store_type', 'store_cluster'], axis=1).groupby("date").mean(numeric_only=True)
  
# Add piecewise linear trend dummies
total_covars1["trend"] = range(1, 1701) # Linear trend dummy 1
total_covars1["trend_knot"] = 0
total_covars1.iloc[728:,-1] = range(0, 972) # Linear trend dummy 2

# Add Fourier features for monthly seasonality
from statsmodels.tsa.deterministic import DeterministicProcess
dp = DeterministicProcess(
  index = total_covars1.index,
  constant = False,
  order = 0, # No trend feature
  seasonal = False, # No seasonal dummy features
  period = 28, # 28-period seasonality (28 days, 1 month)
  fourier = 5, # 5 Fourier pairs
  drop = True # Drop perfectly collinear terms
)
total_covars1 = total_covars1.merge(dp.in_sample(), how="left", on="date")

# Create Darts time series with time features
ts_totalcovars1 = TimeSeries.from_dataframe(total_covars1, freq="D")

# Fill gaps in covars
ts_totalcovars1 = na_filler.transform(ts_totalcovars1)

# Retrieve covars with filled gaps
total_covars1 = ts_totalcovars1.pd_dataframe()
```

```{python TotalCovars2}
#| echo = false

# Aggregate daily covariate series
total_covars2 = df.groupby("date").agg(
 { "oil": "mean",
  "onpromotion": "sum"}
  )
total_covars2["transactions"] = df.groupby(["date", "store_nbr"]).transactions.mean().groupby("date").sum()

# Difference daily covariate series
from sktime.transformations.series.difference import Differencer
diff = Differencer(lags = 1)
total_covars2 = diff.fit_transform(total_covars2)
  
# Replace covariate series with MAs
# Oil
total_covars2["oil_ma28"] = total_covars2["oil"].rolling(window = 28, center = False).mean()
total_covars2["oil_ma28"] = total_covars2["oil_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Onpromotion
total_covars2["onp_ma28"] = total_covars2["onpromotion"].rolling(window = 28, center = False).mean()
total_covars2["onp_ma28"] = total_covars2["onp_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both") 

# Transactions
total_covars2["trns_ma7"] = total_covars2["transactions"].rolling(window = 7, center = False).mean()
total_covars2["trns_ma7"] = total_covars2["trns_ma7"].interpolate("linear", limit_direction = "backward")  

# Drop original covariate series
total_covars2 = total_covars2.drop(["oil", "onpromotion", "transactions"], axis = 1)

# Replace last 15 dates' transactions MAs with NAs
total_covars2.loc[total_covars2.index > "2017-08-15", "trns_ma7"] = np.nan
```

### Category sales covariates

```{python CommonCovars}

# Retrieve copy of total_covars1, drop Fourier terms, trend knot (leaving daily predictors common to all categories).
common_covars = total_covars1[total_covars1.columns[0:21].values.tolist()]

# Add differenced oil price and its MA to common covariates. 
common_covars["oil"] = df.groupby("date").oil.mean()
common_covars["oil"] = diff.fit_transform(common_covars["oil"]).interpolate("time", limit_direction = "both")
common_covars["oil_ma28"] = common_covars["oil"].rolling(window = 28, center = False).mean()
common_covars["oil_ma28"] = common_covars["oil_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Print common covariates
print(common_covars.columns)
```

```{python CategoryCovars}
#| output: false

from darts.utils.timeseries_generation import datetime_attribute_timeseries

# Initialize list of category covariates
ts_catcovars = []

for category in categories:
  
  # Retrieve common covariates
  covars = common_covars.copy()
  
  # Retrieve differenced onpromotion, its MA
  covars["onpromotion"] = diff.fit_transform(
    df[df["category"] == category].groupby("date").onpromotion.sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["onp_ma28"] = covars["onpromotion"].rolling(
    window = 28, center = False
    ).mean().interpolate(
  method = "spline", order = 2, limit_direction = "both"
  ) 
  
  # Retrieve differenced transactions, its MA
  covars["transactions"] = diff.fit_transform(
    df[df["category"] == category].groupby(["date", "store_nbr"]).transactions.mean().groupby("date").sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["trns_ma7"] = covars["transactions"].rolling(
    window = 7, center = False
    ).mean().interpolate(
  "linear", limit_direction = "backward"
  ) 
  
  # Create darts TS, fill gaps
  covars = na_filler.transform(TimeSeries.from_dataframe(covars, freq = "D")) 
  
  # Cyclical encode day of month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "day",
      cyclic = True
      )
    )
  
  # Append TS to list
  ts_catcovars.append(covars)
  
  # Cleanup
  del covars
```

### Store sales covariates

### Category X store sales covariates

## Modeling: Total sales

```{python ScoringFunc}
#| echo = false

# Define model scoring function
from darts.metrics import mape, rmse, rmsle
def perf_scores(val, pred, model="drift"):
  
  scores_dict = {
    "RMSE": rmse(trafo_exp(val), trafo_exp(pred)), 
    "RMSLE": rmse(val, pred), 
    "MAPE": mape(trafo_exp(val), trafo_exp(pred))
      }
      
  print("Model: " + model)
  
  for key in scores_dict:
    print(
      key + ": " + 
      str(round(scores_dict[key], 4))
       )
  print("--------")
```

```{python TotalSalesModel}
#| echo = false

# Perform time decomposition in Sklearn

# Train-test split
y_train_decomp, y_val_decomp = trafo_log(ts_sales["TOTAL"][:-227].pd_series()), trafo_log(ts_sales["TOTAL"][-227:].pd_series()) 
x_train_decomp, x_val_decomp = total_covars1.iloc[:-243,], total_covars1.iloc[-243:,]

# Fit & predict on 13-16, retrieve residuals
from sklearn.linear_model import LinearRegression
model_decomp = LinearRegression()
model_decomp.fit(x_train_decomp, y_train_decomp)
pred_total1 = model_decomp.predict(x_train_decomp)
res_total1 = y_train_decomp - pred_total1

# Predict on 17, retrieve residuals
pred_total2 = model_decomp.predict(x_val_decomp)
res_total2 = y_val_decomp - pred_total2[:-16]

# Concatenate residuals to get decomposed sales
sales_decomp = pd.concat([res_total1, res_total2])

# Concatenate predictions to get model 1 predictions
preds_total1 = pd.Series(np.concatenate((pred_total1, pred_total2)), index = total_covars1.index)


# Add decomped sales ema5 to covars2
total_covars2["sales_ema5"] = sales_decomp.rolling(
  window = 5, min_periods = 1, center = False, win_type = "exponential").mean()

# Make Darts TS with decomposed sales
ts_decomp = TimeSeries.from_series(sales_decomp.rename("sales"), freq="D")

# Make Darts TS with model 1 predictions
ts_preds_total1 = TimeSeries.from_series(preds_total1.rename("sales"), freq="D")

# Make Darts TS with covars2, fill gaps
ts_totalcovars2 = TimeSeries.from_dataframe(total_covars2, freq="D")
ts_totalcovars2 = na_filler.transform(ts_totalcovars2)

# Train-validation split
y_train_total, y_val_total = ts_decomp[:-227], trafo_log(ts_sales["TOTAL"][-227:])
x_train_total, x_val_total = ts_totalcovars2[:-243], ts_totalcovars2[-243:-16]

# Scale covariates (train-validation split only)
from sklearn.preprocessing import StandardScaler
from darts.dataprocessing.transformers import Scaler
scaler = Scaler(StandardScaler())
x_train_total = scaler.fit_transform(x_train_total)
x_val_total = scaler.transform(x_val_total)

# Model spec
from darts.models.forecasting.random_forest import RandomForest
model_forest = RandomForest(
  lags = [-1, -2, -3, -4, -5], # Target lags that can be used
  lags_future_covariates = [0], # No covariate lags
  random_state = 1923,
  n_jobs = -2 # Use all but one of the CPUs
  )

# Fit model & predict validation set
model_forest.fit(y_train_total, future_covariates = x_train_total)
pred_total = model_forest.predict(
  n = 227, future_covariates = x_val_total) + ts_preds_total1[-243:-16]

  
# Score model
perf_scores(y_val_total, pred_total, model="Linear + Random Forest hybrid, total sales")
```

## Modeling: Category sales

```{python HierarchicalScoreFunc}

# Define model scoring function for full hierarchy
def scores_hierarchy(val, pred, subset, model):
  
  def measure_rmse(val, pred, subset):
    return rmse([val[c] for c in subset], [pred[c] for c in subset])

  def measure_rmsle(val, pred, subset):
    return rmsle([(val[c] + 0.0001) for c in subset], [pred[c] for c in subset])

  scores_dict = {
    "RMSE": measure_rmse(val, pred, subset), 
    "RMSLE": measure_rmsle(val, pred, subset)
      }
      
  print("Model=" + model)    
  
  for key in scores_dict:
    print(
      key + ": mean=" + 
      str(round(np.nanmean(scores_dict[key]), 2)) + 
      ", sd=" + 
      str(round(np.nanstd(scores_dict[key]), 2)) + 
      ", min=" + str(round(min(scores_dict[key]), 2)) + 
      ", max=" + 
      str(round(max(scores_dict[key]), 2))
       )
       
  print("--------")


```

```{python CategoryPreprocessing}

# Create min-max scaler
scaler_minmax = Scaler()

# Train-validation split and scaling for covariates
x_cat = []
for series in ts_catcovars:
  
  # Split train-val series
  cov_train, cov_val = series[:-243], series[-243:-16]
  
  # Scale train-val series
  cov_train = scaler_minmax.fit_transform(cov_train)
  cov_val = scaler_minmax.transform(cov_val)
  
  # Cast series to 32-bits for performance gains
  cov_train = cov_train.astype(np.float32)
  cov_val = cov_val.astype(np.float32)
  
  # Rejoin series
  cov_train = cov_train.append(cov_val)
  
  # Append series to list
  x_cat.append(cov_train)
  
  # Cleanup
  del cov_train, cov_val
```

```{python CategoryTargets}

# List of category sales
category_sales = [ts_sales["AUTOMOTIVE":"SEAFOOD"][category] for category in categories]

# Train-validation split for category sales
y_train_cat, y_val_cat = [], []
for series in category_sales:
  
  # Split train-val series
  y_train, y_val = series[:-227], series[-227:]
  
  # Cast series to 32-bits for performance gains
  y_train = y_train.astype(np.float32)
  y_val = y_val.astype(np.float32)
  
  # Append series
  y_train_cat.append(y_train)
  y_val_cat.append(y_val)
  
  # Cleanup
  del y_train, y_val

```

```{python CategoryBaselineSpec}

# Import baseline models
from darts.models.forecasting.baselines import NaiveDrift, NaiveSeasonal
from darts.models.forecasting.auto_arima import AutoARIMA

# Specify baseline models

# Naive
model_drift = NaiveDrift()
model_seasonal = NaiveSeasonal(K=7) # Repeat the last week of the training data

# AutoARIMA
model_arima_cat = AutoARIMA(
  start_p = 0,
  max_p = 7,
  start_q = 0,
  max_q = 7,
  seasonal = False, # Don't include seasonal orders
  information_criterion = 'aicc', # Minimize AICc to choose best model
  trace = False # Don't print tuning iterations
)
```

```{python CategoryBaselineFitVal}

# Fit & validate baseline models

# Naive
model_drift.fit(ts_sales["AUTOMOTIVE":"SEAFOOD"][:-227])
pred_drift_cat = model_drift.predict(n = 227)

model_seasonal.fit(ts_sales["AUTOMOTIVE":"SEAFOOD"][:-227])
pred_seasonal_cat = model_seasonal.predict(n = 227)


# AutoARIMA
arima_covars = ['local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# First fit & validate the first category to initialize series
model_arima_cat.fit(
  y_train_cat[0], 
  future_covariates = x_train_cat[0][arima_covars])
  
pred_arima_cat = model_arima_cat.predict(
  n=227,
  future_covariates = x_val_cat[0][arima_covars])

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_cat))):
  
  # Fit on training data
  model_arima_cat.fit(
  y_train_cat[i], 
  future_covariates = x_train_cat[i][arima_covars])
  
  # Predict validation data
  pred = model_arima_cat.predict(
  n=227,
  future_covariates = x_val_cat[i][arima_covars])
  
  # Stack predictions to multivariate series
  pred_arima_cat = pred_arima_cat.stack(pred)
```

```{python CategoryScoreBaselines}

print("Category sales prediction scores, baseline models")
print("--------")

# Naive
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:], 
  pred_drift_cat,
  categories, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:], 
  pred_seasonal_cat,
  categories, 
  "Naive seasonal"
  )

# AutoARIMA
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:], 
  pred_arima_cat,
  categories, 
  "AutoARIMA"
  )

```

Mention the negative arima predictions

```{python SetupTorch}
from torchmetrics import MeanSquaredLogError
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks import ModelSummary
from pytorch_lightning.callbacks import RichProgressBar

torch.set_float32_matmul_precision("high")
torch.set_printoptions(sci_mode = False)

# Create early stopper
early_stopper = EarlyStopping(
  monitor = "val_loss",
  min_delta = 150000,
  patience = 5
)

progress_bar = RichProgressBar()
```

```{python CategoryModelSpecDLinear}
from darts.models.forecasting.dlinear import DLinearModel as DLinear

# Specify DLinear model
model_dlinear_cat = DLinear(
  input_chunk_length = 28,
  output_chunk_length = 15,
  kernel_size = 28,
  # torch_metrics = MeanSquaredLogError(),
  batch_size = 32,
  n_epochs = 100,
  model_name = "DLinearCat1",
  save_checkpoints = True,
  random_state = 1923,
  pl_trainer_kwargs = {
    "callbacks": [early_stopper],
    "accelerator": "gpu",
    "devices": [0]
    },
  show_warnings = True
)
```

```{python CategoryDLinearFitVal}

# D-linear covariates
dlinear_covars = ['local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'day_sin', 'day_cos']

# Fit & validate d-linear model
model_dlinear_cat.fit(
  series = y_train_cat,
  future_covariates = [x[dlinear_covars] for x in x_cat],
  val_series = y_val_cat,
  val_future_covariates = [x[dlinear_covars] for x in x_cat],
  verbose = True
  #num_loader_workers = 10,
)
```

Fix sanity check issue, maybe change chunk sizes and covariates

Read more about the warning output

```{python CategoryModelSpecRNN}
from darts.models.forecasting.rnn_model import RNNModel as RNN

# Specify RNN model
model_rnn_cat = RNN(
  model = "LSTM",
  input_chunk_length = 30,
  training_length = 45,
  # torch_metrics = MeanSquaredLogError(),
  batch_size = 32,
  n_epochs = 500,
  model_name = "RNNCat1",
  save_checkpoints = True,
  random_state = 1923,
  pl_trainer_kwargs = {
    "callbacks": [early_stopper, progress_bar],
    "accelerator": "gpu",
    "devices": [0]
    },
  show_warnings = True
)

```

```{python CategoryRNNFitVal}
#| output: false
#| warning: false
#| include: false

# RNN covariates
rnn_covars = ['local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'trend', 'oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'day_sin', 'day_cos']

# Fit & validate RNN model
model_rnn_cat.fit(
  series = y_train_cat,
  future_covariates = [x[rnn_covars] for x in x_cat],
  val_series = y_val_cat,
  val_future_covariates = [x[rnn_covars] for x in x_cat],
  verbose = True
  #num_loader_workers = 20
)

```

num_loader_workers causes training / sanity check to get stuck. it works without num_loader_workers, but uses single CPU core for validation data loader and possibly validation epoch

default progress bar prevents RNN checkpoint saving, it works with rich progress bar, but it doesn't print epochs

default progress bar prints epochs but freezes rstudio after a while

Dlinear doesn't train because "tensor a doesn't match tensor b". tensor a is input chunk, tensor b is 1 smaller than that

msle as metric doesn't work

can't suppress scientific notation with torch_setoptions

Use rmsle as loss function? MAE with log transform?

Use an activation that forces \>=0 predictions?

## Modeling: Store sales

## Modeling: Category X store sales

## Hierarchical reconciliation

```{python ReconcileTop}
from darts.dataprocessing.transformers.reconciliation import TopDownReconciliator as TopDown
from darts import concatenate

# Fit top down reconciliator on original series
recon_top = TopDown(verbose = True)
recon_top.fit(ts_sales)
```

```{python TESThier1}
# Distribute total sales forecasts to bottom levels
test = concatenate([trafo_exp(pred_total), ts_sales["AUTOMOTIVE":][-227:]], axis = 1).with_columns_renamed("sales", "TOTAL")
test = recon_top.transform(test.with_hierarchy(hierarchy_target))
```

```{python TESTscorehier1}
# Score each level
scores_hierarchy(ts_sales[-227:], test, categories)
#
scores_hierarchy(ts_sales[-227:], test, stores)
#

categories_stores = []
for category, store in product(categories, stores):
  categories_stores.append("{}-{}".format(category, store))
  
scores_hierarchy(ts_sales[-227:], test, categories_stores)
```

```{python}

pd.isnull(df).sum()

# Select category totals
ts_sales["AUTOMOTIVE":"SEAFOOD"]

# Select one component
ts_sales["AUTOMOTIVE":"SEAFOOD"]["AUTOMOTIVE"]

# Retrieve each component as a time series
category_sales = [ts_sales["AUTOMOTIVE":"SEAFOOD"][category] for category in categories]

# Fit model on multiple time series
fit(target = category_sales, future_covariates = category_covars)
```

## Competition submission

Remember to reverse log and CPI transformations

Past covars for test predictions: "transactions", "trns_ma7"
