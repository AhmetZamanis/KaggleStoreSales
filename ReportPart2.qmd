---
title: "Time series regression - Store sales forecasting, part 2"
author: "Ahmet Zamanis"
format: 
  gfm:
    toc: true
editor: visual
jupyter: python3
execute:
  warning: false
---

## Introduction

```{python Settings}
#| echo: false

# Import libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# Set printing options
np.set_printoptions(suppress=True, precision=4)
pd.options.display.float_format = '{:.4f}'.format
pd.set_option('display.max_columns', None)

# Set plotting options
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams["figure.autolayout"] = True
```

## Data preparation

```{python DataPrepPart1}
#| echo: false
#| output: false

# Load original datasets
df_train = pd.read_csv("./OriginalData/train.csv", encoding="utf-8")
df_test = pd.read_csv("./OriginalData/test.csv", encoding="utf-8")
df_stores = pd.read_csv("./OriginalData/stores.csv", encoding="utf-8")
df_oil = pd.read_csv("./OriginalData/oil.csv", encoding="utf-8")
df_holidays = pd.read_csv("./OriginalData/holidays_events.csv", encoding="utf-8")
df_trans = pd.read_csv("./OriginalData/transactions.csv", encoding="utf-8")

# Combine df_train and df_test
df = pd.concat([df_train, df_test])

# Rename columns
df = df.rename(columns = {"family":"category"})
df_holidays = df_holidays.rename(columns = {"type":"holiday_type"})
df_oil = df_oil.rename(columns = {"dcoilwtico":"oil"})
df_stores = df_stores.rename(columns = {
  "type":"store_type", "cluster":"store_cluster"})

# Add columns from oil, stores and transactions datasets into main data
df = df.merge(df_stores, on = "store_nbr", how = "left")
df = df.merge(df_trans, on = ["date", "store_nbr"], how = "left")
df = df.merge(df_oil, on = "date", how = "left")


# Split holidays data into local, regional, national and events
events = df_holidays[df_holidays["holiday_type"] == "Event"]
df_holidays = df_holidays.drop(labels=(events.index), axis=0)
local = df_holidays.loc[df_holidays["locale"] == "Local"]
regional = df_holidays.loc[df_holidays["locale"] == "Regional"]
national = df_holidays.loc[df_holidays["locale"] == "National"]

# Drop duplicate rows in holidays-events
local = local.drop(265, axis = 0)
national = national.drop([35, 39, 156], axis = 0)
events = events.drop(244, axis = 0)

# Add local_holiday binary column to local holidays data, to be merged into main 
# data.
local["local_holiday"] = (
  local.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((local.holiday_type == "Holiday") & (local.transferred == False))
  ).astype(int)

# Add regional_holiday binary column to regional holidays data
regional["regional_holiday"] = (
  regional.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((regional.holiday_type == "Holiday") & (regional.transferred == False))
  ).astype(int)

# Add national_holiday binary column to national holidays data
national["national_holiday"] = (
  national.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((national.holiday_type == "Holiday") & (national.transferred == False))
  ).astype(int)

# Add event column to events
events["event"] = 1

# Merge local holidays binary column to main data, on date and city
local_merge = local.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"city"})
df = df.merge(local_merge, how="left", on=["date", "city"])
df["local_holiday"] = df["local_holiday"].fillna(0).astype(int)

# Merge regional holidays binary column to main data
regional_merge = regional.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"state"})
df = df.merge(regional_merge, how="left", on=["date", "state"])
df["regional_holiday"] = df["regional_holiday"].fillna(0).astype(int)

# Merge national holidays binary column to main data, on date
national_merge = national.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df = df.merge(national_merge, how="left", on="date")
df["national_holiday"] = df["national_holiday"].fillna(0).astype(int)

# Merge events binary column to main data
events_merge = events.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df = df.merge(events_merge, how="left", on="date")
df["event"] = df["event"].fillna(0).astype(int)

# Set datetime index
df = df.set_index(pd.to_datetime(df.date))
df = df.drop("date", axis=1)


# CPI adjust sales and oil, with CPI 2010 = 100
cpis = {
  "2010":100, "2013":112.8, "2014":116.8, "2015":121.5, "2016":123.6, 
  "2017":123.6
  }
  
for year in [2013, 2014, 2015, 2016, 2017]:
  df["sales"].loc[df.index.year==year] = df["sales"].loc[
    df.index.year==year] / cpis[str(year)] * cpis["2010"]
  df["oil"].loc[df.index.year==year] = df["oil"].loc[
    df.index.year==year] / cpis[str(year)] * cpis["2010"]

# Interpolate missing values in oil
df["oil"] = df["oil"].interpolate("time", limit_direction = "both")


# New year's day features
df["ny1"] = ((df.index.day == 1) & (df.index.month == 1)).astype(int)

# Set holiday dummies to 0 if NY dummies are 1
df.loc[df["ny1"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0
df["ny2"] = ((df.index.day == 2) & (df.index.month == 1)).astype(int)
df.loc[df["ny2"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# NY's eve features
df["ny_eve31"] = ((df.index.day == 31) & (df.index.month == 12)).astype(int)

df["ny_eve30"] = ((df.index.day == 30) & (df.index.month == 12)).astype(int)

df.loc[(df["ny_eve31"] == 1) | (df["ny_eve30"] == 1), ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# Proximity to Christmas sales peak
df["xmas_before"] = 0

df.loc[
  (df.index.day.isin(range(13,24))) & (df.index.month == 12), "xmas_before"] = df.loc[
  (df.index.day.isin(range(13,24))) & (df.index.month == 12)].index.day - 12

df["xmas_after"] = 0
df.loc[
  (df.index.day.isin(range(24,28))) & (df.index.month == 12), "xmas_after"] = abs(df.loc[
  (df.index.day.isin(range(24,28))) & (df.index.month == 12)].index.day - 27)

df.loc[(df["xmas_before"] != 0) | (df["xmas_after"] != 0), ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# Strength of earthquake effect on sales
# April 18 > 17 > 19 > 20 > 21 > 22
df["quake_after"] = 0
df.loc[df.index == "2016-04-18", "quake_after"] = 6
df.loc[df.index == "2016-04-17", "quake_after"] = 5
df.loc[df.index == "2016-04-19", "quake_after"] = 4
df.loc[df.index == "2016-04-20", "quake_after"] = 3
df.loc[df.index == "2016-04-21", "quake_after"] = 2
df.loc[df.index == "2016-04-22", "quake_after"] = 1

# Split events, delete events column
df["dia_madre"] = ((df["event"] == 1) & (df.index.month == 5) & (df.index.day.isin([8,10,11,12,14]))).astype(int)

df["futbol"] = ((df["event"] == 1) & (df.index.isin(pd.date_range(start = "2014-06-12", end = "2014-07-13")))).astype(int)

df["black_friday"] = ((df["event"] == 1) & (df.index.isin(["2014-11-28", "2015-11-27", "2016-11-25"]))).astype(int)

df["cyber_monday"] = ((df["event"] == 1) & (df.index.isin(["2014-12-01", "2015-11-30", "2016-11-28"]))).astype(int)

df = df.drop("event", axis=1)

# Days of week dummies
df["tuesday"] = (df.index.dayofweek == 1).astype(int)
df["wednesday"] = (df.index.dayofweek == 2).astype(int)
df["thursday"] = (df.index.dayofweek == 3).astype(int)
df["friday"] = (df.index.dayofweek == 4).astype(int)
df["saturday"] = (df.index.dayofweek == 5).astype(int)
df["sunday"] = (df.index.dayofweek == 6).astype(int)

# Add category X store_nbr column for Darts hierarchy
df["category_store_nbr"] = df["category"].astype(str) + "-" + df["store_nbr"].astype(str)

# Train-test split
df_train = df.loc[:"2017-08-15"]
df_test = df.loc["2017-08-16":]

# Replace transactions NAs in train with 0
df_train["transactions"] = df_train["transactions"].fillna(0)
  
# Recombine train and test
df = pd.concat([df_train, df_test])
```

```{python PrintRawData}
#| echo: false
print(df.head(2))
```

## Hierarchical time series: Sales

```{python TargetDataFrames}

# Create wide dataframes with dates as rows, sales numbers for each hierarchy node as columns

# Total
total = pd.DataFrame(
  data=df_train.groupby("date").sales.sum(),
  index=df_train.groupby("date").sales.sum().index)

# Category
category = pd.DataFrame(
  data=df_train.groupby(["date", "category"]).sales.sum(),
  index=df_train.groupby(["date", "category"]).sales.sum().index)
category = category.reset_index(level=1)
category = category.pivot(columns="category", values="sales")

# Store
store_nbr = pd.DataFrame(
  data=df_train.groupby(["date", "store_nbr"]).sales.sum(),
  index=df_train.groupby(["date", "store_nbr"]).sales.sum().index)
store_nbr = store_nbr.reset_index(level=1)
store_nbr = store_nbr.pivot(columns="store_nbr", values="sales")

# Category x store
category_store_nbr = pd.DataFrame(
  data=df_train.groupby(["date", "category_store_nbr"]).sales.sum(),
  index=df_train.groupby(["date", "category_store_nbr"]).sales.sum().index)
category_store_nbr = category_store_nbr.reset_index(level=1)
category_store_nbr = category_store_nbr.pivot(columns="category_store_nbr", values="sales")


# Merge all wide dataframes
from functools import reduce
wide_frames = [total, category, store_nbr, category_store_nbr]
df_sales = reduce(lambda left, right: pd.merge(
  left, right, how="left", on="date"), wide_frames)
df_sales = df_sales.rename(columns = {"sales":"TOTAL"})
del total, category, store_nbr, wide_frames, category_store_nbr

# Print wide sales dataframe
print(df_sales.iloc[0:5, [0, 1, 2, 34, 35, 88, 153]])
print("Rows x columns: " + str(df_sales.shape))
```

```{python TargetHierarchy}

from darts import TimeSeries
from itertools import product

# Create multivariate time series with sales components
ts_sales = TimeSeries.from_dataframe(df_sales, freq="D")

# Create lists of hierarchy nodes
categories = df_train.category.unique().tolist()
stores = df_train.store_nbr.unique().astype(str).tolist()

# Initialize empty dict
hierarchy_target = dict()

# Map category sales to total sales
for category in categories:
  hierarchy_target[category] = ["TOTAL"]

# Map store sales to total sales
for store in stores:
  hierarchy_target[store] = ["TOTAL"]

# Map category X store combinations to respective category sales and store sales
for category, store in product(categories, stores):
  hierarchy_target["{}-{}".format(category, store)] = [category, store]

#map hierarchy to ts_train
ts_sales = ts_sales.with_hierarchy(hierarchy_target)
print(ts_sales)
del category, store

```

```{python FillTargetGaps}

# Scan gaps
print(ts_sales.gaps())

# Fill gaps
from darts.dataprocessing.transformers import MissingValuesFiller
na_filler = MissingValuesFiller()
ts_sales = na_filler.transform(ts_sales)
```

```{python CategoryPlots}
_ = ts_sales["BREAD/BAKERY"].plot()
_ = ts_sales["CLEANING"].plot()
_ = ts_sales["CELEBRATION"].plot()
_ = ts_sales["LIQUOR,WINE,BEER"].plot()
_ = ts_sales["SCHOOL AND OFFICE SUPPLIES"].plot()
_ = plt.title("Sales of 5 select categories")
plt.show()
plt.close("all")
```

```{python StorePlots}
#| echo = false
_ = ts_sales["1"].plot()
_ = ts_sales["8"].plot()
_ = ts_sales["23"].plot()
_ = ts_sales["42"].plot()
_ = ts_sales["51"].plot()
_ = plt.title("Sales of 5 select stores")
plt.show()
plt.close("all")

```

```{python CatStorePlots1}
#| echo = false
_ = ts_sales["BREAD/BAKERY-1"].plot()
_ = ts_sales["BREAD/BAKERY-8"].plot()
_ = ts_sales["BREAD/BAKERY-23"].plot()
_ = plt.title("Sales of one category across 3 stores")
plt.show()
plt.close("all")
```

```{python CatStorePlots2}
#| echo = false
_ = ts_sales["CELEBRATION-1"].plot()
_ = ts_sales["CELEBRATION-8"].plot()
_ = ts_sales["CELEBRATION-23"].plot()
_ = plt.title("Sales of one category across 3 stores")
plt.show()
plt.close("all")
```

## Covariate series

```{python LogTrafoFuncs}
#| echo = false

# Define functions to perform log transformation and reverse it. +1 to avoid zeroes
def trafo_log(x):
  return x.map(lambda x: np.log(x+1))

def trafo_exp(x):
  return x.map(lambda x: np.exp(x)-1)
```

### Total sales covariates

```{python TotalCovars1}
#| echo = false

# Aggregate time features by mean
total_covars1 = df.drop(
  columns=['id', 'store_nbr', 'category', 'sales', 'onpromotion', 'transactions', 'oil', 'city', 'state', 'store_type', 'store_cluster'], axis=1).groupby("date").mean(numeric_only=True)
  
# Add piecewise linear trend dummies
total_covars1["trend"] = range(1, 1701) # Linear trend dummy 1
total_covars1["trend_knot"] = 0
total_covars1.iloc[728:,-1] = range(0, 972) # Linear trend dummy 2

# Add Fourier features for monthly seasonality
from statsmodels.tsa.deterministic import DeterministicProcess
dp = DeterministicProcess(
  index = total_covars1.index,
  constant = False,
  order = 0, # No trend feature
  seasonal = False, # No seasonal dummy features
  period = 28, # 28-period seasonality (28 days, 1 month)
  fourier = 5, # 5 Fourier pairs
  drop = True # Drop perfectly collinear terms
)
total_covars1 = total_covars1.merge(dp.in_sample(), how="left", on="date")

# Create Darts time series with time features
ts_totalcovars1 = TimeSeries.from_dataframe(total_covars1, freq="D")

# Fill gaps in covars
ts_totalcovars1 = na_filler.transform(ts_totalcovars1)

# Retrieve covars with filled gaps
total_covars1 = ts_totalcovars1.pd_dataframe()
```

```{python TotalCovars2}
#| echo = false

# Aggregate daily covariate series
total_covars2 = df.groupby("date").agg(
 { "oil": "mean",
  "onpromotion": "sum"}
  )
total_covars2["transactions"] = df.groupby(["date", "store_nbr"]).transactions.mean().groupby("date").sum()

# Difference daily covariate series
from sktime.transformations.series.difference import Differencer
diff = Differencer(lags = 1)
total_covars2 = diff.fit_transform(total_covars2)
  
# Replace covariate series with MAs
# Oil
total_covars2["oil_ma28"] = total_covars2["oil"].rolling(window = 28, center = False).mean()
total_covars2["oil_ma28"] = total_covars2["oil_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Onpromotion
total_covars2["onp_ma28"] = total_covars2["onpromotion"].rolling(window = 28, center = False).mean()
total_covars2["onp_ma28"] = total_covars2["onp_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both") 

# Transactions
total_covars2["trns_ma7"] = total_covars2["transactions"].rolling(window = 7, center = False).mean()
total_covars2["trns_ma7"] = total_covars2["trns_ma7"].interpolate("linear", limit_direction = "backward")  

# Drop original covariate series
total_covars2 = total_covars2.drop(["oil", "onpromotion", "transactions"], axis = 1)

# Replace last 15 dates' transactions MAs with NAs
total_covars2.loc[total_covars2.index > "2017-08-15", "trns_ma7"] = np.nan
```

### Category sales covariates

```{python CommonCovars}

# Retrieve copy of total_covars1, drop Fourier terms, trend knot (leaving daily predictors common to all categories).
common_covars = total_covars1[total_covars1.columns[0:21].values.tolist()]

# Add differenced oil price and its MA to common covariates. 
common_covars["oil"] = df.groupby("date").oil.mean()
common_covars["oil"] = diff.fit_transform(common_covars["oil"]).interpolate("time", limit_direction = "both")
common_covars["oil_ma28"] = common_covars["oil"].rolling(window = 28, center = False).mean()
common_covars["oil_ma28"] = common_covars["oil_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Print common covariates
print(common_covars.columns)
```

```{python CategoryCovars}
#| output: false

from darts.utils.timeseries_generation import datetime_attribute_timeseries

# Initialize list of category covariates
ts_catcovars = []

for category in categories:
  
  # Retrieve common covariates
  covars = common_covars.copy()
  
  # Retrieve differenced onpromotion, its MA
  covars["onpromotion"] = diff.fit_transform(
    df[df["category"] == category].groupby("date").onpromotion.sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["onp_ma28"] = covars["onpromotion"].rolling(
    window = 28, center = False
    ).mean().interpolate(
  method = "spline", order = 2, limit_direction = "both"
  ) 
  
  # Retrieve differenced transactions, its MA
  covars["transactions"] = diff.fit_transform(
    df[df["category"] == category].groupby(["date", "store_nbr"]).transactions.mean().groupby("date").sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["trns_ma7"] = covars["transactions"].rolling(
    window = 7, center = False
    ).mean().interpolate(
  "linear", limit_direction = "backward"
  ) 
  
  # Create darts TS, fill gaps
  covars = na_filler.transform(
    TimeSeries.from_dataframe(covars, freq = "D")
    ) 
  
  # Cyclical encode day of month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "day",
      cyclic = True
      )
    )
    
   # Cyclical encode month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "month",
      cyclic = True
      )
    ) 
  
  # Append TS to list
  ts_catcovars.append(covars)
  
  # Cleanup
  del covars
```

### Store sales covariates

```{python StoreCovars}

# Initialize list of category covariates
store_covars = []

for store in [int(store) for store in stores]:
  
  # Retrieve common covariates
  covars = common_covars.copy()
  
  # Retrieve differenced onpromotion, its MA
  covars["onpromotion"] = diff.fit_transform(
    df[df["store_nbr"] == store].groupby("date").onpromotion.sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["onp_ma28"] = covars["onpromotion"].rolling(
    window = 28, center = False
    ).mean().interpolate(
  method = "spline", order = 2, limit_direction = "both"
  ) 
  
  # Retrieve differenced transactions, its MA
  covars["transactions"] = diff.fit_transform(
    df[df["store_nbr"] == store].groupby("date").transactions.sum().interpolate(
      "time", limit_direction = "both"
      )
    )
  covars["trns_ma7"] = covars["transactions"].rolling(
    window = 7, center = False
    ).mean().interpolate(
  "linear", limit_direction = "backward"
  )
  
  # Create darts TS, fill gaps
  covars = na_filler.transform(
    TimeSeries.from_dataframe(covars, freq = "D")
    )
  
  # Cyclical encode day of month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "day",
      cyclic = True
      )
    )
    
   # Cyclical encode month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "month",
      cyclic = True
      )
    )
    
  # Append TS to list
  store_covars.append(covars)
  
  # # Cleanup
  # del covars
```

```{python StoreStaticCovars}

# Create dataframe where column=static covariate and index=store nbr
store_static = df[["store_nbr", "city", "state", "store_type", "store_cluster"]].reset_index().drop("date", axis=1).drop_duplicates().set_index("store_nbr")
store_static["store_cluster"] = store_static["store_cluster"].astype(str)

# Encode static covariates
store_static = pd.get_dummies(store_static, sparse = True, drop_first = True)
```

### Category X store sales covariates

## Modeling: Total sales

### Linear + random forest hybrid from part 1

```{python ScoringFunc}
#| echo = false

# Define model scoring function
from darts.metrics import rmse, rmsle, mape
def perf_scores(val, pred, model="drift"):
  
  scores_dict = {
    "RMSE": rmse(trafo_exp(val), trafo_exp(pred)), 
    "RMSLE": rmse(val, pred), 
    "MAPE": mape(trafo_exp(val), trafo_exp(pred))
      }
      
  print("Model: " + model)
  
  for key in scores_dict:
    print(
      key + ": " + 
      str(round(scores_dict[key], 4))
       )
  print("--------")
```

```{python TotalSalesModel}
#| echo = false

# Perform time decomposition in Sklearn

# Train-test split
y_train_decomp, y_val_decomp = trafo_log(ts_sales["TOTAL"][:-227].pd_series()), trafo_log(ts_sales["TOTAL"][-227:].pd_series()) 
x_train_decomp, x_val_decomp = total_covars1.iloc[:-243,], total_covars1.iloc[-243:,]

# Fit & predict on 13-16, retrieve residuals
from sklearn.linear_model import LinearRegression
model_decomp = LinearRegression()
model_decomp.fit(x_train_decomp, y_train_decomp)
pred_total1 = model_decomp.predict(x_train_decomp)
res_total1 = y_train_decomp - pred_total1

# Predict on 17, retrieve residuals
pred_total2 = model_decomp.predict(x_val_decomp)
res_total2 = y_val_decomp - pred_total2[:-16]

# Concatenate residuals to get decomposed sales
sales_decomp = pd.concat([res_total1, res_total2])

# Concatenate predictions to get model 1 predictions
preds_total1 = pd.Series(np.concatenate((pred_total1, pred_total2)), index = total_covars1.index)


# Add decomped sales ema5 to covars2
total_covars2["sales_ema5"] = sales_decomp.rolling(
  window = 5, min_periods = 1, center = False, win_type = "exponential").mean()

# Make Darts TS with decomposed sales
ts_decomp = TimeSeries.from_series(sales_decomp.rename("TOTAL"), freq="D")

# Make Darts TS with model 1 predictions
ts_preds_total1 = TimeSeries.from_series(preds_total1.rename("TOTAL"), freq="D")

# Make Darts TS with covars2, fill gaps
ts_totalcovars2 = TimeSeries.from_dataframe(total_covars2, freq="D")
ts_totalcovars2 = na_filler.transform(ts_totalcovars2)

# Train-validation split
y_train_total, y_val_total = ts_decomp[:-227], trafo_log(ts_sales["TOTAL"][-227:])
x_train_total, x_val_total = ts_totalcovars2[:-243], ts_totalcovars2[-243:-16]

# Scale covariates (train-validation split only)
from sklearn.preprocessing import StandardScaler
from darts.dataprocessing.transformers import Scaler
scaler = Scaler(StandardScaler())
x_train_total = scaler.fit_transform(x_train_total)
x_val_total = scaler.transform(x_val_total)

# Model spec
from darts.models.forecasting.random_forest import RandomForest
model_forest = RandomForest(
  lags = [-1, -2, -3, -4, -5], # Target lags that can be used
  lags_future_covariates = [0], # No covariate lags
  random_state = 1923,
  n_jobs = -2 # Use all but one of the CPUs
  )

# Fit model & predict validation set
model_forest.fit(y_train_total, future_covariates = x_train_total)
pred_total = model_forest.predict(
  n = 227, future_covariates = x_val_total) + ts_preds_total1[-243:-16]

  
# Score model
perf_scores(y_val_total, pred_total, model="Linear + Random Forest hybrid, total sales")
```

### Linear + XGBoost hybrid, grid search tuning

```{python TotalSalesXGB}
# from darts.models.forecasting.xgboost import XGBModel 
# 
# #  Hyperparameter search space 
# xgb_params = {
#   "lags": [[-1, -2, -3, -4, -5]],
#   "lags_future_covariates": [[0]],
#   "n_jobs": [-2],
#   "learning_rate": [0.1, 0.2, 0.3],
#   "max_depth": [4, 6, 8],
#   "min_child_weight": [1, 4, 8],
#   "gamma": [0, 0.05, 0.1],
#   "subsample": [0.6, 0.8, 1],
#   "colsample_bytree": [0.6, 0.8, 1],
#   "n_estimators": [100, 250, 500]
#   }
# 
# # Decomposed validation series 
# y_val_total_decomp = y_val_total - ts_preds_total1[-243:-16] 
# 
# # Full covariates series 
# x_total = x_train_total.append(x_val_total)  
# 
# # Grid search 
# model_xgb, xgb_besttunes, xgb_bestscore = XGBModel.gridsearch( 
#   parameters = xgb_params,  
#   series = y_train_total, 
#   forecast_horizon = 15, 
#   start = 0.75,  
#   stride = 15, 
#   future_covariates = x_total,  
#   val_series = y_val_total_decomp,  
#   metric = rmse 
# )

```

grid search runtime: 9-10mins grid search best tune and score:

    {'lags': [-1, -2, -3, -4, -5], 'lags_future_covariates': [0], 'n_jobs': -2, 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1, 'colsample_bytree': 1, 'n_estimators': 500}

    0.0895999112054749 rmsle

```{python TotalSalesXGBScore}

# # Fit model & predict validation set 
# model_xgb.fit(y_train_total, future_covariates = x_total) 
# pred_total = model_xgb.predict(
#   n = 227, 
#   future_covariates = x_total) + ts_preds_total1[-243:-16] 
#   
# # Score model 
# perf_scores(y_val_total, pred_total, model="Linear + XGB hybrid, total sales")

```

## Modeling: Category sales

```{python HierarchicalScoreFunc}
from darts.metrics import mse, mae

# Define model scoring function for full hierarchy
def scores_hierarchy(val, pred, subset, model):
  
  def measure_mae(val, pred, subset):
    return mae([val[c] for c in subset], [pred[c] for c in subset])
  
  def measure_mse(val, pred, subset):
    return mse([val[c] for c in subset], [pred[c] for c in subset])
  
  def measure_rmse(val, pred, subset):
    return rmse([val[c] for c in subset], [pred[c] for c in subset])

  def measure_rmsle(val, pred, subset):
    return rmsle([(val[c]) for c in subset], [pred[c] for c in subset])

  scores_dict = {
    "MAE": measure_mae(val, pred, subset),
    "MSE": measure_mse(val, pred, subset),
    "RMSE": measure_rmse(val, pred, subset), 
    "RMSLE": measure_rmsle(val, pred, subset)
      }
      
  print("Model = " + model)    
  
  for key in scores_dict:
    print(
      key + ": mean = " + 
      str(round(np.nanmean(scores_dict[key]), 2)) + 
      ", sd = " + 
      str(round(np.nanstd(scores_dict[key]), 2)) + 
      ", min = " + str(round(min(scores_dict[key]), 2)) + 
      ", max = " + 
      str(round(max(scores_dict[key]), 2))
       )
       
  print("--------")

```

```{python NegRemover}
# Define function to replace negative predictions with zeroes
def trafo_zero(x):
  return x.map(lambda x: np.clip(x, a_min = 0, a_max = None))
```

### Preprocessing

```{python CategoryCovars}

# Create min-max scaler
scaler_minmax = Scaler()

# Train-validation split and scaling for covariates
x_cat = []
for series in ts_catcovars:
  
  # Split train-val series
  cov_train, cov_val = series[:-243], series[-243:-16]
  
  # Scale train-val series
  cov_train = scaler_minmax.fit_transform(cov_train)
  cov_val = scaler_minmax.transform(cov_val)
  
  # Cast series to 32-bits for performance gains
  cov_train = cov_train.astype(np.float32)
  cov_val = cov_val.astype(np.float32)
  
  # Rejoin series
  cov_train = cov_train.append(cov_val)
  
  # Append series to list
  x_cat.append(cov_train)
  
  # Cleanup
  del cov_train, cov_val
```

```{python CategoryTargets}

# List of category sales
category_sales = [ts_sales[categories][category] for category in categories]

# Train-validation split for category sales
y_train_cat, y_val_cat = [], []
for series in category_sales:
  
  # Split train-val series
  y_train, y_val = series[:-227], series[-227:]
  
  # # Scale train-val series
  # y_train = scaler_minmax.fit_transform(y_train)
  # y_val = scaler_minmax.transform(y_val)
  
  # Cast series to 32-bits for performance gains
  y_train = y_train.astype(np.float32)
  y_val = y_val.astype(np.float32)
  
  # Append series
  y_train_cat.append(y_train)
  y_val_cat.append(y_val)
  
  # Cleanup
  del y_train, y_val

```

### Baseline models

```{python CategoryBaselineSpec}

# Import baseline models
from darts.models.forecasting.baselines import NaiveDrift, NaiveSeasonal

# Specify baseline models
model_drift = NaiveDrift()
model_seasonal = NaiveSeasonal(K=7) # Repeat the last week of the training data
```

```{python CategoryBaselineFitVal}

# Fit & validate baseline models

# Naive
model_drift.fit(
  #scaler_minmax.fit_transform(ts_sales["AUTOMOTIVE":"SEAFOOD"][:-227])
  ts_sales[categories][:-227]
  )
pred_drift_cat = model_drift.predict(n = 227)

model_seasonal.fit(
  ts_sales[categories][:-227]
)
pred_seasonal_cat = model_seasonal.predict(n = 227)
```

```{python CategoryScoreBaselines}

print("Category sales prediction scores, baseline models")
print("--------")

# Naive
scores_hierarchy(
  #scaler_minmax.transform(ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:]),
  ts_sales[categories][-227:],
  pred_drift_cat,
  categories, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales[categories][-227:], 
  pred_seasonal_cat,
  categories, 
  "Naive seasonal"
  )
```

### Hybrid models

#### Linear regression for time effects

```{python CatLinearSpec}
from darts.models.forecasting.linear_regression_model import LinearRegressionModel

# Specify linear regression model
model_linear_cat = LinearRegressionModel(
  lags_future_covariates = [0], # Don't create any covariate lags
  output_chunk_length = 15
)

```

```{python CatLinearFitVal}
#| output: false

# Time covariates
linear_covars = ["trend", 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', "month_sin", "month_cos", 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']

# First fit & validate the first category to initialize series
model_linear_cat.fit(
  y_train_cat[0],
  future_covariates = x_cat[0][linear_covars]
  )

pred_linear_cat = model_linear_cat.predict(
  n=227,
  future_covariates = x_cat[0][linear_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_cat))):

  # Fit on training data
  model_linear_cat.fit(
  y_train_cat[i],
  future_covariates = x_cat[i][linear_covars]
  )

  # Predict validation data
  pred = model_linear_cat.predict(
  n=227,
  future_covariates = x_cat[i][linear_covars]
  )

  # Stack predictions to multivariate series
  pred_linear_cat = pred_linear_cat.stack(pred)
```

#### STL decomposition

```{python CatSTLDecomp}
from darts.utils.statistics import extract_trend_and_seasonality as decomposition
from darts.utils.statistics import remove_from_series
from darts.utils.utils import ModelMode, SeasonalityMode

# Perform STL decomposition on training data
stl_cat = []
decomp_cat = []
for series in y_train_cat:
  
  # Perform STL decomposition
  stl = decomposition(
    series,
    model = ModelMode.ADDITIVE,
    method = "STL",
    robust = True
  )
  
  # Remove trend & seasonality from series
  series = remove_from_series(
    series,
    (stl[0] + stl[1]),
    ModelMode.ADDITIVE
  )
  
  # Append to lists
  stl_cat.append(stl)
  decomp_cat.append(series)
  
  # Cleanup
  del stl, series
```

```{python CatSTLPlot}

stl_cat[5][0].plot()
stl_cat[5][1].plot()
plt.show()
plt.close("all")

decomp_cat[5].plot()
plt.show()
plt.close("all")
```

#### Random forest on decomposed sales

```{python CatRFSpec}

# Specify random forest model
model_rf_cat = RandomForest(
  lags = [-1, -2, -3, -4, -5, -6, -7],
  lags_future_covariates = [0],
  output_chunk_length = 15,
  oob_score = True,
  random_state = 1923,
  n_jobs = -2
)
```

```{python CatRFFitVal}

# RF covariates
rf_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# First fit & validate the first category to initialize series
model_rf_cat.fit(
  decomp_cat[0],
  future_covariates = x_cat[0][rf_covars]
  )

pred_rf_cat = model_rf_cat.predict(
  n=227,
  future_covariates = x_cat[0][rf_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(decomp_cat))):

  # Fit on training data
  model_rf_cat.fit(
  decomp_cat[i],
  future_covariates = x_cat[i][rf_covars]
  )

  # Predict validation data
  pred = model_rf_cat.predict(
  n=227,
  future_covariates = x_cat[i][rf_covars]
  )

  # Stack predictions to multivariate series
  pred_rf_cat = pred_rf_cat.stack(pred)

  # Cleanup
  del pred

```

#### XGBoost on decomposed sales

```{python CatXGBSpec}

# # Specify XGBoost model
# model_xgb_cat = XGBModel(
#   lags = [-1, -2, -3, -4, -5, -6, -7],
#   lags_future_covariates = [0],
#   output_chunk_length = 15,
#   random_state = 1923,
#   nthread = 20
# )
```

```{python CatXGBFitVal}
#| warning: false
#| message: false

# # XGB covariates
# xgb_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']
# 
# # XGB parameter ranges
# xgb_params = {
#   "lags" = [-1, -2, -3, -4, -5, -6, -7],
#   "lags_future_covariates" = [0],
#   "output_chunk_length" = [15],
#   "random_state" = [1923],
#   "learning_rate": [0.1, 0.15, 0.2, 0.25, 0.3],
#   "max_depth": [2, 4, 6],
#   "min_child_weight": [1, 4, 8],
#   "gamma": [0, 0.05, 0.1],
#   "subsample": [0.6, 0.8, 1],
#   "colsample_bytree": [0.6, 0.8, 1],
#   "n_estimators": [5000],
#   "early_stopping_rounds": [10]
# }
# 
# 
# # First fit, tune & predict on the first category to initialize series
# 
# # Tune
# xgb_cat_tune = XGBModel.gridsearch(
#   parameters = xgb_params,
#   series = decomp_cat[0],
#   future_covariates = x_cat[0][xgb_covars],
#   forecast_horizon = 15,
#   stride = 15,
#   start = 0.5,
#   metric = rmsle(),
#   n_jobs = 20
# )
# 
# 
# # Fit
# model_xgb_cat.fit(
#   decomp_cat[0],
#   future_covariates = x_cat[0][xgb_covars]
#   )
# 
# 
# # Predict
# pred_xgb_cat = model_xgb_cat.predict(
#   n=227,
#   future_covariates = x_cat[0][xgb_covars]
#   )
# 
# # Then loop over all categories except first
# for i in tqdm(range(1, len(decomp_cat))):
# 
#   # Fit on training data
#   model_xgb_cat.fit(
#   decomp_cat[i],
#   future_covariates = x_cat[i][xgb_covars]
#   )
#   
#   # Tune
# 
#   # Predict validation data
#   pred = model_xgb_cat.predict(
#   n=227,
#   future_covariates = x_cat[i][xgb_covars]
#   )
# 
#   # Stack predictions to multivariate series
#   pred_xgb_cat = pred_xgb_cat.stack(pred)
# 
#   # Cleanup
#   del pred

```

### Model scores

```{python CategoryScore}

print("Category sales prediction scores")
print("--------")

# Naive
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_drift_cat),
  categories, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:], 
  pred_seasonal_cat,
  categories, 
  "Naive seasonal"
  )

# Linear 
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_linear_cat),
  categories,
  "Linear"
  )

# Linear + RF
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_linear_cat + pred_rf_cat),
  categories,
  "Linear + Random forest"
  )  
  
# # Linear + XGBoost
# scores_hierarchy(
#   ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
#   trafo_zero(pred_linear_cat + pred_xgb_cat),
#   categories,
#   "Linear + XGBoost"
#   )
  
```

    Category sales prediction scores
    --------
    Model = Naive drift
    MAE: mean = 9198.75, sd = 17750.08, min = 3.84, max = 67076.19
    MSE: mean = 486367393.42, sd = 1361201937.41, min = 22.88, max = 5703870251.1
    RMSE: mean = 10119.45, sd = 19595.0, min = 4.78, max = 75523.97
    RMSLE: mean = 0.72, sd = 0.54, min = 0.34, max = 2.45
    --------
    Model = Naive seasonal
    MAE: mean = 5313.53, sd = 10042.0, min = 6.47, max = 43993.57
    MSE: mean = 188739718.7, sd = 559681015.48, min = 71.08, max = 2938535402.64
    RMSE: mean = 6431.06, sd = 12140.07, min = 8.43, max = 54208.26
    RMSLE: mean = 0.62, sd = 0.45, min = 0.32, max = 1.89
    --------
    Model = Linear
    MAE: mean = 2490.68, sd = 4729.57, min = 4.91, max = 18161.36
    MSE: mean = 53357646.8, sd = 157023733.27, min = 33.11, max = 682423288.06
    RMSE: mean = 3328.56, sd = 6502.18, min = 5.75, max = 26123.23
    RMSLE: mean = 0.46, sd = 0.27, min = 0.2, max = 1.28
    --------
    Model = Linear + Random forest
    MAE: mean = 2689.02, sd = 5158.25, min = 4.97, max = 21686.3
    MSE: mean = 58245076.48, sd = 176222421.86, min = 33.96, max = 849969897.75
    RMSE: mean = 3483.65, sd = 6790.38, min = 5.83, max = 29154.24
    RMSLE: mean = 0.44, sd = 0.28, min = 0.15, max = 1.31
    --------

Try to tune XGB with optuna?

## Modeling: Store sales

### Preprocessing

```{python StoreCovars}

# Train-validation split and scaling for covariates
x_store = []
for series in store_covars:
  
  # Split train-val series
  cov_train, cov_val = series[:-243], series[-243:-16]
  
  # Scale train-val series
  cov_train = scaler_minmax.fit_transform(cov_train)
  cov_val = scaler_minmax.transform(cov_val)
  
  # Cast series to 32-bits for performance gains
  cov_train = cov_train.astype(np.float32)
  cov_val = cov_val.astype(np.float32)
  
  # Rejoin series
  cov_train = cov_train.append(cov_val)
  
  # Append series to list
  x_store.append(cov_train)
  
  # Cleanup
  del cov_train, cov_val

```

```{python StoreTargets}

# List of store sales
store_sales = [ts_sales[store] for store in stores]

# Train-validation split for store sales
y_train_store, y_val_store = [], []
for series in store_sales:
  
  # Add static covariates to series
  series = series.with_static_covariates(
    store_static[store_static.index == int(series.components[0])]
  )
  
  # Split train-val series
  y_train, y_val = series[:-227], series[-227:]
  
  # # Scale train-val series
  # y_train = scaler_minmax.fit_transform(y_train)
  # y_val = scaler_minmax.transform(y_val)
  
  # Cast series to 32-bits for performance gains
  y_train = y_train.astype(np.float32)
  y_val = y_val.astype(np.float32)
  
  # Append series
  y_train_store.append(y_train)
  y_val_store.append(y_val)
  
  # Cleanup
  del y_train, y_val
```

### Baseline models

```{python StoreBaselineFitVal}

# Fit & validate baseline models

# Naive drift
model_drift.fit(ts_sales[stores][:-227])
pred_drift_store = model_drift.predict(n = 227)

# Naive seasonal
model_seasonal.fit(ts_sales[stores][:-227])
pred_seasonal_store = model_seasonal.predict(n = 227)

print("Store sales prediction scores, baseline models")
print("--------")

scores_hierarchy(
  ts_sales[stores][-227:],
  pred_drift_store,
  stores, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales[stores][-227:], 
  pred_seasonal_store,
  stores, 
  "Naive seasonal"
  )
```

### Hybrid models

#### Linear regression for time effects

```{python StoreLinearFitVal}
#| output: false

# Model spec
model_linear_store = model_linear_cat

# # Time covariates (trend + seasonality + calendar)
# linear_covars = ["trend",'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']

# Time covariates (trend only)
linear_covars = ["trend"]

# First fit & validate the first store to initialize series
model_linear_store.fit(
  y_train_store[0],
  future_covariates = x_store[0][linear_covars]
  )

pred_linear_store = model_linear_store.predict(
  n=227,
  future_covariates = x_store[0][linear_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_store))):

  # Fit on training data
  model_linear_store.fit(
  y_train_store[i],
  future_covariates = x_store[i][linear_covars]
  )

  # Predict validation data
  pred = model_linear_store.predict(
  n=227,
  future_covariates = x_store[i][linear_covars]
  )

  # Stack predictions to multivariate series
  pred_linear_store = pred_linear_store.stack(pred)
```

#### STL decomposition

```{python StoreSTLDecomp}

# # Perform STL decomposition on training data
# stl_store = []
# decomp_store = []
# for series in y_train_store:
#   
#   # Perform STL decomposition
#   stl = decomposition(
#     series,
#     model = ModelMode.ADDITIVE,
#     method = "STL",
#     robust = True
#   )
#   
#   # Remove trend & seasonality from series
#   series = remove_from_series(
#     series,
#     (stl[0] + stl[1]),
#     ModelMode.ADDITIVE
#   )
#   
#   # Append to lists
#   stl_store.append(stl)
#   decomp_store.append(series)
#   
#   # Cleanup
#   del stl, series
```

```{python StoreSTLDetrend}
from darts.utils.statistics import remove_trend

# Perform STL detrending from training data
decomp_store = []
for series in y_train_store:

  # Remove trend from series
  series = remove_trend(
    ts = series,
    model = ModelMode.ADDITIVE,
    method = "STL",
    trend = 29,
    robust = True
  )

  # Append to list
  decomp_store.append(series)

  # Cleanup
  del series
```

```{python StoreSTLPlot}

# stl_store[8][0].plot()
# stl_store[8][1].plot()
# plt.show()
# plt.close("all")

decomp_store[8].plot()
plt.show()
plt.close("all")
```

#### Random forest on decomposed sales

```{python StoreRFFitVal}

# Model spec
model_rf_store = RandomForest(
  lags = [-1, -2, -3, -4, -5, -6, -7],
  lags_future_covariates = [0],
  output_chunk_length = 15,
  oob_score = True,
  random_state = 1923,
  n_jobs = -2
)

# # RF covariates (cyclical only)
# rf_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# RF covariates (seasonal + calendar + cyclical)
rf_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']

# First fit & validate the first category to initialize series
model_rf_store.fit(
  decomp_store[0],
  future_covariates = x_store[0][rf_covars]
  )

pred_rf_store = model_rf_store.predict(
  n=227,
  future_covariates = x_store[0][rf_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(decomp_store))):

  # Fit on training data
  model_rf_store.fit(
  decomp_store[i],
  future_covariates = x_store[i][rf_covars]
  )

  # Predict validation data
  pred = model_rf_store.predict(
  n=227,
  future_covariates = x_store[i][rf_covars]
  )

  # Stack predictions to multivariate series
  pred_rf_store = pred_rf_store.stack(pred)

  # Cleanup
  del pred

```

### Global models

#### RNN on decomposed sales

```{python InitTorch}
import torch
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks import RichProgressBar
import tensorboard

torch.set_float32_matmul_precision("high")
torch.set_printoptions(sci_mode = False)

# Create early stopper
early_stopper = EarlyStopping(
  monitor = "val_loss",
  min_delta = 5690.3159,
  patience = 5
)

# Progress bar
progress_bar = RichProgressBar()

```

```{python StoreDecompValid}

# Retrieve decomped validation sets as validation sets minus linear model predictions
pred_linear_store_list = [pred_linear_store[store] for store in stores]
decomp_store_val = []

for i in (range(0, len(pred_linear_store_list))):
  
  decomped = y_val_store[i] - pred_linear_store_list[i]
  
  decomp_store_val.append(decomped)
  
  del decomped
```

```{python StoreRNNSpec}
from darts.models.forecasting.rnn_model import RNNModel as RNN

# Specify RNN model
model_rnn_store = RNN(
  model = "LSTM",
  input_chunk_length = 90,
  training_length = 105,
  batch_size = 64,
  n_epochs = 500,
  n_rnn_layers = 2,
  hidden_dim = 32,
  dropout = 0.2,
  model_name = "RNNStore2",
  log_tensorboard = True,
  save_checkpoints = True,
  random_state = 1923,
  pl_trainer_kwargs = {
    "callbacks": [early_stopper, progress_bar],
    "accelerator": "gpu",
    "devices": [0]
    },
  show_warnings = True,
  force_reset = True
)

```

```{python StoreRNNFit}
#| output: false
#| warning: false
#| include: false

# # RNN covariates (cyclical only)
# rnn_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# RNN covariates (seasonal + calendar + cyclical)
rnn_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']


# Fit RNN model
model_rnn_store.fit(
  series = decomp_store,
  future_covariates = [x[rnn_covars] for x in x_store],
  val_series = decomp_store_val,
  val_future_covariates = [x[rnn_covars] for x in x_store],
  verbose = True
)
```

```{python StoreRNNValid}

# First fit & validate the first store to initialize series
pred_rnn_store = model_rnn_store.predict(
  n=227,
  series = y_train_store[0],
  future_covariates = x_store[0][rnn_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_store))):

  # Predict validation data
  pred = model_rnn_store.predict(
  n=227,
  series = y_train_store[i],
  future_covariates = x_store[i][rnn_covars]
  )

  # Stack predictions to multivariate series
  pred_rnn_store = pred_rnn_store.stack(pred)
  
  del pred
```

#### D-linear for time effects

```{python StoreDLinearSpec}
# from darts.models.forecasting.dlinear import DLinearModel as DLinear
# 
# # Specify DLinear model
# model_dlinear_store = DLinear(
#   input_chunk_length = 90,
#   output_chunk_length = 15,
#   kernel_size = 27,
#   batch_size = 32,
#   n_epochs = 500,
#   model_name = "DLinearStore2",
#   log_tensorboard = True,
#   save_checkpoints = True,
#   random_state = 1923,
#   pl_trainer_kwargs = {
#     "callbacks": [early_stopper, progress_bar],
#     "accelerator": "gpu",
#     "devices": [0]
#     },
#   show_warnings = True,
#   force_reset = True
# )
```

```{python StoreDLinearFit}
#| output: false
#| warning: false
#| include: false

# # D-linear covariates (trend + season + calendar)
# dlinear_covars = ['tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']
# 
# # Fit d-linear model
# model_dlinear_store.fit(
#   series = y_train_store,
#   future_covariates = [x[dlinear_covars] for x in x_store],
#   val_series = y_val_store,
#   val_future_covariates = [x[dlinear_covars] for x in x_store],
#   verbose = True
# )
```

```{python StoreDLinearValid}

# # Predict validation data with D-Linear
# pred_dlinear_store_list = model_dlinear_store.predict(
#   n = 227,
#   series = y_train_store,
#   future_covariates = [x[dlinear_covars] for x in x_store]
#   )
# 
# # Stack predictions to get multivariate series
# pred_dlinear_store = pred_dlinear_store_list[0].stack(pred_dlinear_store_list[1])
# for pred in pred_dlinear_store_list[2:]:
#   pred_dlinear_store = pred_dlinear_store.stack(pred)
#   del pred

```

```{python}

# # First fit & validate the first store to initialize series
# pred_dlinear_store = model_dlinear_store.predict(
#   n=227,
#   series = y_train_store[0],
#   future_covariates = x_store[0][dlinear_covars]
#   )
# 
# # Then loop over all categories except first
# for i in tqdm(range(1, len(y_train_store))):
# 
#   # Predict validation data
#   pred = model_dlinear_store.predict(
#   n=227,
#   series = y_train_store[i],
#   future_covariates = x_store[i][dlinear_covars]
#   )
# 
#   # Stack predictions to multivariate series
#   pred_dlinear_store = pred_dlinear_store.stack(pred)
# 
#   del pred

```

### Model scores

```{python StoreScore}

print("Store sales prediction scores")
print("--------")

# Naive
scores_hierarchy(
  ts_sales[stores][-227:],
  trafo_zero(pred_drift_store),
  stores, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales[stores][-227:], 
  pred_seasonal_store,
  stores, 
  "Naive seasonal"
  )

# Linear 
scores_hierarchy(
  ts_sales[stores][-227:],
  trafo_zero(pred_linear_store),
  stores,
  "Linear"
  )

# Linear + Random forest 
scores_hierarchy(
  ts_sales[stores][-227:],
  trafo_zero(pred_linear_store + pred_rf_store),
  stores,
  "Linear + RF"
  )

# Linear + RNN global
scores_hierarchy(
  ts_sales[stores][-227:],
  trafo_zero(pred_linear_store + pred_rnn_store),
  stores,
  "Linear + RNN (global)"
  )

# # D-linear global
# scores_hierarchy(
#   ts_sales[stores][-227:],
#   trafo_zero(pred_dlinear_store),
#   stores,
#   "D-linear (global)"
#   )
# 
# # D-linear + RNN global
# scores_hierarchy(
#   ts_sales[stores][-227:],
#   trafo_zero(pred_dlinear_store + pred_rnn_store),
#   stores,
#   "D-linear (global) + RNN (global)"
#   )

```

Why doesn't Dlinear val loss (6.5m) match with dlinear prediction score (19.4m)?

-   Because validation is one score from one flattened validation set. Prediction is average of 54 different validation sets so the average gets skewed.

```{=html}
<!-- -->
```
    Store sales prediction scores
    --------
    Model = Naive drift
    MAE: mean = 6363.13, sd = 3694.66, min = 1156.91, max = 21050.07
    MSE: mean = 64192549.51, sd = 76134854.1, min = 2097909.98, max = 457287381.6
    RMSE: mean = 7013.42, sd = 3873.57, min = 1448.42, max = 21384.28
    RMSLE: mean = 0.95, sd = 0.86, min = 0.62, max = 7.07
    --------
    Model = Naive seasonal
    MAE: mean = 3966.55, sd = 2940.74, min = 1012.22, max = 14603.02
    MSE: mean = 33918822.51, sd = 50677321.75, min = 1744277.29, max = 234667386.71
    RMSE: mean = 4777.28, sd = 3331.13, min = 1320.71, max = 15318.86
    RMSLE: mean = 0.84, sd = 0.86, min = 0.63, max = 7.07
    --------
    Model = Linear
    MAE: mean = 2002.82, sd = 1504.09, min = 585.77, max = 9609.55
    MSE: mean = 11410492.23, sd = 25843486.45, min = 596159.8, max = 186283233.07
    RMSE: mean = 2684.66, sd = 2050.14, min = 772.11, max = 13648.56
    RMSLE: mean = 0.72, sd = 0.88, min = 0.23, max = 7.07
    --------
    Model = Linear + RF
    MAE: mean = 2103.61, sd = 1540.66, min = 577.5, max = 9609.55
    MSE: mean = 11758632.46, sd = 25904173.41, min = 569031.59, max = 186283233.07
    RMSE: mean = 2749.23, sd = 2049.48, min = 754.34, max = 13648.56
    RMSLE: mean = 0.59, sd = 0.91, min = 0.11, max = 7.07
    --------
    Model = Linear + RNN (global)
    MAE: mean = 2008.33, sd = 1498.65, min = 579.57, max = 9575.43
    MSE: mean = 11402095.54, sd = 25638472.35, min = 590154.86, max = 184616098.11
    RMSE: mean = 2688.82, sd = 2042.63, min = 768.22, max = 13587.35
    RMSLE: mean = 0.69, sd = 0.59, min = 0.23, max = 4.95
    --------
    Model = D-linear (global)
    MAE: mean = 2899.19, sd = 1863.69, min = 888.22, max = 9557.04
    MSE: mean = 19424505.91, sd = 30125865.27, min = 1374606.52, max = 183539618.23
    RMSE: mean = 3682.08, sd = 2422.15, min = 1172.44, max = 13547.68
    RMSLE: mean = 0.76, sd = 0.49, min = 0.6, max = 4.34
    --------
    Model = D-linear (global) + RNN (global)
    MAE: mean = 2918.89, sd = 1855.89, min = 893.44, max = 9522.03
    MSE: mean = 19570486.82, sd = 29983195.08, min = 1415213.83, max = 181886586.25
    RMSE: mean = 3707.05, sd = 2414.18, min = 1189.63, max = 13486.53
    RMSLE: mean = 0.77, sd = 0.48, min = 0.61, max = 4.24
    --------

    Model = Linear (trend only)
    MAE: mean = 3131.3, sd = 2347.77, min = 752.36, max = 10208.38
    MSE: mean = 23417424.1, sd = 37249525.55, min = 1074358.28, max = 186283233.07
    RMSE: mean = 3905.48, sd = 2857.39, min = 1036.51, max = 13648.56
    RMSLE: mean = 0.79, sd = 0.86, min = 0.38, max = 7.07
    --------
    Model = Linear (trend only) + RF (only trend removed, all covars except trend)
    MAE: mean = 2312.4, sd = 1696.02, min = 635.87, max = 9609.55
    MSE: mean = 14055413.6, sd = 27619496.76, min = 815966.5, max = 186283233.07
    RMSE: mean = 3017.0, sd = 2225.56, min = 903.31, max = 13648.56
    RMSLE: mean = 0.77, sd = 0.87, min = 0.3, max = 7.07
    --------
    Model = Linear + RNN (global, only trend removed, all covars except trend)
    MAE: mean = 3161.25, sd = 2299.0, min = 893.75, max = 10149.53
    MSE: mean = 23440051.16, sd = 36149416.73, min = 1472506.43, max = 174243039.23
    RMSE: mean = 3953.56, sd = 2794.54, min = 1213.47, max = 13200.12
    RMSLE: mean = 0.76, sd = 0.63, min = 0.4, max = 5.34
    --------

## Modeling: Category X store sales

## Hierarchical reconciliation

```{python ReconcileTop}
from darts.dataprocessing.transformers.reconciliation import TopDownReconciliator as TopDown
from darts import concatenate

# Fit top down reconciliator on original series
recon_top = TopDown(verbose = True)
recon_top.fit(ts_sales)
```

```{python TESThier1}
# Distribute total sales forecasts to bottom levels
test = concatenate([trafo_exp(pred_total), ts_sales["AUTOMOTIVE":][-227:]], axis = 1).with_columns_renamed("sales", "TOTAL")
test = recon_top.transform(test.with_hierarchy(hierarchy_target))
```

```{python TESTscorehier1}
# Score each level
scores_hierarchy(ts_sales[-227:], test, categories)
#
scores_hierarchy(ts_sales[-227:], test, stores)
#

categories_stores = []
for category, store in product(categories, stores):
  categories_stores.append("{}-{}".format(category, store))
  
scores_hierarchy(ts_sales[-227:], test, categories_stores)
```

## Competition submission

Remember to reverse log and CPI transformations

Past covars for test predictions: "transactions", "trns_ma7"
