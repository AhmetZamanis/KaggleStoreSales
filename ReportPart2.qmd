---
title: "Time series regression - Store sales forecasting, part 2"
author: "Ahmet Zamanis"
format: 
  gfm:
    toc: true
editor: visual
jupyter: python3
execute:
  warning: false
---

## Introduction

```{python Settings}
#| echo: false

# Import libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# Set printing options
np.set_printoptions(suppress=True, precision=4)
pd.options.display.float_format = '{:.4f}'.format
pd.set_option('display.max_columns', None)

# Set plotting options
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams["figure.autolayout"] = True
```

## Data preparation

```{python DataPrepPart1}
#| echo: false
#| output: false

# Load original datasets
df_train = pd.read_csv("./OriginalData/train.csv", encoding="utf-8")
df_test = pd.read_csv("./OriginalData/test.csv", encoding="utf-8")
df_stores = pd.read_csv("./OriginalData/stores.csv", encoding="utf-8")
df_oil = pd.read_csv("./OriginalData/oil.csv", encoding="utf-8")
df_holidays = pd.read_csv("./OriginalData/holidays_events.csv", encoding="utf-8")
df_trans = pd.read_csv("./OriginalData/transactions.csv", encoding="utf-8")

# Combine df_train and df_test
df = pd.concat([df_train, df_test])

# Rename columns
df = df.rename(columns = {"family":"category"})
df_holidays = df_holidays.rename(columns = {"type":"holiday_type"})
df_oil = df_oil.rename(columns = {"dcoilwtico":"oil"})
df_stores = df_stores.rename(columns = {
  "type":"store_type", "cluster":"store_cluster"})

# Add columns from oil, stores and transactions datasets into main data
df = df.merge(df_stores, on = "store_nbr", how = "left")
df = df.merge(df_trans, on = ["date", "store_nbr"], how = "left")
df = df.merge(df_oil, on = "date", how = "left")


# Split holidays data into local, regional, national and events
events = df_holidays[df_holidays["holiday_type"] == "Event"]
df_holidays = df_holidays.drop(labels=(events.index), axis=0)
local = df_holidays.loc[df_holidays["locale"] == "Local"]
regional = df_holidays.loc[df_holidays["locale"] == "Regional"]
national = df_holidays.loc[df_holidays["locale"] == "National"]

# Drop duplicate rows in holidays-events
local = local.drop(265, axis = 0)
national = national.drop([35, 39, 156], axis = 0)
events = events.drop(244, axis = 0)

# Add local_holiday binary column to local holidays data, to be merged into main 
# data.
local["local_holiday"] = (
  local.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((local.holiday_type == "Holiday") & (local.transferred == False))
  ).astype(int)

# Add regional_holiday binary column to regional holidays data
regional["regional_holiday"] = (
  regional.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((regional.holiday_type == "Holiday") & (regional.transferred == False))
  ).astype(int)

# Add national_holiday binary column to national holidays data
national["national_holiday"] = (
  national.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((national.holiday_type == "Holiday") & (national.transferred == False))
  ).astype(int)

# Add event column to events
events["event"] = 1

# Merge local holidays binary column to main data, on date and city
local_merge = local.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"city"})
df = df.merge(local_merge, how="left", on=["date", "city"])
df["local_holiday"] = df["local_holiday"].fillna(0).astype(int)

# Merge regional holidays binary column to main data
regional_merge = regional.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"state"})
df = df.merge(regional_merge, how="left", on=["date", "state"])
df["regional_holiday"] = df["regional_holiday"].fillna(0).astype(int)

# Merge national holidays binary column to main data, on date
national_merge = national.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df = df.merge(national_merge, how="left", on="date")
df["national_holiday"] = df["national_holiday"].fillna(0).astype(int)

# Merge events binary column to main data
events_merge = events.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df = df.merge(events_merge, how="left", on="date")
df["event"] = df["event"].fillna(0).astype(int)

# Set datetime index
df = df.set_index(pd.to_datetime(df.date))
df = df.drop("date", axis=1)


# CPI adjust sales and oil, with CPI 2010 = 100
cpis = {
  "2010":100, "2013":112.8, "2014":116.8, "2015":121.5, "2016":123.6, 
  "2017":123.6
  }
  
for year in [2013, 2014, 2015, 2016, 2017]:
  df["sales"].loc[df.index.year==year] = df["sales"].loc[
    df.index.year==year] / cpis[str(year)] * cpis["2010"]
  df["oil"].loc[df.index.year==year] = df["oil"].loc[
    df.index.year==year] / cpis[str(year)] * cpis["2010"]

# Interpolate missing values in oil
df["oil"] = df["oil"].interpolate("time", limit_direction = "both")


# New year's day features
df["ny1"] = ((df.index.day == 1) & (df.index.month == 1)).astype(int)

# Set holiday dummies to 0 if NY dummies are 1
df.loc[df["ny1"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0
df["ny2"] = ((df.index.day == 2) & (df.index.month == 1)).astype(int)
df.loc[df["ny2"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# NY's eve features
df["ny_eve31"] = ((df.index.day == 31) & (df.index.month == 12)).astype(int)

df["ny_eve30"] = ((df.index.day == 30) & (df.index.month == 12)).astype(int)

df.loc[(df["ny_eve31"] == 1) | (df["ny_eve30"] == 1), ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# Proximity to Christmas sales peak
df["xmas_before"] = 0

df.loc[
  (df.index.day.isin(range(13,24))) & (df.index.month == 12), "xmas_before"] = df.loc[
  (df.index.day.isin(range(13,24))) & (df.index.month == 12)].index.day - 12

df["xmas_after"] = 0
df.loc[
  (df.index.day.isin(range(24,28))) & (df.index.month == 12), "xmas_after"] = abs(df.loc[
  (df.index.day.isin(range(24,28))) & (df.index.month == 12)].index.day - 27)

df.loc[(df["xmas_before"] != 0) | (df["xmas_after"] != 0), ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# Strength of earthquake effect on sales
# April 18 > 17 > 19 > 20 > 21 > 22
df["quake_after"] = 0
df.loc[df.index == "2016-04-18", "quake_after"] = 6
df.loc[df.index == "2016-04-17", "quake_after"] = 5
df.loc[df.index == "2016-04-19", "quake_after"] = 4
df.loc[df.index == "2016-04-20", "quake_after"] = 3
df.loc[df.index == "2016-04-21", "quake_after"] = 2
df.loc[df.index == "2016-04-22", "quake_after"] = 1

# Split events, delete events column
df["dia_madre"] = ((df["event"] == 1) & (df.index.month == 5) & (df.index.day.isin([8,10,11,12,14]))).astype(int)

df["futbol"] = ((df["event"] == 1) & (df.index.isin(pd.date_range(start = "2014-06-12", end = "2014-07-13")))).astype(int)

df["black_friday"] = ((df["event"] == 1) & (df.index.isin(["2014-11-28", "2015-11-27", "2016-11-25"]))).astype(int)

df["cyber_monday"] = ((df["event"] == 1) & (df.index.isin(["2014-12-01", "2015-11-30", "2016-11-28"]))).astype(int)

df = df.drop("event", axis=1)

# Days of week dummies
df["tuesday"] = (df.index.dayofweek == 1).astype(int)
df["wednesday"] = (df.index.dayofweek == 2).astype(int)
df["thursday"] = (df.index.dayofweek == 3).astype(int)
df["friday"] = (df.index.dayofweek == 4).astype(int)
df["saturday"] = (df.index.dayofweek == 5).astype(int)
df["sunday"] = (df.index.dayofweek == 6).astype(int)

# Add category X store_nbr column for Darts hierarchy
df["category_store_nbr"] = df["category"].astype(str) + "-" + df["store_nbr"].astype(str)

# Train-test split
df_train = df.loc[:"2017-08-15"]
df_test = df.loc["2017-08-16":]

# Replace transactions NAs in train with 0
df_train["transactions"] = df_train["transactions"].fillna(0)
  
# Recombine train and test
df = pd.concat([df_train, df_test])
```

```{python PrintRawData}
#| echo: false
print(df.head(2))
```

## Hierarchical time series: Sales

```{python TargetDataFrames}

# Create wide dataframes with dates as rows, sales numbers for each hierarchy node as columns

# Total
total = pd.DataFrame(
  data=df_train.groupby("date").sales.sum(),
  index=df_train.groupby("date").sales.sum().index)

# Category
category = pd.DataFrame(
  data=df_train.groupby(["date", "category"]).sales.sum(),
  index=df_train.groupby(["date", "category"]).sales.sum().index)
category = category.reset_index(level=1)
category = category.pivot(columns="category", values="sales")

# Store
store_nbr = pd.DataFrame(
  data=df_train.groupby(["date", "store_nbr"]).sales.sum(),
  index=df_train.groupby(["date", "store_nbr"]).sales.sum().index)
store_nbr = store_nbr.reset_index(level=1)
store_nbr = store_nbr.pivot(columns="store_nbr", values="sales")

# Category x store
category_store_nbr = pd.DataFrame(
  data=df_train.groupby(["date", "category_store_nbr"]).sales.sum(),
  index=df_train.groupby(["date", "category_store_nbr"]).sales.sum().index)
category_store_nbr = category_store_nbr.reset_index(level=1)
category_store_nbr = category_store_nbr.pivot(columns="category_store_nbr", values="sales")


# Merge all wide dataframes
from functools import reduce
wide_frames = [total, category, store_nbr, category_store_nbr]
df_sales = reduce(lambda left, right: pd.merge(
  left, right, how="left", on="date"), wide_frames)
df_sales = df_sales.rename(columns = {"sales":"TOTAL"})
del total, category, store_nbr, wide_frames, category_store_nbr

# Print wide sales dataframe
print(df_sales.iloc[0:5, [0, 1, 2, 34, 35, 88, 153]])
print("Rows x columns: " + str(df_sales.shape))
```

```{python TargetHierarchy}

from darts import TimeSeries
from itertools import product

# Create multivariate time series with sales components
ts_sales = TimeSeries.from_dataframe(df_sales, freq="D")

# Create lists of hierarchy nodes
categories = df_train.category.unique().tolist()
stores = df_train.store_nbr.unique().astype(str).tolist()

# Initialize empty dict
hierarchy_target = dict()

# Map category sales to total sales
for category in categories:
  hierarchy_target[category] = ["TOTAL"]

# Map store sales to total sales
for store in stores:
  hierarchy_target[store] = ["TOTAL"]

# Map category X store combinations to respective category sales and store sales
for category, store in product(categories, stores):
  hierarchy_target["{}-{}".format(category, store)] = [category, store]

#map hierarchy to ts_train
ts_sales = ts_sales.with_hierarchy(hierarchy_target)
print(ts_sales)
del category, store

```

```{python FillTargetGaps}

# Scan gaps
print(ts_sales.gaps())

# Fill gaps
from darts.dataprocessing.transformers import MissingValuesFiller
na_filler = MissingValuesFiller()
ts_sales = na_filler.transform(ts_sales)
```

```{python CategoryPlots}
_ = ts_sales["BREAD/BAKERY"].plot()
_ = ts_sales["CLEANING"].plot()
_ = ts_sales["CELEBRATION"].plot()
_ = ts_sales["LIQUOR,WINE,BEER"].plot()
_ = ts_sales["SCHOOL AND OFFICE SUPPLIES"].plot()
_ = plt.title("Sales of 5 select categories")
plt.show()
plt.close("all")
```

```{python StorePlots}
#| echo = false
_ = ts_sales["1"].plot()
_ = ts_sales["8"].plot()
_ = ts_sales["23"].plot()
_ = ts_sales["42"].plot()
_ = ts_sales["51"].plot()
_ = plt.title("Sales of 5 select stores")
plt.show()
plt.close("all")

```

```{python CatStorePlots1}
#| echo = false
_ = ts_sales["BREAD/BAKERY-1"].plot()
_ = ts_sales["BREAD/BAKERY-8"].plot()
_ = ts_sales["BREAD/BAKERY-23"].plot()
_ = plt.title("Sales of one category across 3 stores")
plt.show()
plt.close("all")
```

```{python CatStorePlots2}
#| echo = false
_ = ts_sales["CELEBRATION-1"].plot()
_ = ts_sales["CELEBRATION-8"].plot()
_ = ts_sales["CELEBRATION-23"].plot()
_ = plt.title("Sales of one category across 3 stores")
plt.show()
plt.close("all")
```

## Covariate series

```{python LogTrafoFuncs}
#| echo = false

# Define functions to perform log transformation and reverse it. +1 to avoid zeroes
def trafo_log(x):
  return x.map(lambda x: np.log(x+1))

def trafo_exp(x):
  return x.map(lambda x: np.exp(x)-1)
```

### Total sales covariates

```{python TotalCovars1}
#| echo = false

# Aggregate time features by mean
total_covars1 = df.drop(
  columns=['id', 'store_nbr', 'category', 'sales', 'onpromotion', 'transactions', 'oil', 'city', 'state', 'store_type', 'store_cluster'], axis=1).groupby("date").mean(numeric_only=True)
  
# Add piecewise linear trend dummies
total_covars1["trend"] = range(1, 1701) # Linear trend dummy 1
total_covars1["trend_knot"] = 0
total_covars1.iloc[728:,-1] = range(0, 972) # Linear trend dummy 2

# Add Fourier features for monthly seasonality
from statsmodels.tsa.deterministic import DeterministicProcess
dp = DeterministicProcess(
  index = total_covars1.index,
  constant = False,
  order = 0, # No trend feature
  seasonal = False, # No seasonal dummy features
  period = 28, # 28-period seasonality (28 days, 1 month)
  fourier = 5, # 5 Fourier pairs
  drop = True # Drop perfectly collinear terms
)
total_covars1 = total_covars1.merge(dp.in_sample(), how="left", on="date")

# Create Darts time series with time features
ts_totalcovars1 = TimeSeries.from_dataframe(total_covars1, freq="D")

# Fill gaps in covars
ts_totalcovars1 = na_filler.transform(ts_totalcovars1)

# Retrieve covars with filled gaps
total_covars1 = ts_totalcovars1.pd_dataframe()
```

```{python TotalCovars2}
#| echo = false

# Aggregate daily covariate series
total_covars2 = df.groupby("date").agg(
 { "oil": "mean",
  "onpromotion": "sum"}
  )
total_covars2["transactions"] = df.groupby(["date", "store_nbr"]).transactions.mean().groupby("date").sum()

# Difference daily covariate series
from sktime.transformations.series.difference import Differencer
diff = Differencer(lags = 1)
total_covars2 = diff.fit_transform(total_covars2)
  
# Replace covariate series with MAs
# Oil
total_covars2["oil_ma28"] = total_covars2["oil"].rolling(window = 28, center = False).mean()
total_covars2["oil_ma28"] = total_covars2["oil_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Onpromotion
total_covars2["onp_ma28"] = total_covars2["onpromotion"].rolling(window = 28, center = False).mean()
total_covars2["onp_ma28"] = total_covars2["onp_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both") 

# Transactions
total_covars2["trns_ma7"] = total_covars2["transactions"].rolling(window = 7, center = False).mean()
total_covars2["trns_ma7"] = total_covars2["trns_ma7"].interpolate("linear", limit_direction = "backward")  

# Drop original covariate series
total_covars2 = total_covars2.drop(["oil", "onpromotion", "transactions"], axis = 1)

# Replace last 15 dates' transactions MAs with NAs
total_covars2.loc[total_covars2.index > "2017-08-15", "trns_ma7"] = np.nan
```

### Category sales covariates

```{python CommonCovars}

# Retrieve copy of total_covars1, drop Fourier terms, trend knot (leaving daily predictors common to all categories).
common_covars = total_covars1[total_covars1.columns[0:21].values.tolist()]

# Add differenced oil price and its MA to common covariates. 
common_covars["oil"] = df.groupby("date").oil.mean()
common_covars["oil"] = diff.fit_transform(common_covars["oil"]).interpolate("time", limit_direction = "both")
common_covars["oil_ma28"] = common_covars["oil"].rolling(window = 28, center = False).mean()
common_covars["oil_ma28"] = common_covars["oil_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Print common covariates
print(common_covars.columns)
```

```{python CategoryCovars}
#| output: false

from darts.utils.timeseries_generation import datetime_attribute_timeseries

# Initialize list of category covariates
ts_catcovars = []

for category in categories:
  
  # Retrieve common covariates
  covars = common_covars.copy()
  
  # Retrieve differenced onpromotion, its MA
  covars["onpromotion"] = diff.fit_transform(
    df[df["category"] == category].groupby("date").onpromotion.sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["onp_ma28"] = covars["onpromotion"].rolling(
    window = 28, center = False
    ).mean().interpolate(
  method = "spline", order = 2, limit_direction = "both"
  ) 
  
  # Retrieve differenced transactions, its MA
  covars["transactions"] = diff.fit_transform(
    df[df["category"] == category].groupby(["date", "store_nbr"]).transactions.mean().groupby("date").sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["trns_ma7"] = covars["transactions"].rolling(
    window = 7, center = False
    ).mean().interpolate(
  "linear", limit_direction = "backward"
  ) 
  
  # Create darts TS, fill gaps
  covars = na_filler.transform(
    TimeSeries.from_dataframe(covars, freq = "D")
    ) 
  
  # Cyclical encode day of month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "day",
      cyclic = True
      )
    )
    
   # Cyclical encode month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "month",
      cyclic = True
      )
    ) 
  
  # Append TS to list
  ts_catcovars.append(covars)
  
  # Cleanup
  del covars
```

### Store sales covariates

### Category X store sales covariates

## Modeling: Total sales

```{python ScoringFunc}
#| echo = false

# Define model scoring function
from darts.metrics import rmse, rmsle, mape
def perf_scores(val, pred, model="drift"):
  
  scores_dict = {
    "RMSE": rmse(trafo_exp(val), trafo_exp(pred)), 
    "RMSLE": rmse(val, pred), 
    "MAPE": mape(trafo_exp(val), trafo_exp(pred))
      }
      
  print("Model: " + model)
  
  for key in scores_dict:
    print(
      key + ": " + 
      str(round(scores_dict[key], 4))
       )
  print("--------")
```

```{python TotalSalesModel}
#| echo = false

# Perform time decomposition in Sklearn

# Train-test split
y_train_decomp, y_val_decomp = trafo_log(ts_sales["TOTAL"][:-227].pd_series()), trafo_log(ts_sales["TOTAL"][-227:].pd_series()) 
x_train_decomp, x_val_decomp = total_covars1.iloc[:-243,], total_covars1.iloc[-243:,]

# Fit & predict on 13-16, retrieve residuals
from sklearn.linear_model import LinearRegression
model_decomp = LinearRegression()
model_decomp.fit(x_train_decomp, y_train_decomp)
pred_total1 = model_decomp.predict(x_train_decomp)
res_total1 = y_train_decomp - pred_total1

# Predict on 17, retrieve residuals
pred_total2 = model_decomp.predict(x_val_decomp)
res_total2 = y_val_decomp - pred_total2[:-16]

# Concatenate residuals to get decomposed sales
sales_decomp = pd.concat([res_total1, res_total2])

# Concatenate predictions to get model 1 predictions
preds_total1 = pd.Series(np.concatenate((pred_total1, pred_total2)), index = total_covars1.index)


# Add decomped sales ema5 to covars2
total_covars2["sales_ema5"] = sales_decomp.rolling(
  window = 5, min_periods = 1, center = False, win_type = "exponential").mean()

# Make Darts TS with decomposed sales
ts_decomp = TimeSeries.from_series(sales_decomp.rename("sales"), freq="D")

# Make Darts TS with model 1 predictions
ts_preds_total1 = TimeSeries.from_series(preds_total1.rename("sales"), freq="D")

# Make Darts TS with covars2, fill gaps
ts_totalcovars2 = TimeSeries.from_dataframe(total_covars2, freq="D")
ts_totalcovars2 = na_filler.transform(ts_totalcovars2)

# Train-validation split
y_train_total, y_val_total = ts_decomp[:-227], trafo_log(ts_sales["TOTAL"][-227:])
x_train_total, x_val_total = ts_totalcovars2[:-243], ts_totalcovars2[-243:-16]

# Scale covariates (train-validation split only)
from sklearn.preprocessing import StandardScaler
from darts.dataprocessing.transformers import Scaler
scaler = Scaler(StandardScaler())
x_train_total = scaler.fit_transform(x_train_total)
x_val_total = scaler.transform(x_val_total)

# Model spec
from darts.models.forecasting.random_forest import RandomForest
model_forest = RandomForest(
  lags = [-1, -2, -3, -4, -5], # Target lags that can be used
  lags_future_covariates = [0], # No covariate lags
  random_state = 1923,
  n_jobs = -2 # Use all but one of the CPUs
  )

# Fit model & predict validation set
model_forest.fit(y_train_total, future_covariates = x_train_total)
pred_total = model_forest.predict(
  n = 227, future_covariates = x_val_total) + ts_preds_total1[-243:-16]

  
# Score model
perf_scores(y_val_total, pred_total, model="Linear + Random Forest hybrid, total sales")
```

## Modeling: Category sales

```{python HierarchicalScoreFunc}
from darts.metrics import mse, mae

# Define model scoring function for full hierarchy
def scores_hierarchy(val, pred, subset, model):
  
  def measure_mae(val, pred, subset):
    return mae([val[c] for c in subset], [pred[c] for c in subset])
  
  def measure_mse(val, pred, subset):
    return mse([val[c] for c in subset], [pred[c] for c in subset])
  
  def measure_rmse(val, pred, subset):
    return rmse([val[c] for c in subset], [pred[c] for c in subset])

  def measure_rmsle(val, pred, subset):
    return rmsle([(val[c]) for c in subset], [pred[c] for c in subset])

  scores_dict = {
    "MAE": measure_mae(val, pred, subset),
    "MSE": measure_mse(val, pred, subset),
    "RMSE": measure_rmse(val, pred, subset), 
    "RMSLE": measure_rmsle(val, pred, subset)
      }
      
  print("Model = " + model)    
  
  for key in scores_dict:
    print(
      key + ": mean = " + 
      str(round(np.nanmean(scores_dict[key]), 2)) + 
      ", sd = " + 
      str(round(np.nanstd(scores_dict[key]), 2)) + 
      ", min = " + str(round(min(scores_dict[key]), 2)) + 
      ", max = " + 
      str(round(max(scores_dict[key]), 2))
       )
       
  print("--------")

 
```

```{python NegRemover}

# Define function to replace negative predictions with zeroes
def trafo_zero(x):
  return x.map(lambda x: np.clip(x, a_min = 0, a_max = None))
```

### Preprocessing

```{python CategoryCovars}

# Create min-max scaler
scaler_minmax = Scaler()

# Train-validation split and scaling for covariates
x_cat = []
for series in ts_catcovars:
  
  # Split train-val series
  cov_train, cov_val = series[:-243], series[-243:-16]
  
  # Scale train-val series
  cov_train = scaler_minmax.fit_transform(cov_train)
  cov_val = scaler_minmax.transform(cov_val)
  
  # Cast series to 32-bits for performance gains
  cov_train = cov_train.astype(np.float32)
  cov_val = cov_val.astype(np.float32)
  
  # Rejoin series
  cov_train = cov_train.append(cov_val)
  
  # Append series to list
  x_cat.append(cov_train)
  
  # Cleanup
  del cov_train, cov_val
```

```{python CategoryTargets}

# List of category sales
category_sales = [ts_sales["AUTOMOTIVE":"SEAFOOD"][category] for category in categories]

# Train-validation split for category sales
y_train_cat, y_val_cat = [], []
for series in category_sales:
  
  # Split train-val series
  y_train, y_val = series[:-227], series[-227:]
  
  # # Scale train-val series
  # y_train = scaler_minmax.fit_transform(y_train)
  # y_val = scaler_minmax.transform(y_val)
  
  # Cast series to 32-bits for performance gains
  y_train = y_train.astype(np.float32)
  y_val = y_val.astype(np.float32)
  
  # Append series
  y_train_cat.append(y_train)
  y_val_cat.append(y_val)
  
  # Cleanup
  del y_train, y_val

```

### Baseline models

```{python CategoryBaselineSpec}

# Import baseline models
from darts.models.forecasting.baselines import NaiveDrift, NaiveSeasonal

# Specify baseline models
model_drift = NaiveDrift()
model_seasonal = NaiveSeasonal(K=7) # Repeat the last week of the training data
```

```{python CategoryBaselineFitVal}

# Fit & validate baseline models

# Naive
model_drift.fit(
  #scaler_minmax.fit_transform(ts_sales["AUTOMOTIVE":"SEAFOOD"][:-227])
  ts_sales["AUTOMOTIVE":"SEAFOOD"][:-227]
  )
pred_drift_cat = model_drift.predict(n = 227)

model_seasonal.fit(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][:-227]
)
pred_seasonal_cat = model_seasonal.predict(n = 227)
```

```{python CategoryScoreBaselines}

print("Category sales prediction scores, baseline models")
print("--------")

# Naive
scores_hierarchy(
  #scaler_minmax.transform(ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:]),
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  pred_drift_cat,
  categories, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:], 
  pred_seasonal_cat,
  categories, 
  "Naive seasonal"
  )
```

### AutoARIMA

```{python CategoryArimaSpec}
from darts.models.forecasting.auto_arima import AutoARIMA

# AutoARIMA
model_arima_cat = AutoARIMA(
  start_p = 0,
  max_p = 7,
  start_q = 0,
  max_q = 7,
  seasonal = False, # Don't include seasonal orders
  information_criterion = 'aicc', # Minimize AICc to choose best model
  trace = False # Don't print tuning iterations
  )
```

```{python CategoryArimaFitVal}

# AutoARIMA
arima_covars = ['local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'day_sin', 'day_cos', "month_sin", "month_cos"]

# First fit & validate the first category to initialize series
model_arima_cat.fit(
  y_train_cat[0],
  future_covariates = x_cat[0][arima_covars])

pred_arima_cat = model_arima_cat.predict(
  n=227,
  future_covariates = x_cat[0][arima_covars])

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_cat))):

  # Fit on training data
  model_arima_cat.fit(
  y_train_cat[i],
  future_covariates = x_cat[i][arima_covars])

  # Predict validation data
  pred = model_arima_cat.predict(
  n=227,
  future_covariates = x_cat[i][arima_covars])

  # Stack predictions to multivariate series
  pred_arima_cat = pred_arima_cat.stack(pred)
  
  # Cleanup
  del pred
```

### Hybrid models

#### Linear regression for trend & seasonality

```{python CatLinearSpec}
from darts.models.forecasting.linear_regression_model import LinearRegressionModel

# Specify linear regression model
model_linear_cat = LinearRegressionModel(
  lags_future_covariates = [0], # Don't create any covariate lags
  output_chunk_length = 15
)

```

```{python CatLinearFitVal}

# Time covariates
linear_covars = ["trend", 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', "month_sin", "month_cos", 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']

# First fit & validate the first category to initialize series
model_linear_cat.fit(
  y_train_cat[0],
  future_covariates = x_cat[0][linear_covars]
  )

pred_linear_cat = model_linear_cat.predict(
  n=227,
  future_covariates = x_cat[0][linear_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_cat))):

  # Fit on training data
  model_linear_cat.fit(
  y_train_cat[i],
  future_covariates = x_cat[i][linear_covars]
  )

  # Predict validation data
  pred = model_linear_cat.predict(
  n=227,
  future_covariates = x_cat[i][linear_covars]
  )

  # Stack predictions to multivariate series
  pred_linear_cat = pred_linear_cat.stack(pred)
```

trend & seasonal terms only:

    Model=Linear
    MAE: mean=2621.4, sd=4923.58, min=5.03, max=19160.13
    MSE: mean=70499625.75, sd=212692761.21, min=34.44, max=994111456.04
    RMSE: mean=3806.15, sd=7484.17, min=5.87, max=31529.53
    RMSLE: mean=0.49, sd=0.25, min=0.3, max=1.2x

\+ calendar features

    Model=Linear
    MAE: mean=2490.91, sd=4729.51, min=4.91, max=18161.36
    MSE: mean=53358370.35, sd=157023502.31, min=33.11, max=682423288.06
    RMSE: mean=3328.76, sd=6502.13, min=5.75, max=26123.23
    RMSLE: mean=0.43, sd=0.23, min=0.2, max=1.28

```{python CatLinearResids}

# Retrieve 2014 > residuals from linear decomposition model

# Initialize list of linear model residuals
res_linear_cat = []

# Then loop over all categories except first
for i in tqdm(range(0, len(y_train_cat))):

  # Retrieve residuals
  res = model_linear_cat.residuals(
  y_train_cat[i],
  future_covariates = x_cat[i][linear_covars]
  )
  
  # Drop residuals before 2014
  res = res.split_after(pd.Timestamp("2013-12-31"))[1]
  
  # Append residuals to list
  res_linear_cat.append(res)
  
  #Cleanup
  del res
```

#### STL decomposition

```{python CatSTLDecomp}
from darts.utils.statistics import extract_trend_and_seasonality as decomposition
from darts.utils.statistics import remove_from_series
from darts.utils.utils import ModelMode, SeasonalityMode

# Perform STL decomposition on training data
stl_cat = []
decomp_cat = []
for series in y_train_cat:
  
  # Perform STL decomposition
  stl = decomposition(
    series,
    freq = 7,
    model = ModelMode.ADDITIVE,
    method = "STL",
    robust = True
  )
  
  # Remove trend & seasonality from series
  series = remove_from_series(
    series,
    (stl[0] + stl[1]),
    ModelMode.ADDITIVE
  )
  
  # Append to lists
  stl_cat.append(stl)
  decomp_cat.append(series)
  
  # Cleanup
  del stl, series
```

#### Random forest on decomposed sales

```{python CatRFSpec}

# Specify random forest model
model_rf_cat = RandomForest(
  lags = [-1, -2, -3, -4, -5, -6, -7],
  lags_future_covariates = [0],
  output_chunk_length = 15,
  oob_score = True,
  random_state = 1923,
  n_jobs = 20
)
```

```{python CatRFFitVal}

# RF covariates
rf_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# First fit & validate the first category to initialize series
model_rf_cat.fit(
  decomp_cat[0],
  future_covariates = x_cat[0][rf_covars]
  )

pred_rf_cat = model_rf_cat.predict(
  n=227,
  future_covariates = x_cat[0][rf_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(decomp_cat))):

  # Fit on training data
  model_rf_cat.fit(
  decomp_cat[i],
  future_covariates = x_cat[i][rf_covars]
  )

  # Predict validation data
  pred = model_rf_cat.predict(
  n=227,
  future_covariates = x_cat[i][rf_covars]
  )

  # Stack predictions to multivariate series
  pred_rf_cat = pred_rf_cat.stack(pred)

  # Cleanup
  del pred

```

#### XGBoost on decomposed sales

```{python CatXGBSpec}
from darts.models.forecasting.xgboost import XGBModel

# Specify XGBoost model
model_xgb_cat = XGBModel(
  lags = [-1, -2, -3, -4, -5, -6, -7],
  lags_future_covariates = [0],
  output_chunk_length = 15,
  random_state = 1923,
  nthread = 20
)
```

```{python CatXGBFitVal}
#| warning: false
#| message: false

# XGB covariates
xgb_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# First fit, tune & predict on the first category to initialize series

# Fit
model_xgb_cat.fit(
  decomp_cat[0],
  future_covariates = x_cat[0][xgb_covars]
  )

# Tune

# Predict
pred_xgb_cat = model_xgb_cat.predict(
  n=227,
  future_covariates = x_cat[0][xgb_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(decomp_cat))):

  # Fit on training data
  model_xgb_cat.fit(
  decomp_cat[i],
  future_covariates = x_cat[i][xgb_covars]
  )
  
  # Tune

  # Predict validation data
  pred = model_xgb_cat.predict(
  n=227,
  future_covariates = x_cat[i][xgb_covars]
  )

  # Stack predictions to multivariate series
  pred_xgb_cat = pred_xgb_cat.stack(pred)

  # Cleanup
  del pred

```

### Model scores

```{python CategoryScore}

print("Category sales prediction scores")
print("--------")

# Naive
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_drift_cat),
  categories, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:], 
  pred_seasonal_cat,
  categories, 
  "Naive seasonal"
  )

# # AutoARIMA
# scores_hierarchy(
#   ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
#   trafo_zero(pred_arima_cat),
#   categories,
#   "AutoARIMA"
#   )

# Linear 
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_linear_cat),
  categories,
  "Linear"
  )

# Linear + RF
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_linear_cat + pred_rf_cat),
  categories,
  "Linear + Random forest"
  )  
  
# Linear + XGB
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_linear_cat + pred_xgb_cat),
  categories,
  "Linear + XGBoost"
  )
  
  
```

    Model=AutoARIMA (all features)
    MAE: mean=3177.22, sd=5938.97, min=3.59, max=24151.07
    MSE: mean=65090234.22, sd=174104280.35, min=22.94, max=817507351.93
    RMSE: mean=3825.76, sd=7103.09, min=4.79, max=28592.09
    RMSLE: mean=0.52, sd=0.61, min=0.2, max=3.67
    --------

    Model = Linear (time & calendar)
    MAE: mean=2490.68, sd=4729.57, min=4.91, max=18161.36
    MSE: mean=53357646.8, sd=157023733.27, min=33.11, max=682423288.06
    RMSE: mean=3328.56, sd=6502.18, min=5.75, max=26123.23
    RMSLE: mean=0.46, sd=0.27, min=0.2, max=1.28

    Model = Linear + Random forest (linear decomp, covariate features)
    MAE: mean=2949.54, sd=5760.42, min=6.22, max=23345.68
    MSE: mean=67404319.58, sd=199819874.33, min=51.01, max=927516253.58
    RMSE: mean=3729.04, sd=7314.27, min=7.14, max=30455.15
    RMSLE: mean=0.5, sd=0.4, min=0.16, max=1.9
    --------

    Model = Linear + XGBoost untuned (linear decomp, covariate features)
    MAE: mean=3060.46, sd=5984.15, min=6.35, max=24577.0
    MSE: mean=76354996.81, sd=223252746.14, min=54.88, max=1061127656.54
    RMSE: mean=3989.13, sd=7774.43, min=7.41, max=32574.95
    RMSLE: mean=0.53, sd=0.47, min=0.19, max=1.89
    --------

The residuals retrieved from darts are likely reducing performance of hybrid models above.

    Model = Linear (time only)
    MAE: mean = 2621.4, sd = 4923.58, min = 5.03, max = 19160.13
    MSE: mean = 70499625.75, sd = 212692761.21, min = 34.44, max = 994111456.04
    RMSE: mean = 3806.15, sd = 7484.17, min = 5.87, max = 31529.53
    RMSLE: mean = 0.49, sd = 0.25, min = 0.3, max = 1.26
    --------
    Model = Linear + Random forest (STL decomp, covariate + calendar features)
    MAE: mean = 2775.22, sd = 5294.9, min = 5.11, max = 22540.5
    MSE: mean = 68710801.31, sd = 212215204.9, min = 35.71, max = 1044254758.16
    RMSE: mean = 3741.69, sd = 7396.66, min = 5.98, max = 32314.93
    RMSLE: mean = 0.48, sd = 0.27, min = 0.26, max = 1.3
    --------
    Model = Linear + XGBoost untuned (STL decomp, covariate + calendar features)
    MAE: mean = 2986.53, sd = 5742.79, min = 5.15, max = 25868.4
    MSE: mean = 79903065.08, sd = 259141086.04, min = 36.09, max = 1394927088.03
    RMSE: mean = 4095.4, sd = 7945.49, min = 6.01, max = 37348.72
    RMSLE: mean = 0.51, sd = 0.36, min = 0.26, max = 1.82
    --------

    Model = Linear + Random forest (STL decomp, covariate features)
    MAE: mean = 2689.02, sd = 5158.25, min = 4.97, max = 21686.3
    MSE: mean = 58245076.48, sd = 176222421.86, min = 33.96, max = 849969897.75
    RMSE: mean = 3483.65, sd = 6790.38, min = 5.83, max = 29154.24
    RMSLE: mean = 0.44, sd = 0.28, min = 0.15, max = 1.31
    --------
    Model = Linear + XGBoost untuned (STL decomp, covariate features)
    MAE: mean = 3031.09, sd = 5880.44, min = 4.92, max = 25721.48
    MSE: mean = 78410131.67, sd = 243713513.61, min = 33.87, max = 1260756454.48
    RMSE: mean = 4075.08, sd = 7861.54, min = 5.82, max = 35507.13
    RMSLE: mean = 0.53, sd = 0.48, min = 0.19, max = 2.69
    --------

## Modeling: Store sales

## Modeling: Category X store sales

## Hierarchical reconciliation

```{python ReconcileTop}
from darts.dataprocessing.transformers.reconciliation import TopDownReconciliator as TopDown
from darts import concatenate

# Fit top down reconciliator on original series
recon_top = TopDown(verbose = True)
recon_top.fit(ts_sales)
```

```{python TESThier1}
# Distribute total sales forecasts to bottom levels
test = concatenate([trafo_exp(pred_total), ts_sales["AUTOMOTIVE":][-227:]], axis = 1).with_columns_renamed("sales", "TOTAL")
test = recon_top.transform(test.with_hierarchy(hierarchy_target))
```

```{python TESTscorehier1}
# Score each level
scores_hierarchy(ts_sales[-227:], test, categories)
#
scores_hierarchy(ts_sales[-227:], test, stores)
#

categories_stores = []
for category, store in product(categories, stores):
  categories_stores.append("{}-{}".format(category, store))
  
scores_hierarchy(ts_sales[-227:], test, categories_stores)
```

## Competition submission

Remember to reverse log and CPI transformations

Past covars for test predictions: "transactions", "trns_ma7"
