---
title: "Time series regression - Store sales forecasting, part 2"
author: "Ahmet Zamanis"
format: 
  gfm:
    toc: true
editor: visual
jupyter: python3
execute:
  warning: false
---

## Introduction

```{python Settings}
#| echo: false

# Import libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# Set printing options
np.set_printoptions(suppress=True, precision=4)
pd.options.display.float_format = '{:.4f}'.format
pd.set_option('display.max_columns', None)

# Set plotting options
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams["figure.autolayout"] = True
```

## Data preparation

```{python DataPrepPart1}
#| echo: false
#| output: false

# Load original datasets
df_train = pd.read_csv("./OriginalData/train.csv", encoding="utf-8")
df_test = pd.read_csv("./OriginalData/test.csv", encoding="utf-8")
df_stores = pd.read_csv("./OriginalData/stores.csv", encoding="utf-8")
df_oil = pd.read_csv("./OriginalData/oil.csv", encoding="utf-8")
df_holidays = pd.read_csv("./OriginalData/holidays_events.csv", encoding="utf-8")
df_trans = pd.read_csv("./OriginalData/transactions.csv", encoding="utf-8")

# Combine df_train and df_test
df = pd.concat([df_train, df_test])

# Rename columns
df = df.rename(columns = {"family":"category"})
df_holidays = df_holidays.rename(columns = {"type":"holiday_type"})
df_oil = df_oil.rename(columns = {"dcoilwtico":"oil"})
df_stores = df_stores.rename(columns = {
  "type":"store_type", "cluster":"store_cluster"})

# Add columns from oil, stores and transactions datasets into main data
df = df.merge(df_stores, on = "store_nbr", how = "left")
df = df.merge(df_trans, on = ["date", "store_nbr"], how = "left")
df = df.merge(df_oil, on = "date", how = "left")


# Split holidays data into local, regional, national and events
events = df_holidays[df_holidays["holiday_type"] == "Event"]
df_holidays = df_holidays.drop(labels=(events.index), axis=0)
local = df_holidays.loc[df_holidays["locale"] == "Local"]
regional = df_holidays.loc[df_holidays["locale"] == "Regional"]
national = df_holidays.loc[df_holidays["locale"] == "National"]

# Drop duplicate rows in holidays-events
local = local.drop(265, axis = 0)
national = national.drop([35, 39, 156], axis = 0)
events = events.drop(244, axis = 0)

# Add local_holiday binary column to local holidays data, to be merged into main 
# data.
local["local_holiday"] = (
  local.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((local.holiday_type == "Holiday") & (local.transferred == False))
  ).astype(int)

# Add regional_holiday binary column to regional holidays data
regional["regional_holiday"] = (
  regional.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((regional.holiday_type == "Holiday") & (regional.transferred == False))
  ).astype(int)

# Add national_holiday binary column to national holidays data
national["national_holiday"] = (
  national.holiday_type.isin(["Transfer", "Additional", "Bridge"]) |
  ((national.holiday_type == "Holiday") & (national.transferred == False))
  ).astype(int)

# Add event column to events
events["event"] = 1

# Merge local holidays binary column to main data, on date and city
local_merge = local.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"city"})
df = df.merge(local_merge, how="left", on=["date", "city"])
df["local_holiday"] = df["local_holiday"].fillna(0).astype(int)

# Merge regional holidays binary column to main data
regional_merge = regional.drop(
  labels = [
    "holiday_type", "locale", "description", "transferred"], axis = 1).rename(
      columns = {"locale_name":"state"})
df = df.merge(regional_merge, how="left", on=["date", "state"])
df["regional_holiday"] = df["regional_holiday"].fillna(0).astype(int)

# Merge national holidays binary column to main data, on date
national_merge = national.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df = df.merge(national_merge, how="left", on="date")
df["national_holiday"] = df["national_holiday"].fillna(0).astype(int)

# Merge events binary column to main data
events_merge = events.drop(
  labels = [
    "holiday_type", "locale", "locale_name", "description", 
    "transferred"], axis = 1)
df = df.merge(events_merge, how="left", on="date")
df["event"] = df["event"].fillna(0).astype(int)

# Set datetime index
df = df.set_index(pd.to_datetime(df.date))
df = df.drop("date", axis=1)


# CPI adjust sales and oil, with CPI 2010 = 100
cpis = {
  "2010":100, "2013":112.8, "2014":116.8, "2015":121.5, "2016":123.6, 
  "2017":123.6
  }
  
for year in [2013, 2014, 2015, 2016, 2017]:
  df["sales"].loc[df.index.year==year] = df["sales"].loc[
    df.index.year==year] / cpis[str(year)] * cpis["2010"]
  df["oil"].loc[df.index.year==year] = df["oil"].loc[
    df.index.year==year] / cpis[str(year)] * cpis["2010"]

# Interpolate missing values in oil
df["oil"] = df["oil"].interpolate("time", limit_direction = "both")


# New year's day features
df["ny1"] = ((df.index.day == 1) & (df.index.month == 1)).astype(int)

# Set holiday dummies to 0 if NY dummies are 1
df.loc[df["ny1"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0
df["ny2"] = ((df.index.day == 2) & (df.index.month == 1)).astype(int)
df.loc[df["ny2"] == 1, ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# NY's eve features
df["ny_eve31"] = ((df.index.day == 31) & (df.index.month == 12)).astype(int)

df["ny_eve30"] = ((df.index.day == 30) & (df.index.month == 12)).astype(int)

df.loc[(df["ny_eve31"] == 1) | (df["ny_eve30"] == 1), ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# Proximity to Christmas sales peak
df["xmas_before"] = 0

df.loc[
  (df.index.day.isin(range(13,24))) & (df.index.month == 12), "xmas_before"] = df.loc[
  (df.index.day.isin(range(13,24))) & (df.index.month == 12)].index.day - 12

df["xmas_after"] = 0
df.loc[
  (df.index.day.isin(range(24,28))) & (df.index.month == 12), "xmas_after"] = abs(df.loc[
  (df.index.day.isin(range(24,28))) & (df.index.month == 12)].index.day - 27)

df.loc[(df["xmas_before"] != 0) | (df["xmas_after"] != 0), ["local_holiday", "regional_holiday", "national_holiday"]] = 0

# Strength of earthquake effect on sales
# April 18 > 17 > 19 > 20 > 21 > 22
df["quake_after"] = 0
df.loc[df.index == "2016-04-18", "quake_after"] = 6
df.loc[df.index == "2016-04-17", "quake_after"] = 5
df.loc[df.index == "2016-04-19", "quake_after"] = 4
df.loc[df.index == "2016-04-20", "quake_after"] = 3
df.loc[df.index == "2016-04-21", "quake_after"] = 2
df.loc[df.index == "2016-04-22", "quake_after"] = 1

# Split events, delete events column
df["dia_madre"] = ((df["event"] == 1) & (df.index.month == 5) & (df.index.day.isin([8,10,11,12,14]))).astype(int)

df["futbol"] = ((df["event"] == 1) & (df.index.isin(pd.date_range(start = "2014-06-12", end = "2014-07-13")))).astype(int)

df["black_friday"] = ((df["event"] == 1) & (df.index.isin(["2014-11-28", "2015-11-27", "2016-11-25"]))).astype(int)

df["cyber_monday"] = ((df["event"] == 1) & (df.index.isin(["2014-12-01", "2015-11-30", "2016-11-28"]))).astype(int)

df = df.drop("event", axis=1)

# Days of week dummies
df["tuesday"] = (df.index.dayofweek == 1).astype(int)
df["wednesday"] = (df.index.dayofweek == 2).astype(int)
df["thursday"] = (df.index.dayofweek == 3).astype(int)
df["friday"] = (df.index.dayofweek == 4).astype(int)
df["saturday"] = (df.index.dayofweek == 5).astype(int)
df["sunday"] = (df.index.dayofweek == 6).astype(int)

# Add category X store_nbr column for Darts hierarchy
df["category_store_nbr"] = df["category"].astype(str) + "-" + df["store_nbr"].astype(str)

# Train-test split
df_train = df.loc[:"2017-08-15"]
df_test = df.loc["2017-08-16":]

# Replace transactions NAs in train with 0
df_train["transactions"] = df_train["transactions"].fillna(0)
  
# Recombine train and test
df = pd.concat([df_train, df_test])
```

```{python PrintRawData}
#| echo: false
print(df.head(2))
```

## Hierarchical time series: Sales

```{python TargetDataFrames}

# Create wide dataframes with dates as rows, sales numbers for each hierarchy node as columns

# Total
total = pd.DataFrame(
  data=df_train.groupby("date").sales.sum(),
  index=df_train.groupby("date").sales.sum().index)

# Category
category = pd.DataFrame(
  data=df_train.groupby(["date", "category"]).sales.sum(),
  index=df_train.groupby(["date", "category"]).sales.sum().index)
category = category.reset_index(level=1)
category = category.pivot(columns="category", values="sales")

# Store
store_nbr = pd.DataFrame(
  data=df_train.groupby(["date", "store_nbr"]).sales.sum(),
  index=df_train.groupby(["date", "store_nbr"]).sales.sum().index)
store_nbr = store_nbr.reset_index(level=1)
store_nbr = store_nbr.pivot(columns="store_nbr", values="sales")

# Category x store
category_store_nbr = pd.DataFrame(
  data=df_train.groupby(["date", "category_store_nbr"]).sales.sum(),
  index=df_train.groupby(["date", "category_store_nbr"]).sales.sum().index)
category_store_nbr = category_store_nbr.reset_index(level=1)
category_store_nbr = category_store_nbr.pivot(columns="category_store_nbr", values="sales")


# Merge all wide dataframes
from functools import reduce
wide_frames = [total, category, store_nbr, category_store_nbr]
df_sales = reduce(lambda left, right: pd.merge(
  left, right, how="left", on="date"), wide_frames)
df_sales = df_sales.rename(columns = {"sales":"TOTAL"})
del total, category, store_nbr, wide_frames, category_store_nbr

# Print wide sales dataframe
print(df_sales.iloc[0:5, [0, 1, 2, 34, 35, 88, 153]])
print("Rows x columns: " + str(df_sales.shape))
```

```{python TargetHierarchy}

from darts import TimeSeries
from itertools import product

# Create multivariate time series with sales components
ts_sales = TimeSeries.from_dataframe(df_sales, freq="D")

# Create lists of hierarchy nodes
categories = df_train.category.unique().tolist()
stores = df_train.store_nbr.unique().astype(str).tolist()

# Initialize empty dict
hierarchy_target = dict()

# Map category sales to total sales
for category in categories:
  hierarchy_target[category] = ["TOTAL"]

# Map store sales to total sales
for store in stores:
  hierarchy_target[store] = ["TOTAL"]

# Map category X store combinations to respective category sales and store sales
for category, store in product(categories, stores):
  hierarchy_target["{}-{}".format(category, store)] = [category, store]

#map hierarchy to ts_train
ts_sales = ts_sales.with_hierarchy(hierarchy_target)
print(ts_sales)
del category, store

```

```{python FillTargetGaps}

# Scan gaps
print(ts_sales.gaps())

# Fill gaps
from darts.dataprocessing.transformers import MissingValuesFiller
na_filler = MissingValuesFiller()
ts_sales = na_filler.transform(ts_sales)
```

```{python CategoryPlots}
_ = ts_sales["BREAD/BAKERY"].plot()
_ = ts_sales["CLEANING"].plot()
_ = ts_sales["CELEBRATION"].plot()
_ = ts_sales["LIQUOR,WINE,BEER"].plot()
_ = ts_sales["SCHOOL AND OFFICE SUPPLIES"].plot()
_ = plt.title("Sales of 5 select categories")
plt.show()
plt.close("all")
```

```{python StorePlots}
#| echo = false
_ = ts_sales["1"].plot()
_ = ts_sales["8"].plot()
_ = ts_sales["23"].plot()
_ = ts_sales["42"].plot()
_ = ts_sales["51"].plot()
_ = plt.title("Sales of 5 select stores")
plt.show()
plt.close("all")

```

```{python CatStorePlots1}
#| echo = false
_ = ts_sales["BREAD/BAKERY-1"].plot()
_ = ts_sales["BREAD/BAKERY-8"].plot()
_ = ts_sales["BREAD/BAKERY-23"].plot()
_ = plt.title("Sales of one category across 3 stores")
plt.show()
plt.close("all")
```

```{python CatStorePlots2}
#| echo = false
_ = ts_sales["CELEBRATION-1"].plot()
_ = ts_sales["CELEBRATION-8"].plot()
_ = ts_sales["CELEBRATION-23"].plot()
_ = plt.title("Sales of one category across 3 stores")
plt.show()
plt.close("all")
```

## Covariate series

```{python LogTrafoFuncs}
#| echo = false

# Define functions to perform log transformation and reverse it. +1 to avoid zeroes
def trafo_log(x):
  return x.map(lambda x: np.log(x+1))

def trafo_exp(x):
  return x.map(lambda x: np.exp(x)-1)
```

### Total sales covariates

```{python TotalCovars1}
#| echo = false

# Aggregate time features by mean
total_covars1 = df.drop(
  columns=['id', 'store_nbr', 'category', 'sales', 'onpromotion', 'transactions', 'oil', 'city', 'state', 'store_type', 'store_cluster'], axis=1).groupby("date").mean(numeric_only=True)
  
# Add piecewise linear trend dummies
total_covars1["trend"] = range(1, 1701) # Linear trend dummy 1
total_covars1["trend_knot"] = 0
total_covars1.iloc[728:,-1] = range(0, 972) # Linear trend dummy 2

# Add Fourier features for monthly seasonality
from statsmodels.tsa.deterministic import DeterministicProcess
dp = DeterministicProcess(
  index = total_covars1.index,
  constant = False,
  order = 0, # No trend feature
  seasonal = False, # No seasonal dummy features
  period = 28, # 28-period seasonality (28 days, 1 month)
  fourier = 5, # 5 Fourier pairs
  drop = True # Drop perfectly collinear terms
)
total_covars1 = total_covars1.merge(dp.in_sample(), how="left", on="date")

# Create Darts time series with time features
ts_totalcovars1 = TimeSeries.from_dataframe(total_covars1, freq="D")

# Fill gaps in covars
ts_totalcovars1 = na_filler.transform(ts_totalcovars1)

# Retrieve covars with filled gaps
total_covars1 = ts_totalcovars1.pd_dataframe()
```

```{python TotalCovars2}
#| echo = false

# Aggregate daily covariate series
total_covars2 = df.groupby("date").agg(
 { "oil": "mean",
  "onpromotion": "sum"}
  )
total_covars2["transactions"] = df.groupby(["date", "store_nbr"]).transactions.mean().groupby("date").sum()

# Difference daily covariate series
from sktime.transformations.series.difference import Differencer
diff = Differencer(lags = 1)
total_covars2 = diff.fit_transform(total_covars2)
  
# Replace covariate series with MAs
# Oil
total_covars2["oil_ma28"] = total_covars2["oil"].rolling(window = 28, center = False).mean()
total_covars2["oil_ma28"] = total_covars2["oil_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Onpromotion
total_covars2["onp_ma28"] = total_covars2["onpromotion"].rolling(window = 28, center = False).mean()
total_covars2["onp_ma28"] = total_covars2["onp_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both") 

# Transactions
total_covars2["trns_ma7"] = total_covars2["transactions"].rolling(window = 7, center = False).mean()
total_covars2["trns_ma7"] = total_covars2["trns_ma7"].interpolate("linear", limit_direction = "backward")  

# Drop original covariate series
total_covars2 = total_covars2.drop(["oil", "onpromotion", "transactions"], axis = 1)

# Replace last 15 dates' transactions MAs with NAs
total_covars2.loc[total_covars2.index > "2017-08-15", "trns_ma7"] = np.nan
```

### Category sales covariates

```{python CommonCovars}

# Retrieve copy of total_covars1, drop Fourier terms, trend knot (leaving daily predictors common to all categories).
common_covars = total_covars1[total_covars1.columns[0:21].values.tolist()]

# Add differenced oil price and its MA to common covariates. 
common_covars["oil"] = df.groupby("date").oil.mean()
common_covars["oil"] = diff.fit_transform(common_covars["oil"]).interpolate("time", limit_direction = "both")
common_covars["oil_ma28"] = common_covars["oil"].rolling(window = 28, center = False).mean()
common_covars["oil_ma28"] = common_covars["oil_ma28"].interpolate(
  method = "spline", order = 2, limit_direction = "both")

# Print common covariates
print(common_covars.columns)
```

```{python CategoryCovars}
#| output: false

from darts.utils.timeseries_generation import datetime_attribute_timeseries

# Initialize list of category covariates
ts_catcovars = []

for category in categories:
  
  # Retrieve common covariates
  covars = common_covars.copy()
  
  # Retrieve differenced onpromotion, its MA
  covars["onpromotion"] = diff.fit_transform(
    df[df["category"] == category].groupby("date").onpromotion.sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["onp_ma28"] = covars["onpromotion"].rolling(
    window = 28, center = False
    ).mean().interpolate(
  method = "spline", order = 2, limit_direction = "both"
  ) 
  
  # Retrieve differenced transactions, its MA
  covars["transactions"] = diff.fit_transform(
    df[df["category"] == category].groupby(["date", "store_nbr"]).transactions.mean().groupby("date").sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["trns_ma7"] = covars["transactions"].rolling(
    window = 7, center = False
    ).mean().interpolate(
  "linear", limit_direction = "backward"
  ) 
  
  # Create darts TS, fill gaps
  covars = na_filler.transform(
    TimeSeries.from_dataframe(covars, freq = "D")
    ) 
  
  # Cyclical encode day of month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "day",
      cyclic = True
      )
    )
    
   # Cyclical encode month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "month",
      cyclic = True
      )
    ) 
  
  # Append TS to list
  ts_catcovars.append(covars)
  
  # Cleanup
  del covars
```

### Store sales covariates

```{python StoreCovars}

# Initialize list of category covariates
store_covars = []

for store in [int(store) for store in stores]:
  
  # Retrieve common covariates
  covars = common_covars.copy()
  
  # Retrieve differenced onpromotion, its MA
  covars["onpromotion"] = diff.fit_transform(
    df[df["store_nbr"] == store].groupby("date").onpromotion.sum()
    ).interpolate(
      "time", limit_direction = "both"
      )
  covars["onp_ma28"] = covars["onpromotion"].rolling(
    window = 28, center = False
    ).mean().interpolate(
  method = "spline", order = 2, limit_direction = "both"
  ) 
  
  # Retrieve differenced transactions, its MA
  covars["transactions"] = diff.fit_transform(
    df[df["store_nbr"] == store].groupby("date").transactions.sum().interpolate(
      "time", limit_direction = "both"
      )
    )
  covars["trns_ma7"] = covars["transactions"].rolling(
    window = 7, center = False
    ).mean().interpolate(
  "linear", limit_direction = "backward"
  )
  
  # Create darts TS, fill gaps
  covars = na_filler.transform(
    TimeSeries.from_dataframe(covars, freq = "D")
    )
  
  # Cyclical encode day of month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "day",
      cyclic = True
      )
    )
    
   # Cyclical encode month using datetime_attribute_timeseries
  covars = covars.stack(
    datetime_attribute_timeseries(
      time_index = covars,
      attribute = "month",
      cyclic = True
      )
    )
    
  # Append TS to list
  store_covars.append(covars)
  
  # # Cleanup
  # del covars
```

```{python StoreStaticCovars}

# Create dataframe where column=static covariate and index=store nbr
store_static = df[["store_nbr", "city", "state", "store_type", "store_cluster"]].reset_index().drop("date", axis=1).drop_duplicates().set_index("store_nbr")
store_static["store_cluster"] = store_static["store_cluster"].astype(str)

# Encode static covariates
store_static = pd.get_dummies(store_static, sparse = True, drop_first = True)
```

### Category X store sales covariates

## Modeling: Total sales

### Linear + random forest hybrid from part 1

```{python ScoringFunc}
#| echo = false

# Define model scoring function
from darts.metrics import rmse, rmsle, mape
def perf_scores(val, pred, model="drift"):
  
  scores_dict = {
    "RMSE": rmse(trafo_exp(val), trafo_exp(pred)), 
    "RMSLE": rmse(val, pred), 
    "MAPE": mape(trafo_exp(val), trafo_exp(pred))
      }
      
  print("Model: " + model)
  
  for key in scores_dict:
    print(
      key + ": " + 
      str(round(scores_dict[key], 4))
       )
  print("--------")
```

```{python TotalSalesModel}
#| echo = false

# Perform time decomposition in Sklearn

# Train-test split
y_train_decomp, y_val_decomp = trafo_log(ts_sales["TOTAL"][:-227].pd_series()), trafo_log(ts_sales["TOTAL"][-227:].pd_series()) 
x_train_decomp, x_val_decomp = total_covars1.iloc[:-243,], total_covars1.iloc[-243:,]

# Fit & predict on 13-16, retrieve residuals
from sklearn.linear_model import LinearRegression
model_decomp = LinearRegression()
model_decomp.fit(x_train_decomp, y_train_decomp)
pred_total1 = model_decomp.predict(x_train_decomp)
res_total1 = y_train_decomp - pred_total1

# Predict on 17, retrieve residuals
pred_total2 = model_decomp.predict(x_val_decomp)
res_total2 = y_val_decomp - pred_total2[:-16]

# Concatenate residuals to get decomposed sales
sales_decomp = pd.concat([res_total1, res_total2])

# Concatenate predictions to get model 1 predictions
preds_total1 = pd.Series(np.concatenate((pred_total1, pred_total2)), index = total_covars1.index)


# Add decomped sales ema5 to covars2
total_covars2["sales_ema5"] = sales_decomp.rolling(
  window = 5, min_periods = 1, center = False, win_type = "exponential").mean()

# Make Darts TS with decomposed sales
ts_decomp = TimeSeries.from_series(sales_decomp.rename("TOTAL"), freq="D")

# Make Darts TS with model 1 predictions
ts_preds_total1 = TimeSeries.from_series(preds_total1.rename("TOTAL"), freq="D")

# Make Darts TS with covars2, fill gaps
ts_totalcovars2 = TimeSeries.from_dataframe(total_covars2, freq="D")
ts_totalcovars2 = na_filler.transform(ts_totalcovars2)

# Train-validation split
y_train_total, y_val_total = ts_decomp[:-227], trafo_log(ts_sales["TOTAL"][-227:])
x_train_total, x_val_total = ts_totalcovars2[:-243], ts_totalcovars2[-243:-16]

# Scale covariates (train-validation split only)
from sklearn.preprocessing import StandardScaler
from darts.dataprocessing.transformers import Scaler
scaler = Scaler(StandardScaler())
x_train_total = scaler.fit_transform(x_train_total)
x_val_total = scaler.transform(x_val_total)

# Model spec
from darts.models.forecasting.random_forest import RandomForest
model_forest = RandomForest(
  lags = [-1, -2, -3, -4, -5], # Target lags that can be used
  lags_future_covariates = [0], # No covariate lags
  n_estimators = 500, # Build 500 trees
  random_state = 1923,
  n_jobs = -2 # Use all but one of the CPUs
  )

# Fit model & predict validation set
model_forest.fit(y_train_total, future_covariates = x_train_total)
pred_total = model_forest.predict(
  n = 227, future_covariates = x_val_total) + ts_preds_total1[-243:-16]

  
# Score model
perf_scores(y_val_total, pred_total, model="Linear + Random Forest hybrid, total sales")
```

### Linear + XGBoost hybrid, grid search tuning

```{python TotalSalesXGB}
# from darts.models.forecasting.xgboost import XGBModel 
# 
# #  Hyperparameter search space 
# xgb_params = {
#   "lags": [[-1, -2, -3, -4, -5]],
#   "lags_future_covariates": [[0]],
#   "n_jobs": [-2],
#   "learning_rate": [0.1, 0.2, 0.3],
#   "max_depth": [4, 6, 8],
#   "min_child_weight": [1, 4, 8],
#   "gamma": [0, 0.05, 0.1],
#   "subsample": [0.6, 0.8, 1],
#   "colsample_bytree": [0.6, 0.8, 1],
#   "n_estimators": [100, 250, 500]
#   }
# 
# # Decomposed validation series 
# y_val_total_decomp = y_val_total - ts_preds_total1[-243:-16] 
# 
# # Full covariates series 
# x_total = x_train_total.append(x_val_total)  
# 
# # Grid search 
# model_xgb, xgb_besttunes, xgb_bestscore = XGBModel.gridsearch( 
#   parameters = xgb_params,  
#   series = y_train_total, 
#   forecast_horizon = 15, 
#   start = 0.75,  
#   stride = 15, 
#   future_covariates = x_total,  
#   val_series = y_val_total_decomp,  
#   metric = rmse 
# )

```

grid search runtime: 9-10mins grid search best tune and score:

    {'lags': [-1, -2, -3, -4, -5], 'lags_future_covariates': [0], 'n_jobs': -2, 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1, 'colsample_bytree': 1, 'n_estimators': 500}

    0.0895999112054749 rmsle

```{python TotalSalesXGBScore}

# # Fit model & predict validation set 
# model_xgb.fit(y_train_total, future_covariates = x_total) 
# pred_total = model_xgb.predict(
#   n = 227, 
#   future_covariates = x_total) + ts_preds_total1[-243:-16] 
#   
# # Score model 
# perf_scores(y_val_total, pred_total, model="Linear + XGB hybrid, total sales")

```

## Modeling: Category sales

```{python HierarchicalScoreFunc}
from darts.metrics import mse, mae

# Define model scoring function for full hierarchy
def scores_hierarchy(val, pred, subset, model):
  
  def measure_mae(val, pred, subset):
    return mae([val[c] for c in subset], [pred[c] for c in subset])
  
  def measure_mse(val, pred, subset):
    return mse([val[c] for c in subset], [pred[c] for c in subset])
  
  def measure_rmse(val, pred, subset):
    return rmse([val[c] for c in subset], [pred[c] for c in subset])

  def measure_rmsle(val, pred, subset):
    return rmsle([(val[c]) for c in subset], [pred[c] for c in subset])

  scores_dict = {
    "MAE": measure_mae(val, pred, subset),
    "MSE": measure_mse(val, pred, subset),
    "RMSE": measure_rmse(val, pred, subset), 
    "RMSLE": measure_rmsle(val, pred, subset)
      }
      
  print("Model = " + model)    
  
  for key in scores_dict:
    print(
      key + ": mean = " + 
      str(round(np.nanmean(scores_dict[key]), 2)) + 
      ", sd = " + 
      str(round(np.nanstd(scores_dict[key]), 2)) + 
      ", min = " + str(round(min(scores_dict[key]), 2)) + 
      ", max = " + 
      str(round(max(scores_dict[key]), 2))
       )
       
  print("--------")

```

```{python NegRemover}
# Define function to replace negative predictions with zeroes
def trafo_zero(x):
  return x.map(lambda x: np.clip(x, a_min = 0, a_max = None))
```

### Preprocessing

```{python CategoryCovars}

# Create min-max scaler
scaler_minmax = Scaler()

# Train-validation split and scaling for covariates
x_cat = []
for series in ts_catcovars:
  
  # Split train-val series
  cov_train, cov_val = series[:-243], series[-243:-16]
  
  # Scale train-val series
  cov_train = scaler_minmax.fit_transform(cov_train)
  cov_val = scaler_minmax.transform(cov_val)
  
  # Cast series to 32-bits for performance gains
  cov_train = cov_train.astype(np.float32)
  cov_val = cov_val.astype(np.float32)
  
  # Rejoin series
  cov_train = cov_train.append(cov_val)
  
  # Append series to list
  x_cat.append(cov_train)
  
  # Cleanup
  del cov_train, cov_val
```

```{python CategoryTargets}

# List of category sales
category_sales = [ts_sales[categories][category] for category in categories]

# Train-validation split for category sales
y_train_cat, y_val_cat = [], []
for series in category_sales:
  
  # Split train-val series
  y_train, y_val = series[:-227], series[-227:]
  
  # # Scale train-val series
  # y_train = scaler_minmax.fit_transform(y_train)
  # y_val = scaler_minmax.transform(y_val)
  
  # Cast series to 32-bits for performance gains
  y_train = y_train.astype(np.float32)
  y_val = y_val.astype(np.float32)
  
  # Append series
  y_train_cat.append(y_train)
  y_val_cat.append(y_val)
  
  # Cleanup
  del y_train, y_val

```

### Baseline models

```{python CategoryBaselineSpec}

# Import baseline models
from darts.models.forecasting.baselines import NaiveDrift, NaiveSeasonal

# Specify baseline models
model_drift = NaiveDrift()
model_seasonal = NaiveSeasonal(K=7) # Repeat the last week of the training data
```

```{python CategoryBaselineFitVal}

# Fit & validate baseline models

# Naive
model_drift.fit(
  #scaler_minmax.fit_transform(ts_sales["AUTOMOTIVE":"SEAFOOD"][:-227])
  ts_sales[categories][:-227]
  )
pred_drift_cat = model_drift.predict(n = 227)

model_seasonal.fit(
  ts_sales[categories][:-227]
)
pred_seasonal_cat = model_seasonal.predict(n = 227)
```

```{python CategoryScoreBaselines}

print("Category sales prediction scores, baseline models")
print("--------")

# Naive
scores_hierarchy(
  #scaler_minmax.transform(ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:]),
  ts_sales[categories][-227:],
  pred_drift_cat,
  categories, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales[categories][-227:], 
  pred_seasonal_cat,
  categories, 
  "Naive seasonal"
  )
```

### Hybrid models

#### STL decomposition

```{python CatSTLDecomp}
from darts.utils.statistics import extract_trend_and_seasonality as decomposition
from darts.utils.statistics import remove_from_series
from darts.utils.utils import ModelMode, SeasonalityMode

# Perform STL decomposition on training data to get trend + seasonality and remainder series
trend_cat = []
season_cat = []
remainder_cat = []

for series in y_train_cat:
  
  # # Log transform series
  # series = trafo_log(series)
  
  # Perform STL decomposition
  trend, seasonality = decomposition(
    series,
    model = ModelMode.ADDITIVE,
    method = "STL",
    freq = 7, # N. of obs in each seasonality cycle (12 for monthly CO2 data with yearly seasonality cycle)
    seasonal = 7, # Size of seasonal smoother (last n lags)
    trend = 85, # Size of trend smoother (last n lags)
    robust = True
  )
  
  # Rename components in trend and seasonality series
  trend = trend.with_columns_renamed(trend.components[0], series.components[0])
  seasonality = seasonality.with_columns_renamed(seasonality.components[0], series.components[0])
  
  # Remove trend & seasonality from series
  remainder = remove_from_series(
    series,
    (trend + seasonality),
    ModelMode.ADDITIVE
  )
  
  # Append to lists
  trend_cat.append(
    # trafo_exp(trend)
    trend
    )
  
  season_cat.append(
    # trafo_exp(seasonality)
    seasonality
  )  
    
  remainder_cat.append(
    # trafo_exp(remainder)
    remainder
  )
  
  # Cleanup
  del series, trend, seasonality, remainder

```

```{python CatSTLPlot}

y_train_cat[5].plot()
trend_cat[5].plot(label = "STL trend")
plt.show()
plt.close("all")

season_cat[5].plot(label = "STL seasonality")
plt.show()
plt.close("all")

remainder_cat[5].plot(label = "STL remainder")
plt.show()
plt.close("all")
```

#### Linear regression on trend + seasonality

```{python CatLinearSpec}
from darts.models.forecasting.linear_regression_model import LinearRegressionModel

# Specify linear regression model
model_linear_cat = LinearRegressionModel(
  lags_future_covariates = [0], # Don't create any covariate lags
  output_chunk_length = 15
)
```

```{python CatLinearFitVal}
#| output: false

# # Time covariates (trend + seasonality + calendar)
# linear_covars = ["trend", 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', "month_sin", "month_cos", 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']

# Time covariates (trend + seasonality)
linear_covars = ["trend", 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', "month_sin", "month_cos"]


# First fit & validate the first category to initialize series
model_linear_cat.fit(
  (trend_cat[0] + season_cat[0]),
  future_covariates = x_cat[0][linear_covars]
  )

pred_linear_cat = model_linear_cat.predict(
  n=227,
  future_covariates = x_cat[0][linear_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_cat))):

  # Fit on training data
  model_linear_cat.fit(
  (trend_cat[i] + season_cat[i]),
  future_covariates = x_cat[i][linear_covars]
  )

  # Predict validation data
  pred = model_linear_cat.predict(
  n=227,
  future_covariates = x_cat[i][linear_covars]
  )

  # Stack predictions to multivariate series
  pred_linear_cat = pred_linear_cat.stack(pred)
  
del pred, i
```

#### AutoARIMA on remainder

```{python CatArimaSpec}
from darts.models.forecasting.auto_arima import AutoARIMA

# AutoARIMA
model_arima_cat = AutoARIMA(
  start_p = 0,
  max_p = 7,
  start_q = 0,
  max_q = 7,
  seasonal = False, # Don't include seasonal orders
  information_criterion = 'aicc', # Minimize AICc to choose best model
  trace = False # Don't print tuning iterations
  )

```

```{python CategoryArimaFitVal}

# # AutoARIMA covars (cyclical only)
# arima_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# AutoARIMA covars (cyclical + calendar)
arima_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']


# First fit & validate the first category to initialize series
model_arima_cat.fit(
  remainder_cat[0],
  future_covariates = x_cat[0][arima_covars])
  
pred_arima_cat = model_arima_cat.predict(
  n=227,
  future_covariates = x_cat[0][arima_covars])
  
# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_cat))):
  
  # Fit on training data
  model_arima_cat.fit(
  remainder_cat[i],
  future_covariates = x_cat[i][arima_covars])
  
  # Predict validation data
  pred = model_arima_cat.predict(
  n=227,
  future_covariates = x_cat[i][arima_covars])
  
  # Stack predictions to multivariate series
  pred_arima_cat = pred_arima_cat.stack(pred)
  
del pred, i
```

#### Random forest on remainder

```{python CatRFSpec}

# Specify random forest model
model_rf_cat = RandomForest(
  lags = [-1, -2, -3, -4, -5, -6, -7],
  lags_future_covariates = [0],
  output_chunk_length = 15,
  n_estimators = 500,
  oob_score = True,
  random_state = 1923,
  n_jobs = -2
)
```

```{python CatRFFitVal}

# # RF covariates (cylical)
# rf_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# RF covariates (cylical + calendar)
rf_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']

# First fit & validate the first category to initialize series
model_rf_cat.fit(
  remainder_cat[0],
  future_covariates = x_cat[0][rf_covars]
  )

pred_rf_cat = model_rf_cat.predict(
  n=227,
  future_covariates = x_cat[0][rf_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_cat))):

  # Fit on training data
  model_rf_cat.fit(
  remainder_cat[i],
  future_covariates = x_cat[i][rf_covars]
  )

  # Predict validation data
  pred = model_rf_cat.predict(
  n=227,
  future_covariates = x_cat[i][rf_covars]
  )

  # Stack predictions to multivariate series
  pred_rf_cat = pred_rf_cat.stack(pred)

del pred, i
```

### Model scores

```{python CategoryScore}

print("Category sales prediction scores")
print("--------")

# Naive
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_drift_cat),
  categories, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:], 
  pred_seasonal_cat,
  categories, 
  "Naive seasonal"
  )

# Linear 
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_linear_cat),
  categories,
  "Linear"
  )

# Linear + AutoARIMA
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_linear_cat + pred_arima_cat),
  categories,
  "Linear + AutoARIMA"
  )
  

# Linear + RF
scores_hierarchy(
  ts_sales["AUTOMOTIVE":"SEAFOOD"][-227:],
  trafo_zero(pred_linear_cat + pred_rf_cat),
  categories,
  "Linear + Random forest"
  )  
  

```

    Category sales prediction scores (STL: 7 freq, 7 seasonal, 365 trend, robust)
    --------
    Model = Linear
    MAE: mean = 3046.03, sd = 6534.27, min = 4.16, max = 28588.28
    MSE: mean = 98286888.96, sd = 292192049.04, min = 24.87, max = 1212049616.12
    RMSE: mean = 4267.58, sd = 8948.44, min = 4.99, max = 34814.5
    RMSLE: mean = 0.49, sd = 0.28, min = 0.22, max = 1.4
    --------
    Model = Linear + AutoARIMA
    MAE: mean = 5959.81, sd = 16077.21, min = 4.38, max = 86022.96
    MSE: mean = 324237512.48, sd = 1344769719.96, min = 27.08, max = 7676345316.24
    RMSE: mean = 6611.52, sd = 16748.89, min = 5.2, max = 87614.76
    RMSLE: mean = 0.53, sd = 0.36, min = 0.2, max = 1.86
    --------
    Model = Linear + Random forest
    MAE: mean = 5370.76, sd = 15105.83, min = 5.27, max = 84914.55
    MSE: mean = 309365404.75, sd = 1305675223.25, min = 37.2, max = 7563237010.71
    RMSE: mean = 6538.93, sd = 16328.13, min = 6.1, max = 86966.87
    RMSLE: mean = 0.54, sd = 0.35, min = 0.26, max = 1.81
    --------

    Category sales prediction scores (STL: 7 freq, 7 seasonal, 85 trend, robust)
    --------
    Model = Linear
    MAE: mean = 2639.5, sd = 5292.82, min = 4.6, max = 21243.23
    MSE: mean = 81261537.6, sd = 250978792.24, min = 29.46, max = 1212390953.88
    RMSE: mean = 3923.55, sd = 8115.87, min = 5.43, max = 34819.4
    RMSLE: mean = 0.48, sd = 0.25, min = 0.23, max = 1.29
    --------
    Model = Linear + AutoARIMA
    MAE: mean = 2926.01, sd = 6253.96, min = 4.83, max = 25884.62
    MSE: mean = 88461466.49, sd = 272291133.42, min = 31.89, max = 1218547823.09
    RMSE: mean = 3954.99, sd = 8533.44, min = 5.65, max = 34907.7
    RMSLE: mean = 0.46, sd = 0.33, min = 0.19, max = 1.8
    --------
    Model = Linear + Random forest
    MAE: mean = 3023.34, sd = 6120.32, min = 4.91, max = 24538.09
    MSE: mean = 93857560.67, sd = 278523809.53, min = 33.6, max = 1253212175.1
    RMSE: mean = 4296.76, sd = 8683.05, min = 5.8, max = 35400.74
    RMSLE: mean = 0.48, sd = 0.24, min = 0.26, max = 1.22
    --------

## Modeling: Store sales

### Preprocessing

```{python StoreCovars}

# Train-validation split and scaling for covariates
x_store = []
for series in store_covars:
  
  # Split train-val series
  cov_train, cov_val = series[:-243], series[-243:-16]
  
  # Scale train-val series
  cov_train = scaler_minmax.fit_transform(cov_train)
  cov_val = scaler_minmax.transform(cov_val)
  
  # Cast series to 32-bits for performance gains
  cov_train = cov_train.astype(np.float32)
  cov_val = cov_val.astype(np.float32)
  
  # Rejoin series
  cov_train = cov_train.append(cov_val)
  
  # Append series to list
  x_store.append(cov_train)
  
  # Cleanup
  del cov_train, cov_val

```

```{python StoreTargets}

# List of store sales
store_sales = [ts_sales[store] for store in stores]

# Train-validation split for store sales
y_train_store, y_val_store = [], []
for series in store_sales:
  
  # Add static covariates to series
  series = series.with_static_covariates(
    store_static[store_static.index == int(series.components[0])]
  )
  
  # Split train-val series
  y_train, y_val = series[:-227], series[-227:]
  
  # # Scale train-val series
  # y_train = scaler_minmax.fit_transform(y_train)
  # y_val = scaler_minmax.transform(y_val)
  
  # Cast series to 32-bits for performance gains
  y_train = y_train.astype(np.float32)
  y_val = y_val.astype(np.float32)
  
  # Append series
  y_train_store.append(y_train)
  y_val_store.append(y_val)
  
  # Cleanup
  del y_train, y_val
```

### Baseline models

```{python StoreBaselineFitVal}

# Fit & validate baseline models

# Naive drift
model_drift.fit(ts_sales[stores][:-227])
pred_drift_store = model_drift.predict(n = 227)

# Naive seasonal
model_seasonal.fit(ts_sales[stores][:-227])
pred_seasonal_store = model_seasonal.predict(n = 227)

print("Store sales prediction scores, baseline models")
print("--------")

scores_hierarchy(
  ts_sales[stores][-227:],
  pred_drift_store,
  stores, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales[stores][-227:], 
  pred_seasonal_store,
  stores, 
  "Naive seasonal"
  )
```

### Hybrid models

#### STL decomposition

```{python StoreSTLDecomp}

# Perform STL decomposition on training data to get trend + seasonality and remainder series
trend_store = []
season_store = []
remainder_store = []

for series in y_train_store:
  
  # # Log transform series
  # series = trafo_log(series)
  
  # Perform STL decomposition
  trend, seasonality = decomposition(
    series,
    model = ModelMode.ADDITIVE,
    method = "STL",
    freq = 7, # N. of obs in each seasonality cycle (12 for monthly CO2 data with yearly seasonality cycle)
    seasonal = 7, # Size of seasonal smoother (last n lags)
    trend = 85, # Size of trend smoother (last n lags)
    robust = True
  )
  
  # Rename components in trend and seasonality series
  trend = trend.with_columns_renamed(
    trend.components[0], 
    series.components[0]
    )
    
  seasonality = seasonality.with_columns_renamed(
    seasonality.components[0], 
    series.components[0]
    )
  
  # Remove trend & seasonality from series
  remainder = remove_from_series(
    series,
    (trend + seasonality),
    ModelMode.ADDITIVE
  )
  
  # Append to lists
  trend_store.append(
    # trafo_exp(trend)
    trend
    )
  
  season_store.append(
    # trafo_exp(seasonality)
    seasonality
  )  
    
  remainder_store.append(
    # trafo_exp(remainder)
    remainder
  )
  
  # Cleanup
  del series, trend, seasonality, remainder

```

```{python StoreSTLPlot}

y_train_store[8].plot()
trend_store[8].plot(label = "STL trend")
plt.show()
plt.close("all")

season_store[8].plot(label = "STL seasonality")
plt.show()
plt.close("all")

remainder_store[8].plot(label = "STL remainder")
plt.show()
plt.close("all")
```

#### Linear regression on trend + seasonality

```{python StoreLinearFitVal}
#| output: false

# Model spec
model_linear_store = model_linear_cat

# # Time covariates (trend + seasonality + calendar)
# linear_covars = ["trend",'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']

# Time covariates (trend + seasonality)
linear_covars = ["trend", 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'day_sin', 'day_cos', 'month_sin', 'month_cos']

# First fit & validate the first store to initialize series
model_linear_store.fit(
  (trend_store[0] + season_store[0]),
  future_covariates = x_store[0][linear_covars]
  )

pred_linear_store = model_linear_store.predict(
  n=227,
  future_covariates = x_store[0][linear_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_store))):

  # Fit on training data
  model_linear_store.fit(
  (trend_store[i] + season_store[i]),
  future_covariates = x_store[i][linear_covars]
  )

  # Predict validation data
  pred = model_linear_store.predict(
  n=227,
  future_covariates = x_store[i][linear_covars]
  )

  # Stack predictions to multivariate series
  pred_linear_store = pred_linear_store.stack(pred)
  
del pred, i
```

#### AutoARIMA on remainder

```{python StoreARIMAFitVal}

# Retrieve model spec
model_arima_store = model_arima_cat

# # AutoARIMA covars (cyclical only)
# arima_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# AutoARIMA covars (cyclical + calendar)
arima_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']


# First fit & validate the first category to initialize series
model_arima_store.fit(
  remainder_store[0],
  future_covariates = x_store[0][arima_covars])
  
pred_arima_store = model_arima_store.predict(
  n=227,
  future_covariates = x_store[0][arima_covars])
  
# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_store))):
  
  # Fit on training data
  model_arima_store.fit(
  remainder_store[i],
  future_covariates = x_store[i][arima_covars])
  
  # Predict validation data
  pred = model_arima_store.predict(
  n=227,
  future_covariates = x_store[i][arima_covars])
  
  # Stack predictions to multivariate series
  pred_arima_store = pred_arima_store.stack(pred)
  
del pred, i
```

Mention: AutoARIMA caught a completely constant input series, returned an ARMA (0 0 0) model.

#### Random forest on remainder

```{python StoreRFFitVal}

# Model spec
model_rf_store = RandomForest(
  lags = [-1, -2, -3, -4, -5, -6, -7],
  lags_future_covariates = [0],
  output_chunk_length = 15,
  n_estimators = 500,
  oob_score = True,
  random_state = 1923,
  n_jobs = -2
)

# # RF covariates (cyclical only)
# rf_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# RF covariates (calendar + cyclical)
rf_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']

# First fit & validate the first category to initialize series
model_rf_store.fit(
  remainder_store[0],
  future_covariates = x_store[0][rf_covars]
  )

pred_rf_store = model_rf_store.predict(
  n=227,
  future_covariates = x_store[0][rf_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_store))):

  # Fit on training data
  model_rf_store.fit(
  remainder_store[i],
  future_covariates = x_store[i][rf_covars]
  )

  # Predict validation data
  pred = model_rf_store.predict(
  n=227,
  future_covariates = x_store[i][rf_covars]
  )

  # Stack predictions to multivariate series
  pred_rf_store = pred_rf_store.stack(pred)

# Cleanup
del pred, i

```

#### RNN on decomposed sales (global)

```{python InitTorch}
import torch
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks import RichProgressBar, RichModelSummary
import tensorboard

torch.set_float32_matmul_precision("high")
torch.set_printoptions(sci_mode = False)

# Create early stopper
early_stopper = EarlyStopping(
  monitor = "val_loss",
  min_delta = 5800, # 1% of min. MSE of best model so far
  patience = 10
)

# Progress bar
progress_bar = RichProgressBar()

# Rich model summary
model_summary = RichModelSummary(max_depth = -1)

```

```{python StoreDecompValid}

# Retrieve remainder validation sets as validation sets minus linear model predictions
pred_linear_store_list = [pred_linear_store[store] for store in stores]
remainder_store_val = []

for i in (range(0, len(pred_linear_store_list))):
  
  remainder = y_val_store[i] - pred_linear_store_list[i]
  
  remainder_store_val.append(remainder)
  
del remainder

```

```{python StoreRNNSpec}
from darts.models.forecasting.rnn_model import RNNModel as RNN

# Specify RNN model
model_rnn_store = RNN(
  model = "LSTM",
  input_chunk_length = 180,
  training_length = 195,
  batch_size = 32,
  n_epochs = 500,
  n_rnn_layers = 2,
  hidden_dim = 32,
  dropout = 0.1,
  model_name = "RNNStore4",
  log_tensorboard = True,
  save_checkpoints = True,
  show_warnings = True,
  force_reset = True,
  optimizer_kwargs = {"lr": 0.005},
  lr_scheduler_cls = torch.optim.lr_scheduler.ReduceLROnPlateau,
  lr_scheduler_kwargs = {"patience": 5},
  pl_trainer_kwargs = {
    "callbacks": [early_stopper, progress_bar, model_summary],
    "accelerator": "gpu",
    "devices": [0]
    }
)

```

```{python StoreRNNFit}
#| output: false
#| warning: false
#| include: false

# # RNN covariates (cyclical only)
# rnn_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7']

# RNN covariates (calendar + cyclical)
rnn_covars = ['oil', 'oil_ma28', 'onpromotion', 'onp_ma28', 'transactions', 'trns_ma7', 'local_holiday', 'regional_holiday', 'national_holiday', 'ny1', 'ny2', 'ny_eve31', 'ny_eve30', 'xmas_before', 'xmas_after', 'quake_after', 'dia_madre', 'futbol', 'black_friday', 'cyber_monday']

# Fit RNN model
model_rnn_store.fit(
  series = remainder_store,
  future_covariates = [x[rnn_covars] for x in x_store],
  val_series = remainder_store_val,
  val_future_covariates = [x[rnn_covars] for x in x_store],
  verbose = True
)
```

```{python StoreRNNValid}

# First fit & validate the first store to initialize series
pred_rnn_store = model_rnn_store.predict(
  n=227,
  series = y_train_store[0],
  future_covariates = x_store[0][rnn_covars]
  )

# Then loop over all categories except first
for i in tqdm(range(1, len(y_train_store))):

  # Predict validation data
  pred = model_rnn_store.predict(
  n=227,
  series = y_train_store[i],
  future_covariates = x_store[i][rnn_covars]
  )

  # Stack predictions to multivariate series
  pred_rnn_store = pred_rnn_store.stack(pred)
  
del pred, i
```

### Model scores

```{python StoreScore}

print("Store sales prediction scores")
print("--------")

# Naive
scores_hierarchy(
  ts_sales[stores][-227:],
  trafo_zero(pred_drift_store),
  stores, 
  "Naive drift"
  )

scores_hierarchy(
  ts_sales[stores][-227:], 
  pred_seasonal_store,
  stores, 
  "Naive seasonal"
  )

# Linear 
scores_hierarchy(
  ts_sales[stores][-227:],
  trafo_zero(pred_linear_store),
  stores,
  "Linear"
  )

# Linear + AutoARIMA
scores_hierarchy(
  ts_sales[stores][-227:],
  trafo_zero(pred_linear_store + pred_arima_store),
  stores,
  "Linear + AutoARIMA"
  )

# Linear + Random forest 
scores_hierarchy(
  ts_sales[stores][-227:],
  trafo_zero(pred_linear_store + pred_rf_store),
  stores,
  "Linear + RF"
  )

# Linear + RNN global
scores_hierarchy(
  ts_sales[stores][-227:],
  trafo_zero(pred_linear_store + pred_rnn_store),
  stores,
  "Linear + RNN (global)"
  )
  
```

    Store sales prediction scores (STL = 85 trend, 7 period, 7 season, robust)
    --------
    Model = Linear (trend + seasonality feats)
    MAE: mean = 2046.32, sd = 1539.36, min = 613.24, max = 9609.55
    MSE: mean = 13563682.38, sd = 26863817.51, min = 898487.02, max = 186283233.07
    RMSE: mean = 2962.48, sd = 2188.01, min = 947.89, max = 13648.56
    RMSLE: mean = 0.78, sd = 0.87, min = 0.36, max = 7.07
    --------
    Model = Linear + AutoARIMA (calendar + cyclicality feats)
    MAE: mean = 2124.35, sd = 1614.63, min = 557.05, max = 9609.55
    MSE: mean = 12206346.55, sd = 25945482.03, min = 594005.27, max = 186283214.04
    RMSE: mean = 2785.3, sd = 2109.14, min = 770.72, max = 13648.56
    RMSLE: mean = 0.72, sd = 0.88, min = 0.2, max = 7.07
    --------
    Model = Linear + RF (calendar + cyclicality feats)
    MAE: mean = 2092.38, sd = 1538.2, min = 573.96, max = 9609.55
    MSE: mean = 13005342.68, sd = 26391260.62, min = 645534.17, max = 186283233.07
    RMSE: mean = 2911.87, sd = 2127.53, min = 803.45, max = 13648.56
    RMSLE: mean = 0.76, sd = 0.87, min = 0.32, max = 7.07
    --------
    Model = Linear + RNN (global) (calendar + cyclicality feats)
    MAE: mean = 5485.38, sd = 1284.81, min = 3229.92, max = 10135.01
    MSE: mean = 38436688.17, sd = 23497295.9, min = 13121037.03, max = 146031353.86
    RMSE: mean = 5992.42, sd = 1589.83, min = 3622.3, max = 12084.34
    RMSLE: mean = 0.9, sd = 0.69, min = 0.51, max = 5.86
    --------

    Store sales prediction scores (STL = 365 trend, 7 period, 7 season, robust)
    --------
    Model = Linear (trend + seasonality feats)
    MAE: mean = 2072.68, sd = 1556.98, min = 621.88, max = 9609.55
    MSE: mean = 13505099.33, sd = 27006508.91, min = 892145.95, max = 186283233.07
    RMSE: mean = 2947.06, sd = 2195.44, min = 944.53, max = 13648.56
    RMSLE: mean = 0.77, sd = 0.87, min = 0.32, max = 7.07
    --------
    Model = Linear + AutoARIMA (calendar + cyclicality feats)
    MAE: mean = 2231.12, sd = 1699.52, min = 548.22, max = 9609.55
    MSE: mean = 13054468.23, sd = 26373898.23, min = 581303.54, max = 186283214.04
    RMSE: mean = 2897.09, sd = 2159.02, min = 762.43, max = 13648.56
    RMSLE: mean = 0.73, sd = 0.87, min = 0.22, max = 7.07
    --------
    Model = Linear + RF (calendar + cyclicality feats)
    MAE: mean = 2330.0, sd = 1719.61, min = 592.78, max = 9609.55
    MSE: mean = 15169178.78, sd = 28198825.95, min = 677384.54, max = 186283233.07
    RMSE: mean = 3130.16, sd = 2317.6, min = 823.03, max = 13648.56
    RMSLE: mean = 0.79, sd = 0.88, min = 0.3, max = 7.07
    --------
    Model = Linear + RNN (global) (calendar + cyclicality feats)
    MAE: mean = 2584.41, sd = 1456.56, min = 841.51, max = 9403.14
    MSE: mean = 15414369.68, sd = 26127776.44, min = 1203581.25, max = 178508470.25
    RMSE: mean = 3340.91, sd = 2062.21, min = 1097.08, max = 13360.71
    RMSLE: mean = 0.78, sd = 0.73, min = 0.29, max = 6.05
    --------

## Modeling: All bottom level series

## Hierarchical reconciliation

```{python ReconcileTop}
from darts.dataprocessing.transformers.reconciliation import TopDownReconciliator as TopDown
from darts import concatenate

# Fit top down reconciliator on original series
recon_top = TopDown(verbose = True)
recon_top.fit(ts_sales)
```

```{python TESThier1}
# Distribute total sales forecasts to bottom levels
test = concatenate([trafo_exp(pred_total), ts_sales["AUTOMOTIVE":][-227:]], axis = 1).with_columns_renamed("sales", "TOTAL")
test = recon_top.transform(test.with_hierarchy(hierarchy_target))
```

```{python TESTscorehier1}
# Score each level
scores_hierarchy(ts_sales[-227:], test, categories)
#
scores_hierarchy(ts_sales[-227:], test, stores)
#

categories_stores = []
for category, store in product(categories, stores):
  categories_stores.append("{}-{}".format(category, store))
  
scores_hierarchy(ts_sales[-227:], test, categories_stores)
```

## Competition submission

Remember to reverse log and CPI transformations

Past covars for test predictions: "transactions", "trns_ma7"
